[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bike_Share",
    "section": "",
    "text": "BikeShare\n\n\n\n\n\n\n\n\n\n\n\nEric Mossotti\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "bikeShare.html",
    "href": "bikeShare.html",
    "title": "BikeShare",
    "section": "",
    "text": "Communicate to shareholders data based insights regarding to better inform their desire to enlist more annual subscribers."
  },
  {
    "objectID": "bikeShare.html#filtering-data-smartly",
    "href": "bikeShare.html#filtering-data-smartly",
    "title": "BikeShare",
    "section": "Filtering Data, Smartly",
    "text": "Filtering Data, Smartly\nTo ensure the conclusions are accurate, outliers should be filtered. Negative and very low trip times might skew trends. The underlying reason for very low trip times is somewhat of an unknown. Perhaps people often change their minds?\n\n\nCode\n# So you don't have to re-download or re-filter everything after making further adjustments.\n\ntblPath &lt;- \"db/data.db\"\ntblPath_fltrd &lt;- \"db/data_fltrd.db\"\n\nif(exists(\"dbconn\") == FALSE) {\n    dbconn &lt;- DBI::dbConnect(duckdb::duckdb(),\n                             dbdir = tblPath,\n                             read_only = FALSE,\n                             check_from = FALSE)\n}\n\nif (duckdb::dbExistsTable(dbconn,\n                          \"tblPath_fltrd\") == FALSE) {\n    source(\"filterDatabase.R\")\n    filterDatabase()\n    }\n\n# To verify the new filtered table exists.\nduckdb::dbListTables(dbconn)\n\n\n[1] \"db/data.db\"       \"db/data_fltrd.db\"\n\n\n\n\nCode\n#|eval: FALSE\n\n# If you need to drop any tables\nsource(\"duckDrops.R\")\n\n\nRemove which tables? (separate by comma)\n\n\nSo this should have removed the major, potentially, non-natural outliers from the dataset which are due to errors (including user errors).\nVerify the tables that now exist:\n\n\nCode\nduckdb::dbListTables(dbconn)\n\n\n[1] \"db/data.db\"       \"db/data_fltrd.db\""
  },
  {
    "objectID": "bikeShare.html#visualizations",
    "href": "bikeShare.html#visualizations",
    "title": "BikeShare",
    "section": "",
    "text": "for quick reference with using Tsibble syntax\n\n\nCode\n#|label: 'grouped tibb'\n\ngrouped_byDay &lt;- dplyr::tbl(dbconn_fltrd,\n                            tblPath_fltrd) |&gt;\n    dplyr::select(started_at,\n                  member_casual) |&gt;\n    dplyr::collect() |&gt;\n    dplyr::mutate(started_at = as.Date(started_at)) |&gt;\n    dplyr::group_by(started_at,\n                    member_casual) |&gt;\n    dplyr::summarize(n = dplyr::n(),\n                     sdev = stats::sd(n))\n\n\n\n\nCode\n#|label: 'to grouped tsibb'\n\n# tsibble, time-series table/tibble seems to make time series plots more straightforward\ngrouped_tsi &lt;- grouped_byDay |&gt;\n    tsibble::as_tsibble(key = c(member_casual,\n                                n),\n                        index = started_at) |&gt;\n    dplyr::arrange(started_at)\n\n\n\n\nCode\n#|label: \"map query setup\"\n\n# chicago starting coordinates for leaflet, setView\nchicago &lt;- maps::us.cities |&gt;\n    dplyr::select(\"name\",\n                  \"long\",\n                  \"lat\") |&gt;\n    dplyr::filter(name == \"Chicago IL\")\n\n# full dataset coordinates, might need to sample\ncoordQry &lt;- dplyr::tbl(dbconn_fltrd,\n                       tblPath_fltrd) |&gt;\n    dplyr::select(start_lng,\n                  start_lat) |&gt;\n    dplyr::add_count(start_lng,\n                     start_lat) |&gt;\n    dplyr::distinct() |&gt;\n    dplyr::arrange(desc(n)) |&gt;\n    dplyr::collect()\n\n\ncoordQry_small &lt;- dplyr::tbl(dbconn_fltrd,\n                             tblPath_fltrd) |&gt;\n    dplyr::select(start_lng,\n                  start_lat) |&gt;\n    dplyr::add_count(start_lng,\n                     start_lat) |&gt;\n    dplyr::distinct() |&gt;\n    dplyr::arrange(desc(n)) |&gt;\n    dplyr::collect() |&gt;\n    dplyr::slice_head(n = 50)\n\n\n\n\nCode\n#|label: \"mapview\"\n\ncoordQry_small |&gt;\n    sf::st_as_sf(coords = c(1:2),\n                crs = 4326) |&gt;\n    mapview::mapview()\n\n\n\n\n\n\n\n\nCode\nojs_define(js_tsi = grouped_tsi)\n\n\n\n\nCode\njsData = transpose(js_tsi)\n\n\n\n\n\n\n\n\n\n\nCode\nPlot.plot({\n    grid: true,\n    color: {legend: true},\n    marks: [\n        Plot.dot(jsData, {x: 'started_at', y: 'n', fill: 'member_casual'})\n]\n})\n\n\n\n\n\n\n\n\n\n\nCode\nPlot.lineY(jsData, {x: \"started_at\", y: \"n\"}).plot()\n\n\n\n\n\n\n\n\n\nCode\nInputs.table(jsData, {\nrows: 20\n})\n\n\n\n\n\n\n\n\n\nCode\n#|label: 'grouped stats tibb'\n\n# Need a useful data frame for basic aggregations\ngroupedStats_byDay &lt;- dplyr::tbl(dbconn_fltrd,\n                                 tblPath_fltrd) |&gt;\n    dplyr::select(started_at,\n                  member_casual,\n                  rideable_type,\n                  trip_time,\n                  miles,\n                  mph) |&gt;\n    dplyr::collect() |&gt;\n    dplyr::mutate(started_at = as.Date(started_at)) |&gt;\n    dplyr::group_by(started_at,\n                    member_casual,\n                    rideable_type) |&gt;\n    dplyr::summarize(n = dplyr::n(),\n                     trip_time_Mean = mean(trip_time),\n                     trip_time_stDev = stats::sd(trip_time),\n                     miles_Mean = mean(miles),\n                     miles_stDev = stats::sd(miles),\n                     mph_Mean = mean(miles),\n                     mph_stDev = stats::sd(mph))\n\n\n\n\nCode\n#|label: 'grouped stats tsibb'\n\n# Would prefer to work with a tsibble for time-series data\ngroupedStats_tsib &lt;- groupedStats_byDay |&gt;\n    tsibble::as_tsibble(key = c(member_casual:mph_stDev),\n                        index = started_at) |&gt;\n    dplyr::arrange(started_at)\n\n\n\n\nCode\n# A solution to help visualize these mutli-dimensional relationships of membership and bike type to time.\ngroupedStats_tsib |&gt;\n    ggplot2::ggplot(ggplot2::aes(x = started_at,\n                                 y = trip_time_Mean,\n                                 color = trip_time_Mean)) +\n    ggplot2::geom_count(size = 3) +\n    ggplot2::facet_wrap(~member_casual+rideable_type) +\n    ggplot2::scale_x_date(date_minor_breaks = \"days\",\n                          date_labels = \"%b\",\n                          breaks = \"months\") +\n    ggplot2::theme(axis.text.x = ggplot2::element_text(angle = 45,\n                                                       hjust = 1),\n                   panel.background = ggplot2::element_rect(fill = 'grey20'),\n                   panel.grid.major.y = ggplot2::element_line(\n                       linetype = 'dashed',\n                       color = 'grey30'),\n                   panel.grid.major.x = ggplot2::element_line(\n                       linetype = 'dotted',\n                       color = 'grey30'),\n                   panel.grid.minor = ggplot2::element_blank(),\n                   strip.background.x = ggplot2::element_rect(\n                       fill = 'yellowgreen')) +\n    ggplot2::scale_color_distiller(palette = 'Spectral')\n\n\n\n\n\n\n\n\n\n\n\nCode\n#|label: \"dygraph\"\n\nsumDF &lt;- groupedStats_byDay |&gt;\n    dplyr::select(started_at,\n                  n) |&gt;\n    dplyr::group_by(started_at,\n                    member_casual) |&gt;\n    dplyr::summarise(\"count\" = sum(n)) |&gt;\n    tidyr::pivot_wider(names_from = member_casual,\n                       values_from = count)\n\ndygraphs::dygraph(sumDF, main = \"ever\",\n                  ylab = \"what\") |&gt;\n    dygraphs::dyGroup(name = c(\"member\", \"casual\"),\n                       color = c(\"green\", \"red\"))\n\n\n\n\n\n\nThese maps track bike-sharing activity going on worldwide and in Chicago.\nBike Share Map (“Bike Share Map: Chicago (Divvy),” n.d.)\nCityBikes: bike sharing networks around the world (“CityBikes: Bike Sharing Networks Around the World,” n.d.)\n\n\nCode\n#|label: 'drops duckDB tables'\n\n# a chunk for easily dropping either all (default) or specific tables from the ddb. Also made it so you can see the current tables and then the tables after running the chunk.\n\npaths &lt;- duckdb::dbListTables(dbconn)\n\npaths[]\n\ndrops_tables &lt;- function(path) {\n    \n    duckdb::dbRemoveTable(dbconn, \n                          path)\n}\n\npaths[] |&gt; purrr::walk(drops_tables)\n\npaths &lt;- duckdb::dbListTables(dbconn)\n\npaths[]\n\n\n\n\nCode\n#|label: 'duckDB Shutdown'\nduckdb::dbDisconnect(dbconn_fltrd, shutdown = TRUE)\nduckdb::dbDisconnect(dbconn, shutdown = TRUE)"
  },
  {
    "objectID": "bikeShare.html#semi-parametric-and-non-parametric-methods",
    "href": "bikeShare.html#semi-parametric-and-non-parametric-methods",
    "title": "BikeShare",
    "section": "Semi-parametric and non-parametric methods",
    "text": "Semi-parametric and non-parametric methods\nDescription\n\nTo predict the outcome variable using independent variables\n\noutcome variable(s) to test\n\nmember_casual\n\n\n\nStatistical methods\n\nBinary Logistic regression analysis\n\nData type\n\nOutcome variable:\n\nCategorical (&gt;= 2 categories)\n\nmember_casual\nrideable_type\n\n\nIndependent variable(s):\n\nCategorical (&gt;= 2 categories) or\n\nhour\n\n24 categories\n\nday_of_week\n\n7 categories\n\nmonth\n\n12 categories\n\nholiday\n\n2 categories (yes or no?)\n\nmember_casual\n\n2 categories\n\nrideable_type\n\n2 categories\n\n\nContinuous or\n\ntrip_time\nmph\nmiles\ngeospatial location\n\nboth\n\nmember_causal\nrideable_type\nhour\nday\ntrip_time\nmph\nmiles\ngeospatial location\n\n\n\n\nMean &gt; Median\n“Right-Skewed Histogram”\n- The right side of the histogram plot has lower frequencies than the left.\nUnimodal\n\n\nCode\nlatsTable &lt;- dplyr::tbl(dbconn,\n                        tblPath_fltrd,\n                        check_from = FALSE) |&gt;\n    dplyr::select(start_lat) |&gt;\n    dplyr::collect() |&gt;\n\n    # because 5 digits after decimal place gives down to 1.1m accuracy\n    # 4 digits are accurate up to 11m\n    dplyr::mutate(start_lat = signif(start_lat, digits = 5),\n                  .keep = \"none\") |&gt;\n    table() |&gt;\n    as.data.frame()\n\n\n\n\nCode\nlongsTable &lt;- dplyr::tbl(dbconn,\n                       tblPath_fltrd,\n                       check_from = FALSE) |&gt;\n    dplyr::select(start_lng) |&gt;\n    dplyr::collect() |&gt;\n    # because 5 digits after decimal place gives down to 1.1m accuracy\n    # 4 digits are accurate up to 11m\n    dplyr::mutate(start_lng = signif(start_lng, digits = 5),\n                  .keep = \"none\") |&gt;\n    table() |&gt;\n    as.data.frame()\n\n\n\n\nCode\nlatlngTable &lt;- dplyr::tbl(dbconn,\n                       tblPath_fltrd,\n                       check_from = FALSE) |&gt;\n    dplyr::select(start_lat, start_lng) |&gt;\n    # because 5 digits after decimal place gives down to 1.1m accuracy\n    # 4 digits are accurate up to 11m\n    dplyr::collect() |&gt;\n    dplyr::mutate(start_lat = round(start_lat, digits = 3),\n                  start_lng = round(start_lng, digits = 3),\n                  .keep = \"none\") |&gt;\n    dplyr::group_by(start_lat, start_lng) |&gt;\n    #dplyr::collect() |&gt;\n    table() |&gt;\n    as.data.frame()\n\nlatlngTable |&gt;\n    head(n = 10)\n\n\n   start_lat start_lng Freq\n1     41.649   -87.844    0\n2      41.65   -87.844    0\n3     41.652   -87.844    0\n4     41.654   -87.844    0\n5     41.655   -87.844    0\n6     41.657   -87.844    0\n7     41.658   -87.844    0\n8     41.659   -87.844    0\n9      41.66   -87.844    0\n10    41.666   -87.844    0\n\n\n\n\nCode\n# Trying to get clues as to how we should alter the table. \ncoordsTable &lt;- dplyr::tbl(dbconn,\n                       tblPath_fltrd,\n                       check_from = FALSE) |&gt;\n    dplyr::select(start_lat, \n                  start_lng) |&gt;\n    # because 5 digits after decimal place gives down to 1.1m accuracy\n    # 4 digits are accurate up to 11m\n   # dplyr::filter(start_lat, signif(start_lat, digits = 7) == TRUE) |&gt;\n    #dplyr::collect() |&gt;\n    dplyr::mutate(start_lat = as.character(start_lat),\n                  start_lng = as.character(start_lng),\n                  .keep = \"none\") |&gt;\n    dplyr::filter(stringr::str_length(start_lat) &gt;= 8,\n                  # 9 to account for negative symbol in lng\n                  stringr::str_length(start_lng) &gt;= 9) |&gt;\n    dplyr::group_by(start_lat, \n                    start_lng) |&gt;\n    dplyr::summarise(n = dplyr::n()) |&gt;\n    dplyr::arrange(n) |&gt;\n    dplyr::collect() |&gt;\n    as.data.frame()\n\n\n\n\nCode\n# Trying to get clues as to how we should alter the table. \ncoordsTable2 &lt;- dplyr::tbl(dbconn,\n                           tblPath_fltrd,\n                           check_from = FALSE) |&gt;\n    dplyr::select(start_lat, start_lng) |&gt;\n    # because 5 digits after decimal place gives down to 1.1m accuracy\n    # 4 digits are accurate up to 11m\n    dplyr::mutate(start_lat = round(start_lat, digits = 5),\n                  start_lng = round(start_lng, digits = 5),\n                  start_lat = as.character(start_lat),\n                  start_lng = as.character(start_lng),\n                  .keep = \"none\") |&gt;\n    dplyr::filter(stringr::str_length(start_lat) &gt;= 8,\n                  stringr::str_length(start_lng) &gt;= 9) |&gt;\n    dplyr::group_by(start_lat, start_lng) |&gt;\n    dplyr::summarise(n = dplyr::n()) |&gt;\n  #  dplyr::filter(n &gt;= 47) |&gt;\n    dplyr::arrange(n) |&gt;\n    dplyr::collect() |&gt;\n    # seems base R's table() is a lot slower than dplyr\n    #table() |&gt;\n    as.data.frame()\n\ncoordsTable2 |&gt;\n    head(n = 10)\n\n\n   start_lat start_lng n\n1   41.88191 -87.63967 1\n2   41.92545 -87.65371 1\n3   41.87619 -87.62996 1\n4   41.88297 -87.64164 1\n5   41.88061 -87.63005 1\n6   41.89885 -87.63005 1\n7   41.78798 -87.58853 1\n8   41.88584 -87.63073 1\n9   41.88442 -87.62986 1\n10  41.88609 -87.62327 1\n\n\nAssuming their plans to expand the “over 800 stations” by 250, I’d like to set the parameters to achieve around ~1050 rows. So I set n to include groups with at least 47 riders.\n\n\nCode\n# according to this, there are 1487 unique station id's in the dataset now\ndplyr::tbl(dbconn,\n           tblPath_fltrd,\n           check_from = FALSE) |&gt;\n    dplyr::select(start_station_name) |&gt;\n    dplyr::distinct(start_station_name) |&gt;\n    dplyr::summarise(n = dplyr::n())\n\n\n# Source:   SQL [1 x 1]\n# Database: DuckDB v0.10.0 [ecmos@Windows 10 x64:R 4.3.3/db/data.db]\n      n\n  &lt;dbl&gt;\n1  1487\n\n\nThe 5th decimal place is accurate up to 1.1m, so there ends up being groupings for bikes rented from stations located along a street or at intersections of two streets. You can tell by the start_station_name. If it’s an intersection it will say something like “1st St & 2nd St.”. If it’s one street, it will likely just say “1st St”. There are likely some exceptions to that rule. That helps explain why the geospatial coordinates slightly differ for entries with the same stations and IDs.\n\n\nCode\nmapTest &lt;- dplyr::tbl(dbconn,\n           tblPath_fltrd,\n           check_from = FALSE) |&gt;\n    dplyr::select(start_station_id,\n                  start_station_name,\n                  start_lat,\n                  start_lng) |&gt;\n    dplyr::group_by(start_station_name,\n                    start_station_id,\n                    start_lat,\n                    start_lng) |&gt;\n    dplyr::summarise(n = dplyr::n()) |&gt;\n    dplyr::filter(stringr::str_detect(\n        start_station_name,\n        \"Ogden Ave & Chicago Ave\") == TRUE,\n        n &gt; 1) |&gt;\n    dplyr::arrange(start_station_name, \n                   start_station_id, \n                   start_lat,\n                   start_lng,\n                   n) |&gt;\n    dplyr::collect()\n\nmapTest |&gt;\n    dplyr::ungroup() |&gt;\n    dplyr::select(start_lng, start_lat, n) |&gt;\n   # dplyr::group_by(start_lng, start_lat) |&gt;\n    sf::st_as_sf(coords = c(1:2),\n             crs = 4326) |&gt;\n    mapview::mapview()\n\n\nA quick test of the data:\n\n\nCode\ncoordsDF &lt;- mapTest |&gt;\n    dplyr::ungroup() |&gt;\n    dplyr::select(start_lng,\n                  start_lat) |&gt;\n    dplyr::add_count(start_lng,\n                     start_lat) |&gt;\n    dplyr::slice_head(n = 100)\n\ncoordsDF |&gt;\n    sf::st_as_sf(coords = c(1:2),\n                 crs = 4326) |&gt;\n    mapview::mapview()\n\n\n\n\nCode\n# according to this, there are 1428 unique station IDs in the dataset now\ncountStations &lt;- dplyr::tbl(dbconn,\n                            tblPath,\n                            check_from = FALSE) |&gt;\n    dplyr::select(start_station_id, start_station_name) |&gt;\n    #dplyr::distinct(dplyr::pick(start_station_id, start_station_name)) |&gt;\n    dplyr::group_by(start_station_id,\n                    start_station_name) |&gt;\n    dplyr::summarise(n = dplyr::n()) |&gt;\n    dplyr::filter(n &gt; 0) |&gt;\n    dplyr::collect()\n\nsummary(countStations)\n\n\n\n\nCode\nmemberCasuals_monthly  &lt;- dplyr::tbl(dbconn,\n           tblPath_fltrd) |&gt;\n    dplyr::select(started_at,\n                  member_casual) |&gt;\n    dplyr::mutate('month' = lubridate::month(started_at)) |&gt;\n    dplyr::group_by(month,\n                    member_casual) |&gt;\n    dplyr::summarize(\"riderCount\" = dplyr::n()) |&gt;\n    dplyr::arrange(month)\n\ndplyr::collect(memberCasuals_monthly) |&gt;\n    ggplot2::ggplot() +\n    ggplot2::geom_col(mapping = ggplot2::aes(x = factor(month),\n                                             y = riderCount,\n                                             fill = member_casual),\n                      color = \"black\",\n                      position = 'dodge2') +\n    ggplot2::scale_x_discrete(labels = month.abb,\n                              name = \"Month\") +\n    ggplot2::scale_fill_brewer(palette = 'Set2') +\n    ggplot2::theme_dark() +\n    ggplot2::labs(\n    title = \"Monthly Ridership: Members vs Casuals\",\n    subtitle = \"(Jan-Dec 2023)\",\n    caption = \"Data from cyclistic database.\",\n    tag = \"Figure 1.b\")\n\n\n\n\n\n\n\n\n\nfor quick reference with using Tsibble syntax\n\n\nCode\n#|label: 'grouped tibb'\n\ngrouped_byDay &lt;- dplyr::tbl(dbconn,\n                            tblPath_fltrd) |&gt;\n    dplyr::select(started_at,\n                  member_casual) |&gt;\n    dplyr::collect() |&gt;\n    dplyr::mutate(started_at = as.Date(started_at)) |&gt;\n    dplyr::group_by(started_at,\n                    member_casual) |&gt;\n    dplyr::summarize(n = dplyr::n(),\n                     sdev = stats::sd(n))\n\n\n\n\nCode\n#|label: 'to grouped tsibb'\n\n# tsibble, time-series table/tibble seems to make time series plots more straightforward\ngrouped_tsi &lt;- grouped_byDay |&gt;\n    tsibble::as_tsibble(key = c(member_casual,\n                                n),\n                        index = started_at) |&gt;\n    dplyr::arrange(started_at)\n\n\n\n\nCode\n#|label: \"map query setup\"\n\n# chicago starting coordinates for leaflet, setView\nchicago &lt;- maps::us.cities |&gt;\n    dplyr::select(\"name\",\n                  \"long\",\n                  \"lat\") |&gt;\n    dplyr::filter(name == \"Chicago IL\")\n\n# full dataset coordinates, might need to sample\ncoordQry &lt;- dplyr::tbl(dbconn,\n                       tblPath_fltrd) |&gt;\n    dplyr::select(start_lng,\n                  start_lat) |&gt;\n    dplyr::add_count(start_lng,\n                     start_lat) |&gt;\n    dplyr::distinct() |&gt;\n    dplyr::arrange(desc(n)) |&gt;\n    dplyr::collect()\n\n\ncoordQry_small &lt;- dplyr::tbl(dbconn,\n                             tblPath_fltrd) |&gt;\n    dplyr::select(start_lng,\n                  start_lat) |&gt;\n    dplyr::add_count(start_lng,\n                     start_lat) |&gt;\n    dplyr::distinct() |&gt;\n    dplyr::arrange(desc(n)) |&gt;\n    dplyr::collect() |&gt;\n    dplyr::slice_head(n = 50)\n\n\n\n\nCode\n#|label: \"mapview\"\n#|eval: false\n\n\ncoordQry_small |&gt;\n    sf::st_as_sf(coords = c(1:2),\n                crs = 4326) |&gt;\n    mapview::mapview()\n\n\n\n\n\n\n\n\nCode\nojs_define(js_tsi = grouped_tsi)\n\n\n\n\nCode\njsData = transpose(js_tsi)\n\n\n\n\n\n\n\n\n\n\nCode\nPlot.plot({\n    grid: true,\n    color: {legend: true},\n    marks: [\n        Plot.dot(jsData, {x: 'started_at', y: 'n', fill: 'member_casual'})\n]\n})\n\n\n\n\n\n\n\n\n\n\nCode\nPlot.lineY(jsData, {x: \"started_at\", y: \"n\"}).plot()\n\n\n\n\n\n\n\n\n\nCode\nlocsStart &lt;- dplyr::tbl(dbconn,\n                      tblPath_fltrd) |&gt;\n    dplyr::select(start_station_name\n                  #start_lng,\n                  #start_lat\n                  ) |&gt;\n    dplyr::distinct() |&gt;\n    dplyr::arrange(start_station_name) |&gt;\n    dplyr::rename(\"station_name\" = start_station_name\n                 # \"lng\" = start_lng,\n                 # \"lat\" = start_lat\n                 )\n\nlocsEnd &lt;- dplyr::tbl(dbconn,\n                      tblPath_fltrd) |&gt;\n    dplyr::select(end_station_name\n                  #end_lng,\n                  #end_lat\n                  ) |&gt;\n    dplyr::distinct() |&gt;\n    dplyr::arrange(end_station_name) |&gt;\n    dplyr::rename(\"station_name\" = end_station_name\n                  #\"lng\" = end_lng,\n                  #\"lat\" = end_lat\n                  )\n\nlocs &lt;- locsStart |&gt;\n    dplyr::left_join(locsEnd) |&gt;\n    dplyr::arrange(station_name)\n\nlocs\n\n\n# Source:     SQL [?? x 1]\n# Database:   DuckDB v0.10.0 [ecmos@Windows 10 x64:R 4.3.3/db/data.db]\n# Ordered by: station_name\n   station_name              \n   &lt;chr&gt;                     \n 1 2112 W Peterson Ave       \n 2 410                       \n 3 63rd St Beach             \n 4 900 W Harrison St         \n 5 Aberdeen St & Jackson Blvd\n 6 Aberdeen St & Monroe St   \n 7 Aberdeen St & Randolph St \n 8 Ada St & 113th St         \n 9 Ada St & Washington Blvd  \n10 Adler Planetarium         \n# ℹ more rows\n\n\n\n\nCode\nflowData &lt;- dplyr::tbl(dbconn,\n                       tblPath_fltrd) |&gt;\n    dplyr::select(start_station_name,\n                  end_station_name) |&gt;\n    dplyr::group_by(start_station_name,\n                    end_station_name) |&gt;\n    dplyr::summarize(n = n()) |&gt;\n    dplyr::ungroup() |&gt;\n    dplyr::arrange(desc(n)) |&gt;\n    dplyr::rename(\"from_station\" = start_station_name,\n                  \"to_station\" = end_station_name) |&gt;\n    dplyr::collect() |&gt;\n    dplyr::slice_head(n = 50)\n\nflowData\n\n\n# A tibble: 50 × 3\n   from_station                      to_station                   n\n   &lt;chr&gt;                             &lt;chr&gt;                    &lt;dbl&gt;\n 1 Ellis Ave & 60th St               Ellis Ave & 55th St       6927\n 2 Ellis Ave & 60th St               University Ave & 57th St  6600\n 3 Ellis Ave & 55th St               Ellis Ave & 60th St       6349\n 4 University Ave & 57th St          Ellis Ave & 60th St       6168\n 5 Calumet Ave & 33rd St             State St & 33rd St        5417\n 6 State St & 33rd St                Calumet Ave & 33rd St     5343\n 7 DuSable Lake Shore Dr & Monroe St Streeter Dr & Grand Ave   4023\n 8 Loomis St & Lexington St          Morgan St & Polk St       3719\n 9 Morgan St & Polk St               Loomis St & Lexington St  3379\n10 University Ave & 57th St          Kimbark Ave & 53rd St     3112\n# ℹ 40 more rows\n\n\nCode\nsummary(flowData$n)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1363    1510    1878    2443    2599    6927 \n\n\n\n\nCode\nlocationData &lt;- dplyr::tbl(dbconn,\n                           tblPath_fltrd) |&gt;\n    dplyr::select(start_station_name,\n                  started_at,\n                  ended_at,\n                  trip_time) |&gt;\n    dplyr::group_by(start_station_name) |&gt;\n    dplyr::mutate(\"trip_time\" = round(trip_time,\n                                      digits = 0)) |&gt;\n    dplyr::summarize(\"trip_count\" = dplyr::n(),\n                     \"first_date\" = min(started_at),\n                     \"last_date\" = max(ended_at),\n                     \"avg_trip_time\" = mean(trip_time)\n                     ) |&gt;\n    dplyr::rename(\"station_name\" = start_station_name) |&gt;\n    dplyr::arrange(station_name) |&gt;\n    dplyr::collect()\n\nlocationData\n\n\n# A tibble: 1,487 × 5\n   station_name trip_count first_date          last_date           avg_trip_time\n   &lt;chr&gt;             &lt;dbl&gt; &lt;dttm&gt;              &lt;dttm&gt;                      &lt;dbl&gt;\n 1 2112 W Pete…        546 2023-01-01 14:38:58 2023-12-31 11:40:34         15.8 \n 2 410                   3 2023-04-27 08:02:30 2023-04-27 08:46:00         12.7 \n 3 63rd St Bea…        648 2023-01-06 12:59:47 2023-12-29 13:31:47         30.6 \n 4 900 W Harri…      10925 2023-01-01 02:18:29 2024-01-01 00:06:30          9.08\n 5 Aberdeen St…      12888 2023-01-01 00:56:42 2023-12-31 22:15:30          9.85\n 6 Aberdeen St…       8486 2023-01-01 08:08:19 2023-12-31 20:06:36         12.0 \n 7 Aberdeen St…       9270 2023-01-01 12:39:37 2023-12-31 14:40:04         11.6 \n 8 Ada St & 11…         19 2023-05-04 18:51:45 2023-11-29 17:03:52         13.8 \n 9 Ada St & Wa…       5874 2023-01-01 06:56:37 2023-12-31 16:10:44         10.8 \n10 Adler Plane…      12354 2023-01-01 07:42:32 2023-12-31 16:10:21         25.0 \n# ℹ 1,477 more rows\n\n\n\n\nCode\nmergd &lt;- merge(x = locationData,\n               y = locs,\n               by.x = \"station_name\",\n               by.y = \"station_name\",\n               sort = FALSE)\n\nmergd &lt;- mergd |&gt;\n    dplyr::mutate(avg_trip_time = round(avg_trip_time,\n                                    digits = 0)) |&gt;\n    dplyr::arrange(desc(trip_count)) |&gt;\n    dplyr::slice_head(n = 25)    \n\nmergd\n\n\n                         station_name trip_count          first_date\n1             Streeter Dr & Grand Ave      43175 2023-01-01 00:12:40\n2            Kingsbury St & Kinzie St      30915 2023-01-01 02:21:08\n3                   Clark St & Elm St      30597 2023-01-01 01:10:53\n4        Clinton St & Washington Blvd      28743 2023-01-01 15:28:03\n5  DuSable Lake Shore Dr & North Blvd      28386 2023-01-01 02:12:09\n6               Wells St & Concord Ln      28357 2023-01-01 01:15:27\n7               Michigan Ave & Oak St      26861 2023-01-01 00:59:17\n8                   Wells St & Elm St      25948 2023-01-01 00:59:22\n9   DuSable Lake Shore Dr & Monroe St      25055 2023-01-01 00:14:47\n10                Theater on the Lake      23893 2023-01-01 03:14:22\n11            Clinton St & Madison St      23143 2023-01-01 01:58:28\n12               Broadway & Barry Ave      22867 2023-01-01 01:48:00\n13           Wilton Ave & Belmont Ave      22612 2023-01-01 01:10:13\n14           University Ave & 57th St      22089 2023-01-01 16:42:37\n15                Ellis Ave & 60th St      21910 2023-01-01 12:01:37\n16            Clark St & Armitage Ave      21642 2023-01-01 00:23:47\n17                Wells St & Huron St      21416 2023-01-01 01:12:35\n18      Sheffield Ave & Fullerton Ave      21259 2023-01-01 00:28:38\n19             State St & Chicago Ave      21016 2023-01-01 01:03:50\n20             Clark St & Lincoln Ave      21011 2023-01-01 01:47:47\n21         Indiana Ave & Roosevelt Rd      20336 2023-01-01 02:13:09\n22                Canal St & Adams St      20302 2023-01-01 02:22:26\n23          Desplaines St & Kinzie St      20201 2023-01-01 04:25:20\n24              Dearborn St & Erie St      20182 2023-01-01 02:31:05\n25             Wabash Ave & Grand Ave      19992 2023-01-01 00:05:43\n             last_date avg_trip_time\n1  2024-01-01 00:19:01            24\n2  2023-12-31 21:30:50             9\n3  2023-12-31 23:29:33            11\n4  2023-12-31 18:03:02            11\n5  2023-12-31 23:34:53            19\n6  2023-12-31 19:23:30            12\n7  2023-12-31 16:48:31            23\n8  2023-12-31 23:51:48            10\n9  2023-12-31 16:49:56            25\n10 2023-12-31 22:53:53            21\n11 2023-12-31 23:56:48            11\n12 2023-12-31 21:18:45            12\n13 2023-12-31 21:21:06            11\n14 2023-12-31 05:54:17             8\n15 2023-12-31 09:22:22             6\n16 2023-12-31 17:54:10            14\n17 2023-12-31 23:51:50            10\n18 2023-12-31 23:43:00             9\n19 2023-12-31 19:48:26            11\n20 2023-12-31 21:09:23            14\n21 2023-12-31 23:12:35            16\n22 2023-12-31 07:16:45            12\n23 2023-12-31 21:04:13            10\n24 2023-12-31 18:34:38            11\n25 2023-12-31 23:24:43            13\n\n\n\n\nCode\n#|class-output: plotMod\n\nef_test &lt;- epiflows::make_epiflows(flows = flowData,\n                                   locations = mergd,\n                                   duration_stay = \"avg_trip_time\",\n                                   num_cases = \"trip_count\")\n\nepiflows::vis_epiflows(ef_test)\n\n\n\n\n\n\nThese maps track bike-sharing activity going on worldwide and in Chicago.\nBike Share Map (“Bike Share Map: Chicago (Divvy),” n.d.)\nCityBikes: bike sharing networks around the world (“CityBikes: Bike Sharing Networks Around the World,” n.d.)\n\n\nCode\n#|label: 'drops duckDB tables'\nsource(\"duckDrops.R\")\n\n\n\n\nCode\n#|label: 'duckDB Shutdown'\nduckdb::dbDisconnect(dbconn, shutdown = TRUE)\nunlink(\"db\",\n       recursive = TRUE)\n\n\n\n\n\nSource Code\n---\ntitle: \"BikeShare\"\nauthor: \"Eric Mossotti\"\n\nbibliography: references.bib\nrepo: https://github.com/ericMossotti/Bike_Share\nsource: bikeShare.qmd\n\ncode-links:\n    - text: \"Project Repo\"\n      href: repo\n      \ncode-fold: true\ncode-copy: hover\ncode-overflow: wrap\ncode-tools: true\n\ntoc: true\ntoc_float: false\nsmooth-scroll: true\n\nfig-responsive: true\necho: true\n\n#sass: styles.sass\n\n#font: merriweather, futura\n---\n```{r, include = FALSE}\n\nknitr::opts_chunk$set(message = FALSE, \n                      warning = FALSE)\n```\n:::grid\n:::g-col-1 \n\n\n\n\n:::\n \n:::g-col-5\n\n# Case Study: Bike-Sharing in Chicago\n\n## Objective:\n\nCommunicate to shareholders data based insights regarding to better inform their desire to enlist more annual subscribers.\n\n# First Off, Import and Clean\n\nI've condensed the initial download, import and cleaning steps to reduce the complexity of this Quarto document. Thinking ahead, with reproducibility in mind, this code chunk should cover most use cases for tinkering and testing. I've personally found it helpful to reduce the need to re-download files and re-process all over again if all one needs to do is reconnect to the database that has already been written. Choosing a persistent DuckDB database filesystem (as opposed to in-memory) was intentional as I wouldn't lose the progress I've made when tinkering over multiple days. It seems just as fast as the in-memory database but also seems to reduce RAM needed in tinkering.\n\nOverall, I think a locally persistent DuckDB solution was an ideal decision on most levels. Desktops and laptop systems tend to have more long term, non-volatile memory, such as drives available than short-term, volatile memory, such as RAM. It was interesting that the db file is only one file despite how many paths are written using the initial connection. The zero-copy feature, I believe, is why it doesn't really increase much in size despite writing multiple processed tables, even if they are of similar length to the original. I think that's because the longer ones contain much of the same data. I'll have to look more into that later in project updates.\n\nAll data was downloaded from @divvydab.\n\n```{r}\nif(exists(\"dbconn\") == FALSE) {\n    # Script to keep this document less cluttered. \n    source(\"import_clean_initial.R\")\n    } else {\n        # you will have to change original_nobs if you use different data\n        # helps with tinkering when you want to skip the import part\n        original_nobs &lt;- as.integer(5719877)\n        \n        tblPath &lt;- \"db/data.db\"\n        \n        dbconn &lt;- DBI::dbConnect(\n        duckdb::duckdb(),\n        dbdir = tblPath,\n        read_only = FALSE,\n        check_from = FALSE)\n    }\n```\n\n```{r}\n#| label: \"View import script\"\n#| code-summary: \"Data import and initial processing script.\"\n#| file: \"import_clean_initial.R\"\n#| eval: false \n```\n\nVerifying DuckDB tables:\n\n```{r}\n#| output: true\n#| code-summary: \"Verifying DuckDB tables\"\n#| message: true\n\nduckdb::dbListTables(dbconn) |&gt;\n    message()\n```\n\n## Hidden Duplicate Observations?\n\nNow to go a little deeper, we can check for duplicates. It might not necessarily be the case that each observation (obs) is unique even if all the Rider IDs are, technically, unique. \n:::\n\n:::g-col-5\n```{r}\n#| label: 'Duplicates Table with GT'\n#| code-overflow: wrap\n\n# This is a separate table used to analyze the observations returned as not distinct (n &gt; 1). This adds an extra column labeled \"n\".\ndupeTable &lt;- dplyr::tbl(dbconn,\n                        tblPath,\n                        check_from = FALSE) |&gt;\n    dplyr::select(started_at:end_station_name) |&gt;\n    # Counts of unique rows added for column 'n'\n    dplyr::add_count(started_at,\n                     ended_at,\n                     start_station_name,\n                     end_station_name) |&gt;\n    # Only observations that have been duplicated 1 or more\n    # times are shown\n    dplyr::filter(n &gt; 1) |&gt;\n    # We want to see all rows, not just one row for each obs\n    dplyr::ungroup() |&gt;\n    dplyr::arrange(started_at) |&gt;\n    dplyr::collect()\n\n\ngtDupes &lt;- dupeTable |&gt;\n    dplyr::group_by(started_at) |&gt;\n    gt::gt(rowname_col = \"row\",\n           groupname_col = \"started_at\",\n           row_group_as_column = TRUE,\n           caption = \"Duplicates_Table1\") |&gt;\n    gt::tab_style(\n    style = list(\n        gt::cell_text(weight = \"bold\",\n                      align = \"center\"),\n        gt::cell_borders(sides = c(\"bottom\"))\n    ),\n    locations = gt::cells_column_labels(gt::everything())\n    ) |&gt;\n    gt::tab_style(\n    style = list(\n        gt::cell_borders(sides = c(\"left\", \"right\")),\n        gt::cell_text(align = \"center\",\n                      v_align = \"middle\")\n    ),\n    locations = gt::cells_body(gt::everything())\n    ) |&gt;\n    gt::data_color(columns = start_station_name,\n                   target_columns = gt::everything(),\n                   method = \"auto\",\n                   palette = \"basetheme::brutal\") |&gt;\n    gt::tab_source_note(gt::md(\"**Source**: Divvy Data\")) |&gt;\n    gt::tab_header(title = \"Duplicate Observations\",\n                   subtitle = \"(by start date)\") |&gt;\n    gt::tab_options(\n        heading.title.font.weight = \"bolder\",\n        heading.subtitle.font.weight = \"lighter\",\n        table.layout = \"auto\"\n        )\n\n\ngtDupes\n\n```\n:::\n\n:::g-col-1\n\n\n\n\n\n\n:::\n:::\n\n\n\nOf the other columns, it seems that the start_time, end_time, start_station, and end_station could show if there are possibly hidden duplicated observations.\n\nWe started with 5,719,877 observations (obs) for dates spanning January to December, 2023, then removed 1,388,170 incomplete obs.\n\nI assumed that having the same times/dates and stations for two different ride IDs was a mistake. Although, I do not know how that error would happen. I could have assumed one person could check out multiple bikes at once. In that instance, each bike could be assigned a unique ride_id. That, however, has only happened 18 times over a year. Since it's only one copy every time, that also raises a red flag. I did not notice any other correlations with station_id/name, member_casual, or ride_type for those particular duplicated data.\n\n```{r}\n#| label: 'output to distinct duplicates and total obs'\n\ndistinctCopiesCount &lt;- dupeTable |&gt;\n    dplyr::distinct(n) |&gt;\n    as.integer() \n\nduplicateObs &lt;- length(dupeTable[[1]])\n```\n\nBy applying distinct() on dupeTable, we see the only distinct value is 2. We can safely conclude that, of the duplicates, each has a minimum and maximum of 1 extra copy.\n\nNumber of rows in the dupeTable is 36. Because each duplicated observation has one duplicate (n = 2), expected removed nobs is 18. The issue is that we need to get rid of not all 36 rows, but just the 1 extra duplicate observation from each, resulting in the expected 18.\n\n```{r}\n#|label: 'create un-duped table, count rows'\n\n# The issue is, we need to get rid of not all of these rows, but just the extra duplicate observations. \n\n# If there were 2 rows of duplicates, we would want to end up with 1 row after removing the extras.\nundupedTable &lt;- dupeTable |&gt;\n    dplyr::distinct(started_at,\n                     start_station_name,\n                     ended_at,\n                     end_station_name,\n                     .keep_all = TRUE)\n\ndistinctUndupedCounts &lt;- undupedTable |&gt;\n    dplyr::select(started_at) |&gt;\n    dplyr::distinct() |&gt;\n    dplyr::count() |&gt;\n    as.integer()\n\nundupedTable\n```\n\nThe count of observed distinct values for the un-duplicated table was indeed 18. So now, it is time to run a count of how rows/observations are in the dataset. There is a difference, though, concerning the correct amount:\n\n```{r}\n#|label: 'incorrect/correct distinct observations'\n\n# Run an incorrect count on how many rows or observations there are in the dataset.\ncount_incorrectDists &lt;- dplyr::tbl(dbconn,\n                                tblPath,\n                                check_from = FALSE) |&gt;\n    dplyr::distinct(dplyr::pick(\"ride_id\")) |&gt;\n    dplyr::count(name = \"Incorrect Distinct Observations\") |&gt;\n    dplyr::collect() |&gt;\n    as.integer()\n\n# For the correct count of obs\ncount_correctDists &lt;- dplyr::tbl(dbconn,\n                              tblPath,\n                              check_from = FALSE) |&gt;\n    dplyr::distinct(\n        dplyr::pick(\n            \"started_at\",\n            \"start_station_name\",\n            \"ended_at\",\n            \"end_station_name\"\n        )\n    ) |&gt;\n    dplyr::count() |&gt;\n    dplyr::collect() |&gt;\n    as.integer()\n```\n\n```{r}\n#| label: 'duplicate correction tibb'\n#| fig-cap: \"Summary of observations removed by processing.\"\n#| fig-cap-location: margin\n#| column: body-outset\n\n# To visualize a summary of what we just determined regarding obs\ntidyr::tribble(~\"\",~\"Counts\",\n    \"Original observations\", original_nobs,\n    \"Observations with duplicates\", count_incorrectDists,\n    \"Duplicate observations removed\", \n    (count_incorrectDists - count_correctDists),\n    \"Corrected observations\", count_correctDists\n) \n```\n\nThe incorrect number of observations (nobs) was 4,331,707. The correct nobs after removing duplicated obs was 4,331,689. In short, 18 additional obs were removed.\n\n```{r}\n#| label: 'overwrite file with correct obs'\n#| column: body-outset\n \ndplyr::tbl(dbconn,\n           tblPath,\n           check_from = FALSE) |&gt;\n    dplyr::select(ride_id:trip_time) |&gt;\n    dplyr::distinct(started_at,\n                    start_station_name,\n                    ended_at,\n                    end_station_name,\n                    .keep_all = TRUE) |&gt;\n    dplyr::arrange(started_at) |&gt;\n    dplyr::collect() |&gt;\n    duckdb::dbWriteTable(conn = dbconn,\n                         name = tblPath,\n                         overwrite = TRUE,\n                         check_from = FALSE)\n\ndplyr::tbl(dbconn,\n           tblPath) |&gt;\n    head()\n```\n\n## Filtering Data, Smartly\n\nTo ensure the conclusions are accurate, outliers should be filtered. Negative and very low trip times might skew trends. The underlying reason for very low trip times is somewhat of an unknown. Perhaps people often change their minds?\n\n```{r}\n#| label: \"filtering db\"\n\n# So you don't have to re-download or re-filter everything after making further adjustments.\n\ntblPath &lt;- \"db/data.db\"\ntblPath_fltrd &lt;- \"db/data_fltrd.db\"\n\nif(exists(\"dbconn\") == FALSE) {\n    dbconn &lt;- DBI::dbConnect(duckdb::duckdb(),\n                             dbdir = tblPath,\n                             read_only = FALSE,\n                             check_from = FALSE)\n}\n\nif (duckdb::dbExistsTable(dbconn,\n                          \"tblPath_fltrd\") == FALSE) {\n    source(\"filterDatabase.R\")\n    filterDatabase()\n    }\n\n# To verify the new filtered table exists.\nduckdb::dbListTables(dbconn)\n```\n\n```{r}\n#|eval: FALSE\n\n# If you need to drop any tables\nsource(\"duckDrops.R\")\n```\n\nSo this should have removed the major, potentially, non-natural outliers from the dataset which are due to errors (including user errors).\n\nVerify the tables that now exist:\n\n```{r}\nduckdb::dbListTables(dbconn)\n```\n\n# Frequency Tables and What They Tell Us\n\nNow we're going to create dimension and frequency tables and add those to our database. We can retain the outliers in those tables and perhaps filter as needed later. This is a good place start. If needed, we can dive deeper into other statistical techniques or adjust parameters in these code chunks and overwrite or create new db tables.\n\nBy generating frequency tables, I quickly glean insights from the data.\n\n```{r}\n# Member_casual\ndplyr::tbl(dbconn,\n           tblPath_fltrd,\n           check_from = FALSE) |&gt;\n    dplyr::select(member_casual) |&gt;\n    dplyr::group_by(member_casual) |&gt;\n    dplyr::summarize(n = dplyr::n()) |&gt;\n    dplyr::collect() |&gt;\n    duckdb::dbWriteTable(conn = dbconn,\n                         name = \"db/freq_member.db\",\n                         overwrite = TRUE,\n                         check_from = FALSE)\n\ndplyr::tbl(dbconn,\n           \"db/freq_member.db\")\n\n```\n\nThis indicates that there are nearly twice as many trips taken by annual subscribers than by casual users. So, it would seem that there is some correlation between how often a person uses the service and whether they choose to subscribe to an annual membership plan.\n\n```{r}\n# Rideable_type \ndplyr::tbl(dbconn,\n           tblPath_fltrd,\n           check_from = FALSE) |&gt;\n    dplyr::select(rideable_type) |&gt;\n    dplyr::group_by(rideable_type) |&gt;\n    dplyr::summarize(n = dplyr::n()) |&gt;\n    dplyr::collect() |&gt;\n    duckdb::dbWriteTable(conn = dbconn,\n                         name = \"db/freq_rTypes.db\",\n                         overwrite = TRUE,\n                         check_from = FALSE)\n\ndplyr::tbl(dbconn,\n           \"db/freq_rTypes.db\")\n```\n\nThis tells us that there are nearly twice as many trips taken with non-electric bikes as electric bikes. There are differences in cost for casual members, so that might be one determining factor. Also, perhaps the associated health benefits with exercise would help explain higher non-electric bicycle use.\n\n```{r}\nduckdb::dbListTables(dbconn)\n```\n\n```{r}\n# Miles\ndplyr::tbl(dbconn,\n           tblPath_fltrd,\n           check_from = FALSE) |&gt;\n    dplyr::select(miles) |&gt;\n    dplyr::collect() |&gt;\n    dplyr::mutate(miles = dplyr::case_when(miles &gt;= 1 ~ round(miles,\n                                                              digits = 0),\n                                           miles &lt; 1 ~ round(signif(miles, 3),\n                                                             digits = 1))) |&gt;\n    dplyr::group_by(miles) |&gt;\n    dplyr::summarize(n = dplyr::n()) |&gt;\n    dplyr::arrange(miles) |&gt;\n    duckdb::dbWriteTable(conn = dbconn,\n                         name = \"db/freq_miles.db\",\n                         check_from = FALSE,\n                         overwrite = TRUE)\n\n\ndplyr::tbl(dbconn,\n           \"db/freq_miles.db\") |&gt;\n    dplyr::arrange(desc(n)) |&gt;\n    head(n = 10)\n```\n\n```{r}\ndplyr::tbl(dbconn,\n           \"db/freq_miles.db\") |&gt;\n    dplyr::collect() |&gt;\n    dplyr::mutate(n = n/1000) |&gt;\n    dplyr::rename(\"Trips (in thousands)\" = n,\n                  \"Miles\" = miles) |&gt;\n    plot()\n```\n\nThis shows that around 0.3 to 3 miles is the distance traveled on most trips taken. The fact that casual bicycles are used more often perhaps makes a little more sense given that people are not traveling long distances.\n\n```{r}\nduckdb::dbListTables(dbconn)\n```\n\n```{r}\n# MPH\ndplyr::tbl(dbconn,\n           tblPath_fltrd,\n           check_from = FALSE) |&gt;\n    dplyr::select(mph) |&gt;\n    dplyr::mutate(mph = round(mph, digits = 0)) |&gt;\n    dplyr::group_by(mph) |&gt;\n    dplyr::summarize(n = dplyr::n()) |&gt;\n    dplyr::collect() |&gt;\n    dplyr::arrange(mph) |&gt;\n    duckdb::dbWriteTable(conn = dbconn,\n                         name = \"db/freq_mph.db\",\n                         check_from = FALSE,\n                         overwrite = TRUE)\n\ndplyr::tbl(dbconn,\n           \"db/freq_mph.db\")\n```\n\n```{r}\ndplyr::tbl(dbconn,\n           \"db/freq_mph.db\") |&gt;\n    dplyr::collect() |&gt;\n    plot()\n\n```\n\nThe most common overall mph for trips fall in between 2-11 mph. This is what we might expect for city travel.\n\n```{r}\nduckdb::dbListTables(dbconn)\n```\n\n```{r}\n# WkDay\ndplyr::tbl(dbconn,\n           tblPath_fltrd,\n           check_from = FALSE) |&gt;\n    dplyr::select(started_at) |&gt;\n    dplyr::mutate(wkday = lubridate::wday(started_at)) |&gt;\n    dplyr::group_by(wkday) |&gt;\n    dplyr::summarize(n = dplyr::n()) |&gt;\n    dplyr::arrange(wkday) |&gt;\n    dplyr::collect() |&gt;\n    dplyr::mutate(wkday = c(\"Sun\",\n                            \"Mon\",\n                            \"Tue\",\n                            \"Wed\",\n                            \"Thu\",\n                            \"Fri\",\n                            \"Sat\"),\n                  wkday = forcats::as_factor(wkday),\n                  wkday = forcats::fct_inorder(wkday)) |&gt;\n    duckdb::dbWriteTable(conn = dbconn,\n                         name = \"db/freq_wkDay.db\",\n                         check_from = FALSE,\n                         overwrite = TRUE)\n\ndplyr::tbl(dbconn,\n           \"db/freq_wkday.db\") \n```\n\n```{r}\ndplyr::tbl(dbconn,\n           \"db/freq_wkday.db\") |&gt;\n    dplyr::collect() |&gt;\n    plot()\n```\n\nThere is a slight reduction in trips taken on Sundays and Mondays. This could be explained by the fact that most trips are taken by annual subscribers, possibly because they are biking to work and such. Much less people will be biking to work on Sundays. Taking 3-day weekends is common in the workplace, where most would likely opt to use up their sick/vacation days on Monday, which might explain the down tick in trips taken on Mondays. Also, those who are working Tuesdays-Saturdays could be taking Sunday and Mondays off.\n\n```{r}\n# Months\ndplyr::tbl(dbconn,\n           tblPath_fltrd,\n           check_from = FALSE) |&gt;\n    dplyr::select(started_at) |&gt;\n    dplyr::mutate(months = lubridate::month(started_at,\n                                            label = FALSE,\n                                            abbr = TRUE\n                                            )) |&gt;\n    dplyr::group_by(months) |&gt;\n    dplyr::summarize(n = dplyr::n()) |&gt;\n    dplyr::collect() |&gt;\n    dplyr::arrange(months) |&gt;\n    dplyr::mutate(months = c(month.abb),\n                  months = forcats::as_factor(months),\n                  months = forcats::fct_inorder(months)) |&gt;\n    duckdb::dbWriteTable(conn = dbconn,\n                         name = \"db/freq_month.db\",\n                         check_from = FALSE,\n                         overwrite = TRUE)\n\ndplyr::tbl(dbconn,\n           \"db/freq_month.db\")\n\ndplyr::tbl(dbconn,\n           \"db/freq_month.db\") |&gt;\n    dplyr::summarize(sum = sum(n))\n```\n\n```{r}\ndplyr::tbl(dbconn,\n           \"db/freq_month.db\") |&gt;\n    dplyr::collect() |&gt;\n    plot()\n```\n\nThe monthly ridership does differ substantially between certain months. Summer months (relative to North America) is when we see the largest increase in ridership. Spring and Fall seasons lie in between Summer and Winter in terms of scale of trips taken\n\n```{r}\ndplyr::tbl(dbconn,\n           tblPath_fltrd) |&gt;\n    dplyr::mutate(trip_time = round(trip_time,\n                                    digits = 0)) |&gt;\n    dplyr::group_by(trip_time) |&gt;\n    dplyr::summarize(n = dplyr::n()) |&gt;\n    dplyr::arrange(trip_time) |&gt;\n    dplyr::collect() |&gt;\n    duckdb::dbWriteTable(conn = dbconn,\n                         name = \"db/freq_tripTime.db\",\n                         check_from = FALSE,\n                         overwrite = TRUE)\n\ndplyr::tbl(dbconn,\n           \"db/freq_tripTime.db\") |&gt;\n    dplyr::collect() |&gt;\n    dplyr::arrange(desc(n)) |&gt;\n    head(n = 20)\n\ndplyr::tbl(dbconn,\n           \"db/freq_tripTime.db\") |&gt;\n    dplyr::summarize(sum = sum(n))\n\n\n\n```\n\n```{r}\ndplyr::tbl(dbconn,\n           \"db/freq_tripTime.db\") |&gt;\n    dplyr::collect() |&gt;\n    plot()\n```\n\nThe time riders are usually spending on these trips lies in between 5-15 minutes.\n\n```{r}\ndplyr::tbl(dbconn,\n           tblPath_fltrd) |&gt;\n    dplyr::select(start_station_name) |&gt;\n    dplyr::group_by(start_station_name) |&gt;\n    dplyr::summarize(n = dplyr::n()) |&gt;\n    dplyr::arrange(start_station_name) |&gt;\n    dplyr::collect() |&gt;\n    duckdb::dbWriteTable(conn = dbconn,\n                         name = \"db/freq_startNames.db\",\n                         check_from = FALSE,\n                         overwrite = TRUE)\n\n\ndplyr::tbl(dbconn,\n           \"db/freq_startNames.db\")\n```\n\n```{r}\ndplyr::tbl(dbconn,\n           tblPath_fltrd) |&gt;\n    dplyr::select(start_station_name,\n                  end_station_name) |&gt;\n    dplyr::group_by(start_station_name,\n                    end_station_name) |&gt;\n    dplyr::summarize(n = dplyr::n()) |&gt;\n    dplyr::arrange(start_station_name) |&gt;\n    dplyr::collect() |&gt;\n    duckdb::dbWriteTable(conn = dbconn,\n                         name = \"db/freq_pairStations.db\",\n                         check_from = FALSE,\n                         overwrite = TRUE)\n\ndplyr::tbl(dbconn,\n           \"db/freq_pairStations.db\")\n\ndplyr::tbl(dbconn,\n           \"db/freq_pairStations.db\") |&gt;\n    dplyr::summarize(sum = sum(n))\n\n```\n\n```{r}\ndplyr::tbl(dbconn,\n           tblPath_fltrd)\n```\n\n```{r}\n#|label: 'rider count by hour table'\ndplyr::tbl(dbconn,\n           tblPath_fltrd,\n           check_from = FALSE) |&gt;\n    dplyr::select(started_at) |&gt;\n    dplyr::mutate(\"hour\" = lubridate::hour(started_at)) |&gt;\n    dplyr::group_by(hour) |&gt;\n    dplyr::summarise(\"Total_Riders\" = dplyr::n()) |&gt; #or count?\n    dplyr::arrange(hour) |&gt;\n    dplyr::collect() |&gt;\n    dplyr::mutate(\"hour\" = hms::hms(hours = hour),\n                  \"hour\" = format(strptime(hour, format = \"%H\"), \"%r\"),\n                  \"index\" = seq(1:24)) |&gt;\n    duckdb::dbWriteTable(conn = dbconn,\n                         name = \"db/rideHours.db\",\n                         check_from = FALSE,\n                         overwrite = TRUE)\n```\n\n```{r}\n#| eval: true\n\nhours_of_Riders &lt;- dplyr::tbl(dbconn,\n                              \"db/rideHours.db\") |&gt;\n    dplyr::collect()\n\nx &lt;-\n    stringr::str_sub_all(hours_of_Riders[[1]],\n                         start = 1,\n                         end = 2) |&gt;\n    as.character() |&gt;\n    stringr::str_remove(pattern = \"^0\")\n\n\ny &lt;- stringr::str_sub(hours_of_Riders[[1]],\n                      start = -2,\n                      end = -1) |&gt;\n    stringr::str_to_lower()\n\nsimpleTimes &lt;- stringr::str_c(x, sep = \" \", y)\n\nhours_of_Riders[[1]] &lt;- simpleTimes\n\nhours_of_Riders\n```\n\n::: column-page\n```{r}\n#| label: 'radial-column plot'\n#| fig-column: page\n#| fig-cap: \"The time of day people tend to be riding.\"\n#| fig-cap-location: bottom\n#| title: \"Time of Day and Volume of Cyclers\"\n#| fig-width: 15\n#| fig-height: 12\n\nhoursAnimate &lt;- ggplot2::ggplot(data = hours_of_Riders,\n                mapping = ggplot2::aes(\n                    x = reorder(hour, .data$index),\n                    y = Total_Riders,\n                    fill = Total_Riders)) +\n    ggplot2::geom_col() +\n    ggplot2::coord_radial(start = 2 * pi,\n                          inner.radius = .2) +\n    ggplot2::xlab(NULL) +\n    ggplot2::ylab(NULL) +\n    ggplot2::scale_fill_distiller(palette = \"Spectral\",\n                                  direction = 1) +\n    ggplot2::labs(\n        title = \"Average Riders by the Hour of Day\",\n        subtitle = \"(Jan-Dec 2023)\",\n        caption = \"Data from cyclistic database.\",\n        tag = \"Figure 1.c\"\n        ) +\n    ggplot2::theme(\n        title = ggplot2::element_text(\n            size = 20,\n            lineheight = 4,\n            color = \"white\"\n        ),\n        \n        text = ggplot2::element_text(color = \"white\"),\n        \n        panel.background = ggplot2::element_rect(fill = \"black\"),\n        panel.grid.major.x = ggplot2::element_line(linewidth = 1,\n                                                   color = 'grey10'),\n        panel.grid.major.y = ggplot2::element_blank(),\n        \n        \n        plot.background = ggplot2::element_rect(fill = \"black\"),\n        \n        axis.line.x = ggplot2::element_line(\n            linewidth = 1,\n            color = 'grey10',\n            arrow = ggplot2::arrow()\n        ),\n        \n        axis.ticks.y = ggplot2::element_blank(),\n        \n        axis.text.x = ggplot2::element_text(\n            size = 18,\n            color = \"grey90\",\n            face = \"bold\"\n        ),\n        axis.text.y = ggplot2::element_blank(),\n        \n        legend.background = ggplot2::element_rect(fill = \"black\"),\n        legend.ticks = ggplot2::element_line(color = \"black\",\n                                             linewidth = .5),\n        legend.text = ggplot2::element_text(color = 'grey80',\n                                            size = 13),\n        legend.position = \"right\",\n        legend.justification = \"center\",\n        legend.direction = \"vertical\",\n        legend.key.size = grid::unit(1.5, \"cm\")\n       ) +\n    gganimate::transition_reveal(along = seq(length(hour)))\n\nmy_anim &lt;- gganimate::animate(hoursAnimate,\n                             renderer = gganimate::gifski_renderer())\n\nmy_anim\n```\n\n:::\n\n:::\n\n# Data Modeling\n\n**Non-parametric descriptive:**\n\n-   median, interquartile-range (IQR)\n\n## **Semi-parametric and non-parametric** methods\n\n**Description**\n\n-   To predict the outcome variable using independent variables\n\n    -   outcome variable(s) to test\n\n        -   member_casual\n\n**Statistical methods**\n\n-   Binary Logistic regression analysis\n\n**Data type**\n\n-   Outcome variable:\n\n    -   Categorical (\\&gt;= 2 categories)\n\n        -   member_casual\n\n        -   rideable_type\n\n-   Independent variable(s):\n\n    -   Categorical (\\&gt;= 2 categories) or\n\n        -   hour\n\n            -   24 categories\n\n        -   day_of_week\n\n            -   7 categories\n\n        -   month\n\n            -   12 categories\n\n        -   holiday\n\n            -   2 categories (yes or no?)\n\n        -   member_casual\n\n            -   2 categories\n\n        -   rideable_type\n\n            -   2 categories\n\n    -   Continuous or\n\n        -   trip_time\n\n        -   mph\n\n        -   miles\n\n        -   geospatial location\n\n    -   both\n\n        -   member_causal\n\n        -   rideable_type\n\n        -   hour\n\n        -   day\n\n        -   trip_time\n\n        -   mph\n\n        -   miles\n\n        -   geospatial location\n\n\\\nMean \\&gt; Median\n\n**\"Right-Skewed Histogram\"**\\\n- The right side of the histogram plot has lower frequencies than the left.\n\nUnimodal\n\n```{r}\nlatsTable &lt;- dplyr::tbl(dbconn,\n                        tblPath_fltrd,\n                        check_from = FALSE) |&gt;\n    dplyr::select(start_lat) |&gt;\n    dplyr::collect() |&gt;\n\n    # because 5 digits after decimal place gives down to 1.1m accuracy\n    # 4 digits are accurate up to 11m\n    dplyr::mutate(start_lat = signif(start_lat, digits = 5),\n                  .keep = \"none\") |&gt;\n    table() |&gt;\n    as.data.frame()\n\n```\n\n```{r}\n#| eval: false\n\n\nlongsTable &lt;- dplyr::tbl(dbconn,\n                       tblPath_fltrd,\n                       check_from = FALSE) |&gt;\n    dplyr::select(start_lng) |&gt;\n    dplyr::collect() |&gt;\n    # because 5 digits after decimal place gives down to 1.1m accuracy\n    # 4 digits are accurate up to 11m\n    dplyr::mutate(start_lng = signif(start_lng, digits = 5),\n                  .keep = \"none\") |&gt;\n    table() |&gt;\n    as.data.frame()\n\n```\n\n```{r}\nlatlngTable &lt;- dplyr::tbl(dbconn,\n                       tblPath_fltrd,\n                       check_from = FALSE) |&gt;\n    dplyr::select(start_lat, start_lng) |&gt;\n    # because 5 digits after decimal place gives down to 1.1m accuracy\n    # 4 digits are accurate up to 11m\n    dplyr::collect() |&gt;\n    dplyr::mutate(start_lat = round(start_lat, digits = 3),\n                  start_lng = round(start_lng, digits = 3),\n                  .keep = \"none\") |&gt;\n    dplyr::group_by(start_lat, start_lng) |&gt;\n    #dplyr::collect() |&gt;\n    table() |&gt;\n    as.data.frame()\n\nlatlngTable |&gt;\n    head(n = 10)\n```\n\n```{r}\n# Trying to get clues as to how we should alter the table. \ncoordsTable &lt;- dplyr::tbl(dbconn,\n                       tblPath_fltrd,\n                       check_from = FALSE) |&gt;\n    dplyr::select(start_lat, \n                  start_lng) |&gt;\n    # because 5 digits after decimal place gives down to 1.1m accuracy\n    # 4 digits are accurate up to 11m\n   # dplyr::filter(start_lat, signif(start_lat, digits = 7) == TRUE) |&gt;\n    #dplyr::collect() |&gt;\n    dplyr::mutate(start_lat = as.character(start_lat),\n                  start_lng = as.character(start_lng),\n                  .keep = \"none\") |&gt;\n    dplyr::filter(stringr::str_length(start_lat) &gt;= 8,\n                  # 9 to account for negative symbol in lng\n                  stringr::str_length(start_lng) &gt;= 9) |&gt;\n    dplyr::group_by(start_lat, \n                    start_lng) |&gt;\n    dplyr::summarise(n = dplyr::n()) |&gt;\n    dplyr::arrange(n) |&gt;\n    dplyr::collect() |&gt;\n    as.data.frame()\n\n```\n\n```{r}\n# Trying to get clues as to how we should alter the table. \ncoordsTable2 &lt;- dplyr::tbl(dbconn,\n                           tblPath_fltrd,\n                           check_from = FALSE) |&gt;\n    dplyr::select(start_lat, start_lng) |&gt;\n    # because 5 digits after decimal place gives down to 1.1m accuracy\n    # 4 digits are accurate up to 11m\n    dplyr::mutate(start_lat = round(start_lat, digits = 5),\n                  start_lng = round(start_lng, digits = 5),\n                  start_lat = as.character(start_lat),\n                  start_lng = as.character(start_lng),\n                  .keep = \"none\") |&gt;\n    dplyr::filter(stringr::str_length(start_lat) &gt;= 8,\n                  stringr::str_length(start_lng) &gt;= 9) |&gt;\n    dplyr::group_by(start_lat, start_lng) |&gt;\n    dplyr::summarise(n = dplyr::n()) |&gt;\n  #  dplyr::filter(n &gt;= 47) |&gt;\n    dplyr::arrange(n) |&gt;\n    dplyr::collect() |&gt;\n    # seems base R's table() is a lot slower than dplyr\n    #table() |&gt;\n    as.data.frame()\n\ncoordsTable2 |&gt;\n    head(n = 10)\n```\n\nAssuming their plans to expand the \"over 800 stations\" by 250, I'd like to set the parameters to achieve around \\~1050 rows. So I set n to include groups with at least 47 riders.\n\n```{r}\n# according to this, there are 1487 unique station id's in the dataset now\ndplyr::tbl(dbconn,\n           tblPath_fltrd,\n           check_from = FALSE) |&gt;\n    dplyr::select(start_station_name) |&gt;\n    dplyr::distinct(start_station_name) |&gt;\n    dplyr::summarise(n = dplyr::n())\n```\n\nThe 5th decimal place is accurate up to 1.1m, so there ends up being groupings for bikes rented from stations located along a street or at intersections of two streets. You can tell by the start_station_name. If it's an intersection it will say something like \"1st St & 2nd St.\". If it's one street, it will likely just say \"1st St\". There are likely some exceptions to that rule. That helps explain why the geospatial coordinates slightly differ for entries with the same stations and IDs.\n\n```{r}\n#| eval: false\n\nmapTest &lt;- dplyr::tbl(dbconn,\n           tblPath_fltrd,\n           check_from = FALSE) |&gt;\n    dplyr::select(start_station_id,\n                  start_station_name,\n                  start_lat,\n                  start_lng) |&gt;\n    dplyr::group_by(start_station_name,\n                    start_station_id,\n                    start_lat,\n                    start_lng) |&gt;\n    dplyr::summarise(n = dplyr::n()) |&gt;\n    dplyr::filter(stringr::str_detect(\n        start_station_name,\n        \"Ogden Ave & Chicago Ave\") == TRUE,\n        n &gt; 1) |&gt;\n    dplyr::arrange(start_station_name, \n                   start_station_id, \n                   start_lat,\n                   start_lng,\n                   n) |&gt;\n    dplyr::collect()\n\nmapTest |&gt;\n    dplyr::ungroup() |&gt;\n    dplyr::select(start_lng, start_lat, n) |&gt;\n   # dplyr::group_by(start_lng, start_lat) |&gt;\n    sf::st_as_sf(coords = c(1:2),\n             crs = 4326) |&gt;\n    mapview::mapview()\n```\n\nA quick test of the data:\n\n```{r}\n#| eval: false\n\ncoordsDF &lt;- mapTest |&gt;\n    dplyr::ungroup() |&gt;\n    dplyr::select(start_lng,\n                  start_lat) |&gt;\n    dplyr::add_count(start_lng,\n                     start_lat) |&gt;\n    dplyr::slice_head(n = 100)\n\ncoordsDF |&gt;\n    sf::st_as_sf(coords = c(1:2),\n                 crs = 4326) |&gt;\n    mapview::mapview()\n```\n\n```{r}\n#| eval: false\n\n# according to this, there are 1428 unique station IDs in the dataset now\ncountStations &lt;- dplyr::tbl(dbconn,\n                            tblPath,\n                            check_from = FALSE) |&gt;\n    dplyr::select(start_station_id, start_station_name) |&gt;\n    #dplyr::distinct(dplyr::pick(start_station_id, start_station_name)) |&gt;\n    dplyr::group_by(start_station_id,\n                    start_station_name) |&gt;\n    dplyr::summarise(n = dplyr::n()) |&gt;\n    dplyr::filter(n &gt; 0) |&gt;\n    dplyr::collect()\n\nsummary(countStations)\n```\n\n```{r}\n#| label: \"column plot\"\n#| column: body-outset\n\nmemberCasuals_monthly  &lt;- dplyr::tbl(dbconn,\n           tblPath_fltrd) |&gt;\n    dplyr::select(started_at,\n                  member_casual) |&gt;\n    dplyr::mutate('month' = lubridate::month(started_at)) |&gt;\n    dplyr::group_by(month,\n                    member_casual) |&gt;\n    dplyr::summarize(\"riderCount\" = dplyr::n()) |&gt;\n    dplyr::arrange(month)\n\ndplyr::collect(memberCasuals_monthly) |&gt;\n    ggplot2::ggplot() +\n    ggplot2::geom_col(mapping = ggplot2::aes(x = factor(month),\n                                             y = riderCount,\n                                             fill = member_casual),\n                      color = \"black\",\n                      position = 'dodge2') +\n    ggplot2::scale_x_discrete(labels = month.abb,\n                              name = \"Month\") +\n    ggplot2::scale_fill_brewer(palette = 'Set2') +\n    ggplot2::theme_dark() +\n    ggplot2::labs(\n    title = \"Monthly Ridership: Members vs Casuals\",\n    subtitle = \"(Jan-Dec 2023)\",\n    caption = \"Data from cyclistic database.\",\n    tag = \"Figure 1.b\")\n```\n\nfor quick reference with using Tsibble syntax\n\n```{r}\n#|label: 'grouped tibb'\n\ngrouped_byDay &lt;- dplyr::tbl(dbconn,\n                            tblPath_fltrd) |&gt;\n    dplyr::select(started_at,\n                  member_casual) |&gt;\n    dplyr::collect() |&gt;\n    dplyr::mutate(started_at = as.Date(started_at)) |&gt;\n    dplyr::group_by(started_at,\n                    member_casual) |&gt;\n    dplyr::summarize(n = dplyr::n(),\n                     sdev = stats::sd(n))\n```\n\n```{r}\n#|label: 'to grouped tsibb'\n\n# tsibble, time-series table/tibble seems to make time series plots more straightforward\ngrouped_tsi &lt;- grouped_byDay |&gt;\n    tsibble::as_tsibble(key = c(member_casual,\n                                n),\n                        index = started_at) |&gt;\n    dplyr::arrange(started_at)\n```\n\n```{r}\n#|label: \"map query setup\"\n\n# chicago starting coordinates for leaflet, setView\nchicago &lt;- maps::us.cities |&gt;\n    dplyr::select(\"name\",\n                  \"long\",\n                  \"lat\") |&gt;\n    dplyr::filter(name == \"Chicago IL\")\n\n# full dataset coordinates, might need to sample\ncoordQry &lt;- dplyr::tbl(dbconn,\n                       tblPath_fltrd) |&gt;\n    dplyr::select(start_lng,\n                  start_lat) |&gt;\n    dplyr::add_count(start_lng,\n                     start_lat) |&gt;\n    dplyr::distinct() |&gt;\n    dplyr::arrange(desc(n)) |&gt;\n    dplyr::collect()\n\n\ncoordQry_small &lt;- dplyr::tbl(dbconn,\n                             tblPath_fltrd) |&gt;\n    dplyr::select(start_lng,\n                  start_lat) |&gt;\n    dplyr::add_count(start_lng,\n                     start_lat) |&gt;\n    dplyr::distinct() |&gt;\n    dplyr::arrange(desc(n)) |&gt;\n    dplyr::collect() |&gt;\n    dplyr::slice_head(n = 50)\n\n```\n\n```{r}\n#|label: \"mapview\"\n#|eval: false\n\n\ncoordQry_small |&gt;\n    sf::st_as_sf(coords = c(1:2),\n                crs = 4326) |&gt;\n    mapview::mapview()\n\n```\n\n```{r}\nojs_define(js_tsi = grouped_tsi)\n```\n\n```{ojs}\njsData = transpose(js_tsi)\n```\n\n::: column-page\n```{ojs}\nPlot.plot({\n    grid: true,\n    color: {legend: true},\n    marks: [\n        Plot.dot(jsData, {x: 'started_at', y: 'n', fill: 'member_casual'})\n]\n})\n```\n:::\n\n```{ojs}\nPlot.lineY(jsData, {x: \"started_at\", y: \"n\"}).plot()\n```\n\n```{r}\nlocsStart &lt;- dplyr::tbl(dbconn,\n                      tblPath_fltrd) |&gt;\n    dplyr::select(start_station_name\n                  #start_lng,\n                  #start_lat\n                  ) |&gt;\n    dplyr::distinct() |&gt;\n    dplyr::arrange(start_station_name) |&gt;\n    dplyr::rename(\"station_name\" = start_station_name\n                 # \"lng\" = start_lng,\n                 # \"lat\" = start_lat\n                 )\n\nlocsEnd &lt;- dplyr::tbl(dbconn,\n                      tblPath_fltrd) |&gt;\n    dplyr::select(end_station_name\n                  #end_lng,\n                  #end_lat\n                  ) |&gt;\n    dplyr::distinct() |&gt;\n    dplyr::arrange(end_station_name) |&gt;\n    dplyr::rename(\"station_name\" = end_station_name\n                  #\"lng\" = end_lng,\n                  #\"lat\" = end_lat\n                  )\n\nlocs &lt;- locsStart |&gt;\n    dplyr::left_join(locsEnd) |&gt;\n    dplyr::arrange(station_name)\n\nlocs\n```\n\n```{r}\nflowData &lt;- dplyr::tbl(dbconn,\n                       tblPath_fltrd) |&gt;\n    dplyr::select(start_station_name,\n                  end_station_name) |&gt;\n    dplyr::group_by(start_station_name,\n                    end_station_name) |&gt;\n    dplyr::summarize(n = n()) |&gt;\n    dplyr::ungroup() |&gt;\n    dplyr::arrange(desc(n)) |&gt;\n    dplyr::rename(\"from_station\" = start_station_name,\n                  \"to_station\" = end_station_name) |&gt;\n    dplyr::collect() |&gt;\n    dplyr::slice_head(n = 50)\n\nflowData\nsummary(flowData$n)\n```\n\n```{r}\nlocationData &lt;- dplyr::tbl(dbconn,\n                           tblPath_fltrd) |&gt;\n    dplyr::select(start_station_name,\n                  started_at,\n                  ended_at,\n                  trip_time) |&gt;\n    dplyr::group_by(start_station_name) |&gt;\n    dplyr::mutate(\"trip_time\" = round(trip_time,\n                                      digits = 0)) |&gt;\n    dplyr::summarize(\"trip_count\" = dplyr::n(),\n                     \"first_date\" = min(started_at),\n                     \"last_date\" = max(ended_at),\n                     \"avg_trip_time\" = mean(trip_time)\n                     ) |&gt;\n    dplyr::rename(\"station_name\" = start_station_name) |&gt;\n    dplyr::arrange(station_name) |&gt;\n    dplyr::collect()\n\nlocationData\n```\n\n```{r}\nmergd &lt;- merge(x = locationData,\n               y = locs,\n               by.x = \"station_name\",\n               by.y = \"station_name\",\n               sort = FALSE)\n\nmergd &lt;- mergd |&gt;\n    dplyr::mutate(avg_trip_time = round(avg_trip_time,\n                                    digits = 0)) |&gt;\n    dplyr::arrange(desc(trip_count)) |&gt;\n    dplyr::slice_head(n = 25)    \n\nmergd\n```\n\n```{r}\n#|class-output: plotMod\n\nef_test &lt;- epiflows::make_epiflows(flows = flowData,\n                                   locations = mergd,\n                                   duration_stay = \"avg_trip_time\",\n                                   num_cases = \"trip_count\")\n\nepiflows::vis_epiflows(ef_test)\n```\n\nThese maps track bike-sharing activity going on worldwide and in Chicago.\n\n[Bike Share Map](https://bikesharemap.com/#/8/-87.5771/41.3747/) [@bikesha]\n\n[CityBikes: bike sharing networks around the world](https://citybik.es/) [@citybike]\n\n```{r, eval = FALSE}\n#|label: 'drops duckDB tables'\nsource(\"duckDrops.R\")\n```\n\n```{r}\n#|label: 'duckDB Shutdown'\nduckdb::dbDisconnect(dbconn, shutdown = TRUE)\nunlink(\"db\",\n       recursive = TRUE)\n```"
  },
  {
    "objectID": "bikeShare.html#objective",
    "href": "bikeShare.html#objective",
    "title": "BikeShare",
    "section": "Objective:",
    "text": "Objective:\nCommunicate to shareholders data based insights regarding to better inform their desire to enlist more annual subscribers."
  },
  {
    "objectID": "bikeShare.html#hidden-duplicate-observations",
    "href": "bikeShare.html#hidden-duplicate-observations",
    "title": "BikeShare",
    "section": "Hidden Duplicate Observations?",
    "text": "Hidden Duplicate Observations?\nNow to go a little deeper, we can check for duplicates. It might not necessarily be the case that each observation (obs) is unique even if all the Rider IDs are, technically, unique."
  }
]