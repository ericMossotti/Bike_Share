[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Case study: Bike-sharing program in the chicago area",
    "section": "",
    "text": "Stepping Through Code\n\n\n\n\n\n\n\n\nFirst, we decide whether to download and do the necessary initial processing steps or skip that if we have already done this and are just tinkering around with the project.if(exists(\"dbconn\") == FALSE &&\n   dir.exists(\"db\") == FALSE) {\n    # Script to keep this document less cluttered.\n    source(\"import_clean_initial.R\")\n} else {\n    # You will have to change original_nobs if you use            \n    #  different data. It helps with tinkering when \n    #   you want to skip the import step.\n    original_nobs &lt;- as.integer(5719877)\n    \n    tblPath &lt;- \"db/data.db\"\n    \n    dbconn &lt;- DBI::dbConnect(\n        duckdb::duckdb(),\n        dbdir = tblPath,\n        read_only = FALSE,\n        check_from = FALSE\n    )\n}\n\n\n\nThe above would source and execute this script code if the conditions were met.# To help organize the quarto project.\n# This part is messy. \n\n# Load external libraries\nsource(\"unz_relocate.R\")\n\ndurls &lt;-\n    sprintf(\"https://divvy-tripdata.s3.amazonaws.com/%d-divvy-tripdata.zip\",\n            202301:202312)\n\n# Need some directories to store the files. \ndir.create(\"tempZips\")\n\ntempZipPaths &lt;- sprintf(\"tempZips/%d-divvy-tripdata.zip\",\n                        202301:202312)\n\n# A simple way to download and relocate several files. \ncurl::multi_download(durls,\n                     destfiles = tempZipPaths)\n\n# create tempFile directory\ndir.create(\"tempFiles\")\n\n# create list of tempFile directory paths\ntempfile_paths &lt;- sprintf(\"tempFiles/%d-divvy-tripdata.csv\",\n                          202301:202312)\n\n\n# create CSV list to specify for unzipping\nfileNames &lt;- sprintf(\"%d-divvy-tripdata.csv\",\n                     202301:202312)\n\n\n# review address info that was just created\n#tibble::tibble(\"URLs\" = durls,\n#               \"Zip File Paths\" = tempZipPaths,\n#               \"File Names\" = fileNames,\n#               \"Parquet File Paths\" = tempfile_paths)\n\nunz_relocate()\n\n# To remove stored files\nunlink(\"tempZips\",\n       recursive = TRUE)\n\ntripTibble &lt;- \n    purrr::map(tempfile_paths[1:12],\n               arrow::read_csv_arrow) |&gt;\n    purrr::list_rbind()\n\n# Need to save this count for later before I drop the incomplete obs\noriginal_nobs &lt;- nrow(tripTibble)\n\ntripTibble &lt;- tripTibble |&gt;\n    tidyr::drop_na()\n\n# duckDB instead ----\n\ndir.create(\"db\")\n\ntblPath &lt;- \"db/data.db\"\n\n# Helpful persistent db\ndbconn &lt;- DBI::dbConnect(duckdb::duckdb(),\n                         dbdir = tblPath,\n                         read_only = FALSE,\n                         check_from = FALSE)\n\n\ntripTibble |&gt;\n    duckdb::dbWriteTable(conn = dbconn,\n                         name = tblPath,\n                         overwrite = TRUE,\n                         check_from = FALSE)\n\nrm(tripTibble)\n\n# To make use of supplied trip interval data\ndplyr::tbl(dbconn,\n           tblPath,\n           check_from = FALSE) |&gt;\n    dplyr::collect() |&gt;\n    dplyr::mutate(\"trip_time\" = lubridate::time_length(\n        lubridate::interval(started_at,\n                            ended_at),\n        unit = \"minute\"), \n        .keep = \"all\"\n    ) |&gt;\n    duckdb::dbWriteTable(conn = dbconn,\n                         name = tblPath,\n                         overwrite = TRUE,\n                         check_from = FALSE)\n\n    \n# all files and folders\nunlink(\"tempFiles\", recursive = TRUE)\n\n\n\nIt would be helpful to verify the tables that were made.dbList &lt;- duckdb::dbListTables(dbconn) |&gt;\n    data.frame() |&gt;\n    gt::gt() |&gt;\n    gt::tab_header(\n        title = \"Database Tables\",\n        subtitle = \"List of table paths in our DuckDB\") |&gt;\n    gt::cols_label(\n        \"duckdb..dbListTables.dbconn.\" = \"Table Paths\"\n    ) |&gt;\n    gt::tab_options(\n        table.background.color = \"transparent\",\n        table.font.color = \"SeaShell\",\n        column_labels.hidden = TRUE,\n        row.striping.background_color = \"gray10\",\n        row.striping.include_table_body = TRUE,\n        table.font.size = gt::px(13)\n    )\n\n\n\n\n\n\n\n\n\nDatabase Tables\n\n\nList of table paths in our DuckDB\n\n\n\n\ndb/data.db\n\n\ndb/data_fltrd.db\n\n\ndb/dupeless.db\n\n\ndb/freq_member.db\n\n\ndb/freq_miles.db\n\n\ndb/freq_month.db\n\n\ndb/freq_mph.db\n\n\ndb/freq_pairStations.db\n\n\ndb/freq_rTypes.db\n\n\ndb/freq_startNames.db\n\n\ndb/freq_tripTime.db\n\n\ndb/freq_wkDay.db\n\n\n\n\n\n\n\n\n\nData source for this data analysis was obtained from Divvy Data. Thinking ahead with reproducibility in mind, should cover most use cases for tinkering and testing. I have found it helpful to reduce the need to re-download files and re-process all over again if all one needs to do is reconnect to the database that has already been written. (“Divvy Data,” n.d.)\nAs a counterpart to the if-else design decision at the top of the project, I’ve condensed the initial download, import and cleaning steps inside of an R-script. Choosing a persistent DuckDB filesystem (as opposed to in-memory) was intentional as I wouldn’t lose the progress I’ve made when tinkering over multiple days. It seems just as fast as the in-memory database but also seems to reduce RAM needed in tinkering. (“Why DuckDB,” n.d.)"
  },
  {
    "objectID": "index.html#import-and-project-design",
    "href": "index.html#import-and-project-design",
    "title": "Case study: Bike-sharing program in the chicago area",
    "section": "",
    "text": "Stepping Through Code\n\n\n\n\n\n\n\n\nFirst, we decide whether to download and do the necessary initial processing steps or skip that if we have already done this and are just tinkering around with the project.if(exists(\"dbconn\") == FALSE &&\n   dir.exists(\"db\") == FALSE) {\n    # Script to keep this document less cluttered.\n    source(\"import_clean_initial.R\")\n} else {\n    # You will have to change original_nobs if you use            \n    #  different data. It helps with tinkering when \n    #   you want to skip the import step.\n    original_nobs &lt;- as.integer(5719877)\n    \n    tblPath &lt;- \"db/data.db\"\n    \n    dbconn &lt;- DBI::dbConnect(\n        duckdb::duckdb(),\n        dbdir = tblPath,\n        read_only = FALSE,\n        check_from = FALSE\n    )\n}\n\n\n\nThe above would source and execute this script code if the conditions were met.# To help organize the quarto project.\n# This part is messy. \n\n# Load external libraries\nsource(\"unz_relocate.R\")\n\ndurls &lt;-\n    sprintf(\"https://divvy-tripdata.s3.amazonaws.com/%d-divvy-tripdata.zip\",\n            202301:202312)\n\n# Need some directories to store the files. \ndir.create(\"tempZips\")\n\ntempZipPaths &lt;- sprintf(\"tempZips/%d-divvy-tripdata.zip\",\n                        202301:202312)\n\n# A simple way to download and relocate several files. \ncurl::multi_download(durls,\n                     destfiles = tempZipPaths)\n\n# create tempFile directory\ndir.create(\"tempFiles\")\n\n# create list of tempFile directory paths\ntempfile_paths &lt;- sprintf(\"tempFiles/%d-divvy-tripdata.csv\",\n                          202301:202312)\n\n\n# create CSV list to specify for unzipping\nfileNames &lt;- sprintf(\"%d-divvy-tripdata.csv\",\n                     202301:202312)\n\n\n# review address info that was just created\n#tibble::tibble(\"URLs\" = durls,\n#               \"Zip File Paths\" = tempZipPaths,\n#               \"File Names\" = fileNames,\n#               \"Parquet File Paths\" = tempfile_paths)\n\nunz_relocate()\n\n# To remove stored files\nunlink(\"tempZips\",\n       recursive = TRUE)\n\ntripTibble &lt;- \n    purrr::map(tempfile_paths[1:12],\n               arrow::read_csv_arrow) |&gt;\n    purrr::list_rbind()\n\n# Need to save this count for later before I drop the incomplete obs\noriginal_nobs &lt;- nrow(tripTibble)\n\ntripTibble &lt;- tripTibble |&gt;\n    tidyr::drop_na()\n\n# duckDB instead ----\n\ndir.create(\"db\")\n\ntblPath &lt;- \"db/data.db\"\n\n# Helpful persistent db\ndbconn &lt;- DBI::dbConnect(duckdb::duckdb(),\n                         dbdir = tblPath,\n                         read_only = FALSE,\n                         check_from = FALSE)\n\n\ntripTibble |&gt;\n    duckdb::dbWriteTable(conn = dbconn,\n                         name = tblPath,\n                         overwrite = TRUE,\n                         check_from = FALSE)\n\nrm(tripTibble)\n\n# To make use of supplied trip interval data\ndplyr::tbl(dbconn,\n           tblPath,\n           check_from = FALSE) |&gt;\n    dplyr::collect() |&gt;\n    dplyr::mutate(\"trip_time\" = lubridate::time_length(\n        lubridate::interval(started_at,\n                            ended_at),\n        unit = \"minute\"), \n        .keep = \"all\"\n    ) |&gt;\n    duckdb::dbWriteTable(conn = dbconn,\n                         name = tblPath,\n                         overwrite = TRUE,\n                         check_from = FALSE)\n\n    \n# all files and folders\nunlink(\"tempFiles\", recursive = TRUE)\n\n\n\nIt would be helpful to verify the tables that were made.dbList &lt;- duckdb::dbListTables(dbconn) |&gt;\n    data.frame() |&gt;\n    gt::gt() |&gt;\n    gt::tab_header(\n        title = \"Database Tables\",\n        subtitle = \"List of table paths in our DuckDB\") |&gt;\n    gt::cols_label(\n        \"duckdb..dbListTables.dbconn.\" = \"Table Paths\"\n    ) |&gt;\n    gt::tab_options(\n        table.background.color = \"transparent\",\n        table.font.color = \"SeaShell\",\n        column_labels.hidden = TRUE,\n        row.striping.background_color = \"gray10\",\n        row.striping.include_table_body = TRUE,\n        table.font.size = gt::px(13)\n    )\n\n\n\n\n\n\n\n\n\nDatabase Tables\n\n\nList of table paths in our DuckDB\n\n\n\n\ndb/data.db\n\n\ndb/data_fltrd.db\n\n\ndb/dupeless.db\n\n\ndb/freq_member.db\n\n\ndb/freq_miles.db\n\n\ndb/freq_month.db\n\n\ndb/freq_mph.db\n\n\ndb/freq_pairStations.db\n\n\ndb/freq_rTypes.db\n\n\ndb/freq_startNames.db\n\n\ndb/freq_tripTime.db\n\n\ndb/freq_wkDay.db\n\n\n\n\n\n\n\n\n\nData source for this data analysis was obtained from Divvy Data. Thinking ahead with reproducibility in mind, should cover most use cases for tinkering and testing. I have found it helpful to reduce the need to re-download files and re-process all over again if all one needs to do is reconnect to the database that has already been written. (“Divvy Data,” n.d.)\nAs a counterpart to the if-else design decision at the top of the project, I’ve condensed the initial download, import and cleaning steps inside of an R-script. Choosing a persistent DuckDB filesystem (as opposed to in-memory) was intentional as I wouldn’t lose the progress I’ve made when tinkering over multiple days. It seems just as fast as the in-memory database but also seems to reduce RAM needed in tinkering. (“Why DuckDB,” n.d.)"
  },
  {
    "objectID": "index.html#hidden-duplicates",
    "href": "index.html#hidden-duplicates",
    "title": "Case study: Bike-sharing program in the chicago area",
    "section": "Hidden Duplicates?",
    "text": "Hidden Duplicates?\nNow to go a little deeper, we can check for duplicates. It might not necessarily be the case that each observation (obs) is unique even if all the Rider IDs are, technically, unique. Of the other columns, it seems that the start_time, end_time, start_station, and end_station, if combined, could show if there are possibly hidden duplicated observations. We started with 5,719,877 observations (obs) for dates spanning January to December, 2023, then removed 1,388,170 incomplete obs.\nI assumed that having the same times/dates and stations for two different ride IDs was a mistake. Although, I do not know how that error would happen, I could have assumed one person could check out multiple bikes at once. In that instance, each bike would be assigned a unique ride_id. That, however, has only happened 18 times over a year. Since it’s only one copy every time, that also raises a red flag in my mind. I did not notice any other correlations with station_id/name, member_casual, or ride_type for those particular duplicated data.\n\n\n\nStepping Through the Code\n\n\n\n\n\n\n\n\ncreate gt of duplicates for illustration# This is a separate table used to analyze the observations \n#  returned as not distinct (n &gt; 1). \n#   This adds an extra column, labeled \"n\".\ndupeTable &lt;- dplyr::tbl(dbconn,\n                        tblPath,\n                        check_from = FALSE) |&gt;\n    dplyr::select(started_at:end_station_name) |&gt;\n    # Counts of unique rows added for column 'n'\n    dplyr::add_count(started_at,\n                     ended_at,\n                     start_station_name,\n                     end_station_name) |&gt;\n    # Only observations that have been duplicated \n    #  1 or more times are shown.\n    dplyr::filter(n &gt; 1) |&gt;\n    # We want to see all rows, \n    #  not just one row for each obs.\n    dplyr::ungroup() |&gt;\n    dplyr::arrange(started_at) |&gt;\n    dplyr::collect()\n\n\n\nTo count distinct duplicates and total obs.distinctCopiesCount &lt;- dupeTable |&gt;\n    dplyr::distinct(n) |&gt;\n    as.integer() \n\nduplicateObs &lt;- length(dupeTable[[1]])\n\n\n\ncreate table of the duplicated obs# The issue is, we need to get rid of not all of these rows,\n#  but just the extra duplicate observations. \n\n# If there were 2 rows of duplicates, \n#  we would want to end up with 1 row after \n#   removing the extras.\nundupedTable &lt;- dupeTable |&gt;\n    dplyr::distinct(started_at,\n                     start_station_name,\n                     ended_at,\n                     end_station_name,\n                     .keep_all = TRUE)\n\n\n\ncount of incorrect obs# Run an incorrect count on how many rows or observations \n#  there are in the dataset.\ncount_incorrectDists &lt;- dplyr::tbl(dbconn,\n                                   tblPath,\n                                   check_from = FALSE) |&gt;\n    dplyr::distinct(dplyr::pick(\"ride_id\")) |&gt;\n    dplyr::count(name = \"Incorrect Distinct Observations\") |&gt;\n    dplyr::collect() |&gt;\n    as.integer()\n\n\n\ncount of correct obs# For the correct count of obs\ncount_correctDists &lt;- dplyr::tbl(dbconn,\n                                 tblPath,\n                                 check_from = FALSE) |&gt;\n    dplyr::distinct(\n        dplyr::pick(\n            \"started_at\",\n            \"start_station_name\",\n            \"ended_at\",\n            \"end_station_name\"\n        )\n    ) |&gt;\n    dplyr::count() |&gt;\n    dplyr::collect() |&gt;\n    as.integer()\n\n\n\nwriting dupeless dbdupelessPath &lt;- \"db/dupeless.db\"\n \ndplyr::tbl(dbconn,\n           tblPath,\n           check_from = FALSE) |&gt;\n    dplyr::select(ride_id:trip_time) |&gt;\n    dplyr::distinct(started_at,\n                    start_station_name,\n                    ended_at,\n                    end_station_name,\n                    .keep_all = TRUE) |&gt;\n    dplyr::arrange(started_at) |&gt;\n    dplyr::collect() |&gt;\n    duckdb::dbWriteTable(conn = dbconn,\n                         name = dupelessPath,\n                         overwrite = TRUE,\n                         check_from = FALSE)\n\n\n\n\n\n\n\n\nDuplicates View\nUnduplicated View\nVerify Dupeless\nSummary of Processsing\n\n\n\n\nCodegtDupes &lt;- dupeTable |&gt;\n    dplyr::group_by(started_at) |&gt;\n    gt::gt(rowname_col = \"row\",\n           groupname_col = \"started_at\",\n           row_group_as_column = TRUE,\n           caption = \"Duplicates_Table1\") |&gt;\n    gt::tab_style(\n    style = list(\n        gt::cell_text(weight = \"bold\",\n                      align = \"center\"),\n        gt::cell_borders(sides = c(\"bottom\"))\n    ),\n    locations = gt::cells_column_labels(gt::everything())\n    ) |&gt;\n    gt::tab_style(\n    style = list(\n        gt::cell_borders(sides = c(\"left\", \"right\"),\n                         color = \"transparent\"),\n        gt::cell_text(align = \"center\",\n                      v_align = \"middle\")\n    ),\n    locations = gt::cells_body(gt::everything())\n    ) |&gt;\n    gt::data_color(columns = start_station_name,\n                   target_columns = gt::everything(),\n                   method = \"auto\",\n                   palette = \"basetheme::brutal\") |&gt;\n    gt::tab_source_note(gt::md(\"**Source**: Divvy Data\")) |&gt;\n    gt::tab_header(title = \"Duplicate Observations\",\n                   subtitle = \"By the starting date-time\") |&gt;\n    gt::tab_options(\n        heading.title.font.weight = \"bolder\",\n        heading.subtitle.font.weight = \"lighter\",\n        table.background.color = \"transparent\",\n        table.font.color = \"SeaShell\",\n        table.font.size = gt::pct(75))\n\ngtDupes\n\n\n\n\nDuplicates_Table1\n\n\nDuplicate Observations\n\n\nBy the starting date-time\n\n\n\nended_at\nstart_station_name\nstart_station_id\nend_station_name\nn\n\n\n\n\n2023-02-19 12:10:52\n2023-02-19 12:24:04\nOrleans St & Merchandise Mart Plaza\nTA1305000022\nGreen St & Randolph St*\n2\n\n\n2023-02-19 12:24:04\nOrleans St & Merchandise Mart Plaza\nTA1305000022\nGreen St & Randolph St*\n2\n\n\n2023-04-15 15:56:18\n2023-04-15 16:01:54\nKedzie Ave & 45th St\n342\nFairfield Ave & 44th St\n2\n\n\n2023-04-15 16:01:54\nKedzie Ave & 45th St\n342\nFairfield Ave & 44th St\n2\n\n\n2023-04-21 09:45:26\n2023-04-21 10:01:23\nMichigan Ave & 8th St\n623\nWentworth Ave & Cermak Rd*\n2\n\n\n2023-04-21 10:01:23\nMichigan Ave & 8th St\n623\nWentworth Ave & Cermak Rd*\n2\n\n\n2023-07-08 18:22:07\n2023-07-08 18:37:31\nMilwaukee Ave & Grand Ave\n13033\nBissell St & Armitage Ave*\n2\n\n\n2023-07-08 18:37:31\nMilwaukee Ave & Grand Ave\n13033\nBissell St & Armitage Ave*\n2\n\n\n2023-07-08 18:58:14\n2023-07-08 19:08:47\nClark St & Schiller St\nTA1309000024\nBissell St & Armitage Ave*\n2\n\n\n2023-07-08 19:08:47\nClark St & Schiller St\nTA1309000024\nBissell St & Armitage Ave*\n2\n\n\n2023-07-09 18:00:17\n2023-07-09 18:40:49\nWells St & Hubbard St\nTA1307000151\nFort Dearborn Dr & 31st St*\n2\n\n\n2023-07-09 18:40:49\nWells St & Hubbard St\nTA1307000151\nFort Dearborn Dr & 31st St*\n2\n\n\n2023-07-10 20:10:41\n2023-07-10 20:19:59\nClark St & Newport St\n632\nLincoln Ave & Roscoe St*\n2\n\n\n2023-07-10 20:19:59\nClark St & Newport St\n632\nLincoln Ave & Roscoe St*\n2\n\n\n2023-07-15 10:48:09\n2023-07-15 10:58:22\nRacine Ave & Wrightwood Ave\nTA1309000059\nBissell St & Armitage Ave*\n2\n\n\n2023-07-15 10:58:22\nRacine Ave & Wrightwood Ave\nTA1309000059\nBissell St & Armitage Ave*\n2\n\n\n2023-07-15 19:38:51\n2023-07-15 19:55:04\nAvondale Ave & Irving Park Rd\n15624\nPublic Rack - Hamlin Ave & Fullerton Ave\n2\n\n\n2023-07-15 19:55:04\nAvondale Ave & Irving Park Rd\n15624\nPublic Rack - Hamlin Ave & Fullerton Ave\n2\n\n\n2023-07-23 11:41:36\n2023-07-23 12:07:13\nBurnham Harbor\n15545\nFort Dearborn Dr & 31st St*\n2\n\n\n2023-07-23 12:07:13\nBurnham Harbor\n15545\nFort Dearborn Dr & 31st St*\n2\n\n\n2023-07-25 18:08:47\n2023-07-25 18:21:45\nWabash Ave & Roosevelt Rd\nTA1305000002\nWentworth Ave & Cermak Rd*\n2\n\n\n2023-07-25 18:21:45\nWabash Ave & Roosevelt Rd\nTA1305000002\nWentworth Ave & Cermak Rd*\n2\n\n\n2023-07-26 21:10:55\n2023-07-26 21:28:44\nBurnham Harbor\n15545\nFort Dearborn Dr & 31st St*\n2\n\n\n2023-07-26 21:28:44\nBurnham Harbor\n15545\nFort Dearborn Dr & 31st St*\n2\n\n\n2023-08-05 19:40:37\n2023-08-05 20:08:35\nMorgan St & Lake St*\nchargingstx4\nMorgan St & Lake St*\n2\n\n\n2023-08-05 20:08:35\nMorgan St & Lake St*\nchargingstx4\nMorgan St & Lake St*\n2\n\n\n2023-08-12 17:46:53\n2023-08-12 17:57:40\nCalumet Ave & 18th St\n13102\nWentworth Ave & Cermak Rd*\n2\n\n\n2023-08-12 17:57:40\nCalumet Ave & 18th St\n13102\nWentworth Ave & Cermak Rd*\n2\n\n\n2023-08-17 12:23:50\n2023-08-17 12:37:45\nDuSable Lake Shore Dr & Monroe St\n13300\nStreeter Dr & Grand Ave\n2\n\n\n2023-08-17 12:37:45\nDuSable Lake Shore Dr & Monroe St\n13300\nStreeter Dr & Grand Ave\n2\n\n\n2023-09-03 14:55:59\n2023-09-03 15:58:21\nBissell St & Armitage Ave*\nchargingstx1\nBissell St & Armitage Ave*\n2\n\n\n2023-09-03 15:58:21\nBissell St & Armitage Ave*\nchargingstx1\nBissell St & Armitage Ave*\n2\n\n\n2023-09-25 17:38:05\n2023-09-25 17:52:29\nFairbanks Ct & Grand Ave\nTA1305000003\nDuSable Lake Shore Dr & North Blvd\n2\n\n\n2023-09-25 17:52:29\nFairbanks Ct & Grand Ave\nTA1305000003\nDuSable Lake Shore Dr & North Blvd\n2\n\n\n2023-10-10 13:22:51\n2023-10-10 13:29:37\nLoomis St & Lexington St\n13332\nMorgan St & Polk St\n2\n\n\n2023-10-10 13:29:37\nLoomis St & Lexington St\n13332\nMorgan St & Polk St\n2\n\n\n\n\nSource: Divvy Data\n\n\n\n\n\n\n\n\nCodeundupedTable |&gt;\n    dplyr::collect() |&gt;\n    dplyr::group_by(started_at) |&gt;\n    gt::gt(rowname_col = \"row\",\n           groupname_col = \"started_at\",\n           row_group_as_column = TRUE,\n           caption = \"Table2\") |&gt;\n    gt::tab_style(\n    style = list(\n        gt::cell_text(weight = \"bold\",\n                      align = \"center\"),\n        gt::cell_borders(sides = c(\"bottom\"))\n    ),\n    locations = gt::cells_column_labels(gt::everything())\n    ) |&gt;\n    gt::tab_style(\n    style = list(\n        gt::cell_borders(sides = c(\"left\", \"right\")),\n        gt::cell_text(align = \"center\",\n                      v_align = \"middle\")\n    ),\n    locations = gt::cells_body(gt::everything())\n    ) |&gt;\n    gt::data_color(columns = start_station_name,\n                   target_columns = gt::everything(),\n                   method = \"auto\",\n                   palette = \"basetheme::brutal\") |&gt;\n    gt::tab_source_note(gt::md(\"**Source**: Divvy Data\")) |&gt;\n    gt::tab_header(title = \"Un-Duplicated Observations\",\n                   subtitle = \"Grouped by start date)\") |&gt;\n    gt::tab_options(\n        heading.title.font.weight = \"bolder\",\n        heading.subtitle.font.weight = \"lighter\",\n        table.layout = \"auto\",\n        table.background.color = \"transparent\",\n        table.font.color = \"SeaShell\",\n        table.font.size = gt::pct(75)\n        )\n\n\n\n\nTable2\n\n\nUn-Duplicated Observations\n\n\nGrouped by start date)\n\n\n\nended_at\nstart_station_name\nstart_station_id\nend_station_name\nn\n\n\n\n\n2023-02-19 12:10:52\n2023-02-19 12:24:04\nOrleans St & Merchandise Mart Plaza\nTA1305000022\nGreen St & Randolph St*\n2\n\n\n2023-04-15 15:56:18\n2023-04-15 16:01:54\nKedzie Ave & 45th St\n342\nFairfield Ave & 44th St\n2\n\n\n2023-04-21 09:45:26\n2023-04-21 10:01:23\nMichigan Ave & 8th St\n623\nWentworth Ave & Cermak Rd*\n2\n\n\n2023-07-08 18:22:07\n2023-07-08 18:37:31\nMilwaukee Ave & Grand Ave\n13033\nBissell St & Armitage Ave*\n2\n\n\n2023-07-08 18:58:14\n2023-07-08 19:08:47\nClark St & Schiller St\nTA1309000024\nBissell St & Armitage Ave*\n2\n\n\n2023-07-09 18:00:17\n2023-07-09 18:40:49\nWells St & Hubbard St\nTA1307000151\nFort Dearborn Dr & 31st St*\n2\n\n\n2023-07-10 20:10:41\n2023-07-10 20:19:59\nClark St & Newport St\n632\nLincoln Ave & Roscoe St*\n2\n\n\n2023-07-15 10:48:09\n2023-07-15 10:58:22\nRacine Ave & Wrightwood Ave\nTA1309000059\nBissell St & Armitage Ave*\n2\n\n\n2023-07-15 19:38:51\n2023-07-15 19:55:04\nAvondale Ave & Irving Park Rd\n15624\nPublic Rack - Hamlin Ave & Fullerton Ave\n2\n\n\n2023-07-23 11:41:36\n2023-07-23 12:07:13\nBurnham Harbor\n15545\nFort Dearborn Dr & 31st St*\n2\n\n\n2023-07-25 18:08:47\n2023-07-25 18:21:45\nWabash Ave & Roosevelt Rd\nTA1305000002\nWentworth Ave & Cermak Rd*\n2\n\n\n2023-07-26 21:10:55\n2023-07-26 21:28:44\nBurnham Harbor\n15545\nFort Dearborn Dr & 31st St*\n2\n\n\n2023-08-05 19:40:37\n2023-08-05 20:08:35\nMorgan St & Lake St*\nchargingstx4\nMorgan St & Lake St*\n2\n\n\n2023-08-12 17:46:53\n2023-08-12 17:57:40\nCalumet Ave & 18th St\n13102\nWentworth Ave & Cermak Rd*\n2\n\n\n2023-08-17 12:23:50\n2023-08-17 12:37:45\nDuSable Lake Shore Dr & Monroe St\n13300\nStreeter Dr & Grand Ave\n2\n\n\n2023-09-03 14:55:59\n2023-09-03 15:58:21\nBissell St & Armitage Ave*\nchargingstx1\nBissell St & Armitage Ave*\n2\n\n\n2023-09-25 17:38:05\n2023-09-25 17:52:29\nFairbanks Ct & Grand Ave\nTA1305000003\nDuSable Lake Shore Dr & North Blvd\n2\n\n\n2023-10-10 13:22:51\n2023-10-10 13:29:37\nLoomis St & Lexington St\n13332\nMorgan St & Polk St\n2\n\n\n\n\nSource: Divvy Data\n\n\n\n\n\n\n\n\nCodedplyr::tbl(dbconn,\n           dupelessPath) |&gt;\n    head() |&gt;\n    gt::gt() |&gt;\n    gt::tab_header(\n        title = \"Un-Duplicated DB Table\",\n        subtitle = \"Top 10 rows from the database table\"\n    ) |&gt;\n    gt::tab_style(\n        gt::cell_borders(sides = c(\"left\", \"right\"),\n                         color = \"gray20\",\n                         weight = gt::px(1)),\n    locations = list(\n        gt::cells_body(columns = gt::everything())\n        )\n    ) |&gt;\n    gt::tab_options(\n        table.background.color = \"transparent\",\n        table.font.color = \"SeaShell\",\n        row.striping.background_color = \"gray10\",\n        row.striping.include_table_body = TRUE,\n        table_body.hlines.color = \"gray20\",\n        table_body.vlines.color = \"gray20\",\n        table.font.size = gt::pct(75),\n        #column_labels.border.lr.color = \"gray20\"\n    )\n\n\n\n\n\n\nUn-Duplicated DB Table\n\n\nTop 10 rows from the database table\n\n\nride_id\nrideable_type\nstarted_at\nended_at\nstart_station_name\nstart_station_id\nend_station_name\nend_station_id\nstart_lat\nstart_lng\nend_lat\nend_lng\nmember_casual\ntrip_time\n\n\n\n\nD8EEE72183269F07\nclassic_bike\n2023-01-01 00:02:06\n2023-01-01 00:29:46\nFairbanks Ct & Grand Ave\nTA1305000003\nNew St & Illinois St\nTA1306000013\n41.89185\n-87.62058\n41.89085\n-87.61862\nmember\n27.666667\n\n\nE5AD797A579842F8\nelectric_bike\n2023-01-01 00:03:26\n2023-01-01 00:07:23\nSheridan Rd & Loyola Ave\nRP-009\nSheridan Rd & Loyola Ave\nRP-009\n42.00114\n-87.66126\n42.00104\n-87.66120\ncasual\n3.950000\n\n\n8FBD2AD70B0F6A6F\nclassic_bike\n2023-01-01 00:04:07\n2023-01-01 00:13:56\nLeavitt St & Lawrence Ave\nTA1309000015\nBroadway & Argyle St\n13108\n41.96889\n-87.68400\n41.97382\n-87.65966\ncasual\n9.816667\n\n\nB05BD052B9EBB767\nelectric_bike\n2023-01-01 00:04:27\n2023-01-01 00:16:52\nClark St & Montrose Ave\nKA1503000022\nClark St & Montrose Ave\nKA1503000022\n41.96154\n-87.66619\n41.96159\n-87.66604\nmember\n12.416667\n\n\nF9EA7B9E6C243CFC\nclassic_bike\n2023-01-01 00:04:54\n2023-01-01 00:31:52\nState St & Randolph St\nTA1305000029\nIndiana Ave & Roosevelt Rd\nSL-005\n41.88462\n-87.62783\n41.86789\n-87.62304\nmember\n26.966667\n\n\n27C2A67184C49D01\nelectric_bike\n2023-01-01 00:05:43\n2023-01-01 00:21:37\nWabash Ave & Grand Ave\nTA1307000117\nStreeter Dr & Grand Ave\n13022\n41.89151\n-87.62686\n41.89228\n-87.61204\nmember\n15.900000\n\n\n\n\n\n\n\n\n\nCode# To see the history of obs in our dataset.\nsummaryProcessTable &lt;- tidyr::tribble(\n    ~ \"Observations\",\n    ~ \"Counts\",\n    \"Original   \",\n    original_nobs,\n    \"Processed   \",\n    count_incorrectDists,\n    \"Duplicates   \",\n    (count_incorrectDists - count_correctDists),\n    \"Total Corrected   \",\n    count_correctDists ) |&gt;\n    gt::gt(rownames_to_stub = FALSE) |&gt;\n    gt::tab_header(title = \"Tallying Observations\") |&gt;\n    gt::tab_footnote(\n        footnote = gt::md(\"Row counts throughout the cleaning steps.\"),\n        locations = gt::cells_column_labels(columns = Counts)\n    ) |&gt;\n    gt::tab_style(\n        style = list(\n            gt::cell_borders(sides = \"bottom\"),\n            gt::cell_text(\n                align = \"left\",\n                stretch = \"semi-expanded\",\n                whitespace = \"break-spaces\"\n            )\n        ),\n        locations = gt::cells_body(gt::everything())\n    ) |&gt;\n    gt::tab_style(\n        gt::cell_text(\n            align = \"center\",\n            stretch = \"semi-expanded\",\n            whitespace = \"break-spaces\"),\n        locations = list(\n            gt::cells_title(groups = c(\"title\", \"subtitle\")),\n            gt::cells_column_labels(gt::everything())\n            )\n        ) |&gt;\n    gt::tab_options(quarto.use_bootstrap = TRUE,\n                    column_labels.font.weight = \"bold\",\n                    table.background.color = \"transparent\",\n                    table.font.color = \"SeaShell\",\n                    row.striping.background_color = \"gray10\",\n                    row.striping.include_table_body = TRUE)\n\nsummaryProcessTable\n\n\n\n\n\n\nTallying Observations\n\n\nObservations\nCounts1\n\n\n\n\n\nOriginal\n5719877\n\n\nProcessed\n4331707\n\n\nDuplicates\n18\n\n\nTotal Corrected\n4331689\n\n\n\n\n1 Row counts throughout the cleaning steps.\n\n\n\n\n\n\n\n\n\nBy applying distinct() on dupeTable, we see the only distinct value, n, is 2. I conclude that, of the duplicates, each has a minimum and maximum of 1 extra copy. Number of rows in the dupeTable is 36. Because each duplicated observation has one duplicate, n = 2, expected removed obs is 18. The issue is that we need to get rid of not all 36 rows, but just one extra duplicate observation from each. This will result in the expected 18 obs.\nThe count of distinct n-values for the un-duplicated table was indeed 18. So now, it is time to run a count of how rows/observations are in the dataset. There is a difference, though, concerning the correct amount. The incorrect nobs was 4,331,707. The correct nobs after removing duplicated obs was 4,331,689. In short, 18 additional obs were removed. We can now add the processed table to our database. Might be a good idea to verify the table is where it should be."
  },
  {
    "objectID": "index.html#outlier-filter",
    "href": "index.html#outlier-filter",
    "title": "Case study: Bike-sharing program in the chicago area",
    "section": "Outlier Filter",
    "text": "Outlier Filter\nTo ensure the conclusions are accurate, outliers should be filtered. Negative and very low trip times might skew trends. The underlying reason for very low trip times is somewhat of an unknown. Perhaps people often change their minds?\nAs in the first part, an if-else code-chunk design was chosen because it makes testing easier. It’s not required but is nice-to-have. Removing the nonsensical outliers, on the other hand, is required. This code chunk accomplishes both, regardless if you are just testing and already have the filtered database table or if you still need to create it. A database filtering script was used to make the code chunk easier to follow. We then verify the table does exist now along with all other tables we previously created. So this should have removed outliers from the dataset which don’t serve the scope of this analysis or could be erroneous data.\n\n\nThis would execute if the if-else conditions were met to filter the db/data.db database tablefilterDatabase &lt;- function(conxn = dbconn,\n                           path1 = dupelessPath,\n                           path2 = tblPath_fltrd) {\n    dplyr::tbl(conxn,\n               path1,\n               check_from = FALSE) |&gt;\n        dplyr::filter(trip_time &gt; 1,\n                      trip_time &lt; 480,\n                      rideable_type != \"docked_bike\") |&gt;\n        dplyr::collect() |&gt;\n        # Might as well calculate distance traveled while at it.\n        dplyr::mutate(\n            miles = geosphere::distGeo(\n                p1 = cbind(start_lng, start_lat),\n                p2 = cbind(end_lng, end_lat)\n            ) / 1000 * 0.62137119,\n            mph = (miles / (trip_time / 60))\n        ) |&gt;\n        # It's nonsensical to rent a bike for distances easily walked.\n        dplyr::filter(miles &gt; 0.1,\n                      # Seems that pro cyclists average around 20 mph,\n                      # so I set that as the ceiling.\n                      mph &lt;= 20,\n                      # To account for time spent idling, stoplights and\n                      # traffic.\n                      mph &gt; 1) |&gt;\n        duckdb::dbWriteTable(\n            conn = conxn,\n            name = path2,\n            overwrite = TRUE,\n            check_from = FALSE\n        )\n}\n\n\n\n\n\n\n\n\n\n\n\nStepping Through the Code\n\n\n\n\n\n\n\n\nSo you do not have to re-download or re-filter after making further adjustments. This is for people who re-use this code.tblPath &lt;- \"db/data.db\"\ndupelessPath &lt;- \"db/dupeless.db\"\ntblPath_fltrd &lt;- \"db/data_fltrd.db\"\n\nif (exists(\"dbconn\") == FALSE && dir.exists(\"db\") == TRUE) {\n    dbconn &lt;- DBI::dbConnect(\n        duckdb::duckdb(),\n        dbdir = tblPath,\n        read_only = FALSE,\n        check_from = FALSE\n    )\n}\n\nif (duckdb::dbExistsTable(dbconn,\n                          \"tblPath_fltrd\") == FALSE) {\n    source(\"filterDatabase.R\")\n    filterDatabase()\n}"
  },
  {
    "objectID": "index.html#frequency-tables-and-what-they-tell-us",
    "href": "index.html#frequency-tables-and-what-they-tell-us",
    "title": "Case study: Bike-sharing program in the chicago area",
    "section": "Frequency Tables and What They Tell Us",
    "text": "Frequency Tables and What They Tell Us\nNow we’re going to create frequency tables and add those to the database. We can retain the outliers in those tables and perhaps filter as needed later. This is a good place start. If needed, we can dive deeper into other statistical techniques or adjust parameters in these code chunks and overwrite or create new db tables. By generating frequency tables, one can quickly glean insights from the data.\n\n\n\nStepping Through the Code\n\n\n\n\n\n\n\n\nWriting frequency tables to DuckDB.# For the membership frequency\ndplyr::tbl(dbconn,\n           tblPath_fltrd,\n           check_from = FALSE) |&gt;\n    dplyr::select(member_casual) |&gt;\n    dplyr::group_by(member_casual) |&gt;\n    dplyr::summarize(n = dplyr::n()) |&gt;\n    dplyr::collect() |&gt;\n    duckdb::dbWriteTable(conn = dbconn,\n                         name = \"db/freq_member.db\",\n                         overwrite = TRUE,\n                         check_from = FALSE)\n\n# For the rideable types.\ndplyr::tbl(dbconn,\n           tblPath_fltrd,\n           check_from = FALSE) |&gt;\n    dplyr::select(rideable_type) |&gt;\n    dplyr::group_by(rideable_type) |&gt;\n    dplyr::summarize(n = dplyr::n()) |&gt;\n    dplyr::collect() |&gt;\n    duckdb::dbWriteTable(conn = dbconn,\n                         name = \"db/freq_rTypes.db\",\n                         overwrite = TRUE,\n                         check_from = FALSE)\n\n# For the miles\ndplyr::tbl(dbconn,\n           tblPath_fltrd,\n           check_from = FALSE) |&gt;\n    dplyr::select(miles) |&gt;\n    dplyr::collect() |&gt;\n    dplyr::mutate(miles = dplyr::case_when(\n        miles &gt;= 1 ~ round(miles,\n                           digits = 0),\n        miles &lt; 1 ~ round(signif(miles, 3),\n                          digits = 1)\n    )) |&gt;\n    dplyr::group_by(miles) |&gt;\n    dplyr::summarize(n = dplyr::n()) |&gt;\n    dplyr::arrange(miles) |&gt;\n    duckdb::dbWriteTable(\n        conn = dbconn,\n        name = \"db/freq_miles.db\",\n        check_from = FALSE,\n        overwrite = TRUE)\n\n# For the mph\ndplyr::tbl(dbconn,\n           tblPath_fltrd,\n           check_from = FALSE) |&gt;\n    dplyr::select(mph) |&gt;\n    dplyr::mutate(mph = round(mph, digits = 0)) |&gt;\n    dplyr::group_by(mph) |&gt;\n    dplyr::summarize(n = dplyr::n()) |&gt;\n    dplyr::collect() |&gt;\n    dplyr::arrange(mph) |&gt;\n    duckdb::dbWriteTable(conn = dbconn,\n                         name = \"db/freq_mph.db\",\n                         check_from = FALSE,\n                         overwrite = TRUE)\n\n# For the week days\ndplyr::tbl(dbconn,\n           tblPath_fltrd,\n           check_from = FALSE) |&gt;\n    dplyr::select(started_at) |&gt;\n    dplyr::mutate(wkday = lubridate::wday(started_at)) |&gt;\n    dplyr::group_by(wkday) |&gt;\n    dplyr::summarize(n = dplyr::n()) |&gt;\n    dplyr::arrange(wkday) |&gt;\n    dplyr::collect() |&gt;\n    dplyr::mutate(wkday = c(\"Sun\",\n                            \"Mon\",\n                            \"Tue\",\n                            \"Wed\",\n                            \"Thu\",\n                            \"Fri\",\n                            \"Sat\"),\n                  wkday = forcats::as_factor(wkday),\n                  wkday = forcats::fct_inorder(wkday)) |&gt;\n    duckdb::dbWriteTable(conn = dbconn,\n                         name = \"db/freq_wkDay.db\",\n                         check_from = FALSE,\n                         overwrite = TRUE)\n\n# For the months.\ndplyr::tbl(dbconn,\n           tblPath_fltrd,\n           check_from = FALSE) |&gt;\n    dplyr::select(started_at) |&gt;\n    dplyr::mutate(months = lubridate::month(started_at,\n                                            label = FALSE,\n                                            abbr = TRUE\n                                            )) |&gt;\n    dplyr::group_by(months) |&gt;\n    dplyr::summarize(n = dplyr::n()) |&gt;\n    dplyr::collect() |&gt;\n    dplyr::arrange(months) |&gt;\n    dplyr::mutate(months = c(month.abb),\n                  months = forcats::as_factor(months),\n                  months = forcats::fct_inorder(months)) |&gt;\n    duckdb::dbWriteTable(conn = dbconn,\n                         name = \"db/freq_month.db\",\n                         check_from = FALSE,\n                         overwrite = TRUE)\n\n# For the trip times.\ndplyr::tbl(dbconn,\n           tblPath_fltrd) |&gt;\n    dplyr::mutate(trip_time = round(trip_time,\n                                    digits = 0)) |&gt;\n    dplyr::group_by(trip_time) |&gt;\n    dplyr::summarize(n = dplyr::n()) |&gt;\n    dplyr::arrange(trip_time) |&gt;\n    dplyr::collect() |&gt;\n    duckdb::dbWriteTable(conn = dbconn,\n                         name = \"db/freq_tripTime.db\",\n                         check_from = FALSE,\n                         overwrite = TRUE)\n\n# For the start station names\ndplyr::tbl(dbconn,\n           tblPath_fltrd) |&gt;\n    dplyr::select(start_station_name) |&gt;\n    dplyr::group_by(start_station_name) |&gt;\n    dplyr::summarize(n = dplyr::n()) |&gt;\n    dplyr::arrange(start_station_name) |&gt;\n    dplyr::collect() |&gt;\n    duckdb::dbWriteTable(conn = dbconn,\n                         name = \"db/freq_startNames.db\",\n                         check_from = FALSE,\n                         overwrite = TRUE)\n\n\n# For the station name pairs.\ndplyr::tbl(dbconn,\n           tblPath_fltrd) |&gt;\n    dplyr::select(start_station_name,\n                  end_station_name) |&gt;\n    dplyr::group_by(start_station_name,\n                    end_station_name) |&gt;\n    dplyr::summarize(n = dplyr::n()) |&gt;\n    dplyr::arrange(start_station_name) |&gt;\n    dplyr::collect() |&gt;\n    duckdb::dbWriteTable(conn = dbconn,\n                         name = \"db/freq_pairStations.db\",\n                         check_from = FALSE,\n                         overwrite = TRUE)\n\n\n\n\n\n\n\nMemberships\nCycle Types\nMonths\nDay of Week\nMiles\nMph\nTrip-Times\n\n\n\nMEMBERSHIPS\n\nThere are nearly twice as many trips taken by annual subscribers than by casual users. Maybe there is a relationship between how often a person uses Divvy Bikes service and whether they make the decision to subscribe to an annual membership plan.\n\n\n\n\nCodedplyr::tbl(dbconn,\n           \"db/freq_member.db\") |&gt;\n    gt::gt() |&gt;\n    gt::tab_header(\n        title = \"Subscriber Frequency\",\n        subtitle = \"Annual and casual rider trip counts\"\n    ) |&gt;\n    gt::tab_options(\n        table.background.color = 'transparent',\n        table.font.color = \"SeaShell\",\n        table_body.vlines.color = \"gray20\",\n        table_body.hlines.color = \"gray20\",\n        column_labels.hidden = TRUE\n        )\n\n\n\n\n\n\nSubscriber Frequency\n\n\nAnnual and casual rider trip counts\n\n\n\n\ncasual\n1260621\n\n\nmember\n2636777\n\n\n\n\n\n\n\n\n\nCodemember_freqTbl &lt;- dplyr::tbl(dbconn,\n                             \"db/freq_member.db\") |&gt;\n    dplyr::collect()\n\n\nmember_freqTbl |&gt;\n    ggplot2::ggplot(mapping = ggplot2::aes(x = member_casual,\n                                           y = n,\n                                           fill = n)) +\n    ggplot2::geom_col(show.legend = FALSE) +\n    ggplot2::labs(title = \"Membership Frequency\",\n                  subtitle = \"Trips taken by annual subscribers vs non-subscribers\") +\n    ggplot2::scale_fill_distiller(palette = \"YlOrRd\") +\n    ggplot2::theme(\n        panel.background = ggplot2::element_rect(fill = \"#222222\",\n                                                 color = NA),\n        plot.background = ggplot2::element_rect(fill = \"#222222\",\n                                                color = NA),\n        text = ggplot2::element_text(color = \"seashell\"),\n        panel.grid = ggplot2::element_blank(),\n        axis.title.x = ggplot2::element_text(margin = grid::unit(c(5, 5, 5, 5), \"mm\")),\n        axis.title.y = ggplot2::element_text(margin = grid::unit(c(5, 5, 5, 5), \"mm\")),\n        axis.text.x = ggplot2::element_text(color = \"Snow\",\n                                            margin = grid::unit(c(1, 1, 1, 1), \"mm\")),\n        axis.text.y = ggplot2::element_text(color = \"Snow\",\n                                            margin = grid::unit(c(2, 2, 2, 2), \"mm\")),\n        axis.ticks = ggplot2::element_line(color = \"LavenderBlush\"),\n        axis.ticks.y = ggplot2::element_blank(),\n        panel.grid.major.y = ggplot2::element_line(color = \"grey30\")\n    )\n\n\n\n\n\n\n\n\n\n\n\n\nCYCLE TYPES\n\nThere are nearly twice as many trips taken with non-electric bikes as electric bikes. Perhaps, the associated health benefits with exercise would help explain higher non-electric bicycle use?\n\n\n\n\nCodedplyr::tbl(dbconn,\n           \"db/freq_rTypes.db\") |&gt;\n     gt::gt() |&gt;\n        gt::tab_header(\n            title = \"Bicycle Type Frequency\",\n            subtitle = \"Counts trips taken by cycle type\"\n        ) |&gt;\n        gt::tab_options(\n            table.background.color = 'transparent',\n            table.font.color = \"SeaShell\",\n            table_body.vlines.color = \"gray20\",\n            table_body.hlines.color = \"gray20\",\n            column_labels.hidden = TRUE\n            )\n\n\n\n\n\n\nBicycle Type Frequency\n\n\nCounts trips taken by cycle type\n\n\n\n\nelectric_bike\n1443673\n\n\nclassic_bike\n2453725\n\n\n\n\n\n\n\n\n\nCoderidetype_freqTbl &lt;- dplyr::tbl(dbconn,\n                             \"db/freq_rTypes.db\") |&gt;\n    dplyr::collect()\n\nridetype_freqTbl |&gt;\n    ggplot2::ggplot(mapping = ggplot2::aes(x = rideable_type,\n                                           y = n,\n                                           fill = n)) +\n    ggplot2::geom_col(show.legend = FALSE) +\n    ggplot2::scale_fill_distiller(palette = \"YlOrRd\") +\n    ggplot2::labs(title = \"Cycle Type Frequency\",\n                  subtitle = \"Trips taken per cycle type\") +\n    ggplot2::theme(\n        panel.background = ggplot2::element_rect(fill = \"#222222\",\n                                                 color = NA),\n        plot.background = ggplot2::element_rect(fill = \"#222222\",\n                                                color = NA),\n        text = ggplot2::element_text(color = \"seashell\"),\n        panel.grid = ggplot2::element_blank(),\n        axis.title.x = ggplot2::element_text(margin = grid::unit(c(5, 5, 5, 5), \"mm\")),\n        axis.title.y = ggplot2::element_text(margin = grid::unit(c(5, 5, 5, 5), \"mm\")),\n        axis.text.x = ggplot2::element_text(color = \"Snow\",\n                                            margin = grid::unit(c(1, 1, 1, 1), \"mm\")),\n        axis.text.y = ggplot2::element_text(color = \"Snow\",\n                                            margin = grid::unit(c(2, 2, 2, 2), \"mm\")),\n        axis.ticks = ggplot2::element_line(color = \"LavenderBlush\"),\n        axis.ticks.y = ggplot2::element_blank(),\n        panel.grid.major.y = ggplot2::element_line(color = \"grey30\")\n    )\n\n\n\n\n\n\n\n\n\n\n\n\nMONTHS\n\nTrips by month does differ substantially between certain months. late Spring , Summer, to early Fall months in Chicago is when we see the largest increase in ridership. Most likely the ambient temperature is a major contributor for the seasonal patterns in trips taken.\n\n\n\n\nCodedplyr::tbl(dbconn,\n           \"db/freq_month.db\") |&gt;\n    gt::gt() |&gt;\n    gt::tab_header(\n        title = \"Monthly Trips Frequency\",\n        subtitle = \"Tallies trips taken by month\") |&gt;\n    gt::tab_options(\n        table.background.color = 'transparent',\n        table.font.color = \"SeaShell\",\n        table_body.vlines.color = \"gray20\",\n        table_body.hlines.color = \"gray20\",\n        column_labels.hidden = TRUE\n    )\n\n\n\n\n\n\nMonthly Trips Frequency\n\n\nTallies trips taken by month\n\n\n\n\nJan\n136886\n\n\nFeb\n136818\n\n\nMar\n183129\n\n\nApr\n286343\n\n\nMay\n408373\n\n\nJun\n474756\n\n\nJul\n502519\n\n\nAug\n519423\n\n\nSep\n461164\n\n\nOct\n374268\n\n\nNov\n257513\n\n\nDec\n156206\n\n\n\n\n\n\n\n\n\nCodemonthlyTable &lt;- dplyr::tbl(dbconn,\n           \"db/freq_month.db\") |&gt;\n    dplyr::collect()\n    \nmonthlyTable |&gt;\n    ggplot2::ggplot(mapping = ggplot2::aes(x = months,\n                                           y = n,\n                                           fill = n)) +\n    ggplot2::geom_col(show.legend = FALSE) +\n    ggplot2::labs(title = \"Monthly Trip Frequency\",\n                  subtitle = \"A total count of trips taken by month\") +\n    ggplot2::scale_fill_distiller(palette = \"YlOrRd\") +\n    ggplot2::theme(\n        panel.background = ggplot2::element_rect(fill = \"#222222\",\n                                                 color = NA),\n        plot.background = ggplot2::element_rect(fill = \"#222222\",\n                                                color = NA),\n        text = ggplot2::element_text(color = \"seashell\"),\n        panel.grid = ggplot2::element_blank(),\n        axis.title.x = ggplot2::element_text(margin = grid::unit(c(5, 5, 5, 5), \"mm\")),\n        axis.title.y = ggplot2::element_text(margin = grid::unit(c(5, 5, 5, 5), \"mm\")),\n        axis.text.x = ggplot2::element_text(color = \"Snow\",\n                                            margin = grid::unit(c(1, 1, 1, 1), \"mm\")),\n        axis.text.y = ggplot2::element_text(color = \"Snow\",\n                                            margin = grid::unit(c(2, 2, 2, 2), \"mm\")),\n        axis.ticks = ggplot2::element_line(color = \"LavenderBlush\"),\n        axis.ticks.y = ggplot2::element_blank(),\n        panel.grid.major.y = ggplot2::element_line(color = \"grey30\")\n    )\n\n\n\n\n\n\n\n\n\n\n\n\nDAY OF WEEK\n\nThere is a slight reduction in trips taken on Sundays and Mondays. This could be explained by the fact that most trips are taken by annual subscribers, possibly because they are commuting to work and such. So, much less people will be biking to work on Sundays. Taking 3-day weekends is common in the workplace, where most would likely opt to use up their sick/vacation days on Monday. That might explain the down tick in trips taken on Mondays. Also, those who are working Tuesdays-Saturdays could be taking Sunday and Mondays off.\n\n\n\n\nCodedplyr::tbl(dbconn,\n           \"db/freq_wkday.db\") |&gt;\n    gt::gt() |&gt;\n    gt::tab_header(\n        title = \"Weekday Trip Frequency\",\n        subtitle = \"Tallies trips taken by the day of the week\") |&gt;\n    gt::tab_options(\n        table.background.color = 'transparent',\n        table.font.color = \"SeaShell\",\n        table_body.vlines.color = \"gray20\",\n        table_body.hlines.color = \"gray20\",\n        column_labels.hidden = TRUE\n        )\n\n\n\n\n\n\nWeekday Trip Frequency\n\n\nTallies trips taken by the day of the week\n\n\n\n\nSun\n486275\n\n\nMon\n507355\n\n\nTue\n578095\n\n\nWed\n585266\n\n\nThu\n597363\n\n\nFri\n566008\n\n\nSat\n577036\n\n\n\n\n\n\n\n\n\nCodeweekdayTable &lt;- dplyr::tbl(dbconn,\n           \"db/freq_wkday.db\") |&gt;\n    dplyr::collect()\n    \nweekdayPlot &lt;- weekdayTable |&gt;\n    ggplot2::ggplot(mapping = ggplot2::aes(x = wkday,\n                                           y = n,\n                                           fill = n)) +\n    ggplot2::coord_cartesian(ylim = c(4 * 10 ^ 5, NA)) +\n    ggplot2::geom_col(show.legend = FALSE) +\n    ggplot2::labs(title = \"Weekday Trip Frequency\",\n                  subtitle = \"Trips by day of week\") +\n    ggplot2::scale_fill_distiller(palette = \"YlOrRd\") +\n    ggplot2::theme(\n        panel.background = ggplot2::element_rect(fill = \"#222222\",\n                                                 color = NA),\n        plot.background = ggplot2::element_rect(fill = \"#222222\",\n                                                color = NA),\n        text = ggplot2::element_text(color = \"seashell\"),\n        panel.grid = ggplot2::element_blank(),\n        axis.title.x = ggplot2::element_text(margin = grid::unit(c(5, 5, 5, 5), \"mm\")),\n        axis.title.y = ggplot2::element_text(margin = grid::unit(c(5, 5, 5, 5), \"mm\")),\n        axis.text.x = ggplot2::element_text(color = \"Snow\",\n                                            margin = grid::unit(c(1, 1, 1, 1), \"mm\")),\n        axis.text.y = ggplot2::element_text(color = \"Snow\",\n                                            margin = grid::unit(c(2, 2, 2, 2), \"mm\")),\n        axis.ticks = ggplot2::element_line(color = \"LavenderBlush\"),\n        axis.ticks.y = ggplot2::element_blank(),\n        panel.grid.major.y = ggplot2::element_line(color = \"grey30\")\n    )\n\nweekdayPlot\n\n\n\n\n\n\n\n\n\n\n\n\nMILES\n\nThis shows that around 0.3 to 3 miles is the distance traveled on most trips taken. The fact that casual bicycles are used more often perhaps makes a little more sense considering that people are not traveling long distances.\n\n\n\n\nCodedplyr::tbl(dbconn,\n           \"db/freq_miles.db\") |&gt;\n    dplyr::arrange(desc(n)) |&gt;\n    head(n = 10) |&gt;\n    gt::gt() |&gt;\n        gt::tab_header(\n            title = \"Trip Distance Frequency\",\n            subtitle = \"Distance is measured in miles\"\n        ) |&gt;\n        gt::tab_options(\n            table.background.color = 'transparent',\n            table.font.color = \"SeaShell\",\n            table_body.vlines.color = \"gray20\",\n            table_body.hlines.color = \"gray20\"\n        )\n\n\n\n\n\n\nTrip Distance Frequency\n\n\nDistance is measured in miles\n\n\nmiles\nn\n\n\n\n\n1.0\n867678\n\n\n2.0\n720842\n\n\n3.0\n292483\n\n\n0.6\n288164\n\n\n0.5\n280367\n\n\n0.8\n262418\n\n\n0.7\n250369\n\n\n0.9\n227646\n\n\n0.4\n225723\n\n\n0.3\n178150\n\n\n\n\n\n\n\n\n\nCodemilesTable &lt;- dplyr::tbl(dbconn,\n           \"db/freq_miles.db\") |&gt;\n    dplyr::collect() |&gt;\n    dplyr::mutate(n = n/1000) |&gt;\n    dplyr::rename(\"Trips (in thousands)\" = n,\n                  \"Miles\" = miles)\n\nmilesPlot &lt;- milesTable |&gt;\n    ggplot2::ggplot(mapping = ggplot2::aes(x = Miles,\n                                           y = `Trips (in thousands)`,\n                                           color = `Trips (in thousands)`)) +\n    ggplot2::geom_point(show.legend = FALSE,\n                        size = 2) +\n    #ggplot2::scale_size_area() +\n    ggplot2::scale_color_distiller(palette = \"YlOrRd\") +\n\n    ggplot2::labs(title = \"Trip Distance Frequency\",\n                  subtitle = \"Measured in miles\") +\n    ggplot2::theme(\n        panel.background = ggplot2::element_rect(fill = \"#222222\",\n                                                 color = NA),\n        plot.background = ggplot2::element_rect(fill = \"#222222\",\n                                                color = NA),\n        text = ggplot2::element_text(color = \"seashell\"),\n        panel.grid = ggplot2::element_blank(),\n        axis.title.x = ggplot2::element_text(margin = grid::unit(c(5, 5, 5, 5), \"mm\")),\n        axis.title.y = ggplot2::element_text(margin = grid::unit(c(5, 5, 5, 5), \"mm\")),\n        axis.text.x = ggplot2::element_text(color = \"Snow\",\n                                            margin = grid::unit(c(1, 1, 1, 1), \"mm\")),\n        axis.text.y = ggplot2::element_text(color = \"Snow\",\n                                            margin = grid::unit(c(2, 2, 2, 2), \"mm\")),\n        axis.ticks = ggplot2::element_line(color = \"LavenderBlush\"),\n        axis.ticks.y = ggplot2::element_blank(),\n        panel.grid.major.y = ggplot2::element_line(color = \"grey30\") \n        )\n        \nmilesPlot\n\n\n\n\n\n\n\n\n\n\n\n\nMPH\n\nThe most common overall mph for trips fall in between 2-11 mph. This is what we might expect for city travel.\n\n\n\n\nCodedplyr::tbl(dbconn,\n           \"db/freq_mph.db\") |&gt;\n    gt::gt() |&gt;\n    gt::tab_header(\n        title = \"Mean Speed Frequency\",\n        subtitle = \"Calculated average speed over the duration of the trip\") |&gt;\n    gt::tab_options(\n        table.background.color = 'transparent',\n        table.font.color = \"SeaShell\",\n        table_body.vlines.color = \"gray20\",\n        table_body.hlines.color = \"gray20\",\n        column_labels.hidden = TRUE\n        )\n\n\n\n\n\n\nMean Speed Frequency\n\n\nCalculated average speed over the duration of the trip\n\n\n\n\n1\n45485\n\n\n2\n106779\n\n\n3\n161520\n\n\n4\n278819\n\n\n5\n460475\n\n\n6\n622275\n\n\n7\n647253\n\n\n8\n546353\n\n\n9\n399237\n\n\n10\n265438\n\n\n11\n165825\n\n\n12\n99529\n\n\n13\n55660\n\n\n14\n27274\n\n\n15\n11165\n\n\n16\n3354\n\n\n17\n714\n\n\n18\n138\n\n\n19\n69\n\n\n20\n36\n\n\n\n\n\n\n\n\n\nCodemphTable &lt;- dplyr::tbl(dbconn,\n                       \"db/freq_mph.db\") |&gt;\n    dplyr::collect()\n\nmphPlot &lt;- mphTable |&gt;\n    ggplot2::ggplot(mapping = ggplot2::aes(x = mph,\n                                           y = n,\n                                           color = n)) +\n    ggplot2::geom_point(show.legend = FALSE) +\n    ggplot2::labs(title = \"Mean Speed Frequency\",\n                  subtitle = \"Calculated average speed over the duration of the trip\") +\n    ggplot2::scale_color_distiller(palette = \"YlOrRd\") +\n    ggplot2::theme(\n        panel.background = ggplot2::element_rect(fill = \"#222222\",\n                                                 color = NA),\n        plot.background = ggplot2::element_rect(fill = \"#222222\",\n                                                color = NA),\n        text = ggplot2::element_text(color = \"seashell\"),\n        panel.grid = ggplot2::element_blank(),\n        axis.title.x = ggplot2::element_text(margin = grid::unit(c(5, 5, 5, 5), \"mm\")),\n        axis.title.y = ggplot2::element_text(margin = grid::unit(c(5, 5, 5, 5), \"mm\")),\n        axis.text.x = ggplot2::element_text(color = \"Snow\",\n                                            margin = grid::unit(c(1, 1, 1, 1), \"mm\")),\n        axis.text.y = ggplot2::element_text(color = \"Snow\",\n                                            margin = grid::unit(c(2, 2, 2, 2), \"mm\")),\n        axis.ticks = ggplot2::element_line(color = \"LavenderBlush\"),\n        axis.ticks.y = ggplot2::element_blank(),\n        panel.grid.major.y = ggplot2::element_line(color = \"grey30\")\n    )\n\nmphPlot\n\n\n\n\n\n\n\n\n\n\n\n\nTRIP-TIMES\n\nThe time riders are usually spending on these trips lies in between 5-15 minutes.\n\n\n\n\nCodedplyr::tbl(dbconn,\n           \"db/freq_tripTime.db\") |&gt;\n    dplyr::collect() |&gt;\n    dplyr::arrange(desc(n)) |&gt;\n    head(n = 20) |&gt;\n    gt::gt() |&gt;\n    gt::tab_header(\n        title = \"Trip Duration Frequency\",\n        subtitle = \"Tallies the duration of time people rent cycles for\") |&gt;\n    gt::tab_options(\n        table.background.color = 'transparent',\n        table.font.color = \"SeaShell\",\n        table_body.vlines.color = \"gray20\",\n        table_body.hlines.color = \"gray20\",\n        column_labels.hidden = TRUE)\n\n\n\n\n\n\nTrip Duration Frequency\n\n\nTallies the duration of time people rent cycles for\n\n\n\n\n5\n293803\n\n\n6\n286755\n\n\n4\n277279\n\n\n7\n269364\n\n\n8\n248761\n\n\n9\n225902\n\n\n3\n214975\n\n\n10\n203172\n\n\n11\n180040\n\n\n12\n159778\n\n\n13\n142474\n\n\n14\n126117\n\n\n15\n111231\n\n\n2\n106938\n\n\n16\n98928\n\n\n17\n87472\n\n\n18\n78248\n\n\n19\n70288\n\n\n20\n62967\n\n\n21\n56145"
  },
  {
    "objectID": "index.html#trip-network",
    "href": "index.html#trip-network",
    "title": "Case study: Bike-sharing program in the chicago area",
    "section": "Trip Network",
    "text": "Trip Network\n\n\n\nStepping Through the Code\n\n\n\n\n\n\n\n\nCodeflowData &lt;- dplyr::tbl(dbconn,\n                       tblPath_fltrd) |&gt;\n    dplyr::select(start_station_name,\n                  end_station_name) |&gt;\n    dplyr::group_by(start_station_name,\n                    end_station_name) |&gt;\n    dplyr::summarize(n = n()) |&gt;\n    dplyr::ungroup() |&gt;\n    dplyr::arrange(desc(n)) |&gt;\n    dplyr::rename(\"from_station\" = start_station_name,\n                  \"to_station\" = end_station_name) |&gt;\n    dplyr::collect() |&gt;\n    dplyr::slice_head(n = 50)\n\nflowData50 &lt;- flowData |&gt;\n    dplyr::slice(1:50)\n\nflowData1 &lt;- flowData |&gt;\n    dplyr::slice(1:10)\n\nflowData2 &lt;- flowData |&gt;\n    dplyr::slice(11:20)\n\nflowData3 &lt;- flowData |&gt;\n    dplyr::slice(21:30)\n\nflowData4 &lt;- flowData |&gt;\n    dplyr::slice(31:40)\n\nflowData5 &lt;- flowData |&gt;\n    dplyr::slice(41:50)\n\n\n\nNow we need to obtain statistics but also combine the statistics for every unique station name.locationData &lt;- dplyr::tbl(dbconn,\n                            tblPath_fltrd) |&gt;\n    dplyr::select(start_station_name,\n                  end_station_name,\n                  started_at,\n                  ended_at,\n                  trip_time) |&gt;\n    dplyr::group_by(start_station_name,\n                    end_station_name\n                ) |&gt;\n    dplyr::mutate(\"trip_time\" = round(trip_time,\n                                      digits = 0)) |&gt;\n    dplyr::summarize(\n        \"trip_count\" = dplyr::n(),\n        \"first_date\" = min(started_at),\n        \"last_date\" = max(ended_at),\n        \"avg_trip_time\" = mean(trip_time)\n    ) |&gt;\n    dplyr::ungroup() |&gt;\n    dplyr::rename(\"from_station\" = start_station_name,\n                  \"to_station\" = end_station_name\n               ) |&gt;\n    dplyr::arrange(desc(trip_count)) |&gt;\n    dplyr::collect()\n\n# Need to combine all names to single column and recalculate \n# or retain other stats.\nlocationData_pivoted &lt;- locationData |&gt;\n    tidyr::pivot_longer(cols = 1:2, \n                        values_to = \"allNames\") |&gt;\n    dplyr::group_by(allNames) |&gt;\n    dplyr::summarize(\"trips_toAndfrom\" = sum(trip_count),\n                     first_date = min(first_date),\n                     last_date = max(last_date),\n                     avg_trip_time = mean(avg_trip_time)\n                     ) |&gt;\n    dplyr::mutate(avg_trip_time = round(avg_trip_time,\n                                      digits = 0)) |&gt;\n\n    dplyr::arrange(trips_toAndfrom)\n\n\n\nCodeef_test50 &lt;- epiflows::make_epiflows(flows = flowData50,\n                                   locations = locationData_pivoted,\n                                   duration_stay = \"avg_trip_time\",\n                                   num_cases = \"trips_toAndfrom\")\n\n\nef_test1 &lt;- epiflows::make_epiflows(flows = flowData1,\n                                   locations = locationData_pivoted,\n                                   duration_stay = \"avg_trip_time\",\n                                   num_cases = \"trips_toAndfrom\")\n\nef_test2 &lt;- epiflows::make_epiflows(flows = flowData2,\n                                   locations = locationData_pivoted,\n                                   duration_stay = \"avg_trip_time\",\n                                   num_cases = \"trips_toAndfrom\")\n\nef_test3 &lt;- epiflows::make_epiflows(flows = flowData3,\n                                   locations = locationData_pivoted,\n                                   duration_stay = \"avg_trip_time\",\n                                   num_cases = \"trips_toAndfrom\")\n\nef_test4 &lt;- epiflows::make_epiflows(flows = flowData4,\n                                   locations = locationData_pivoted,\n                                   duration_stay = \"avg_trip_time\",\n                                   num_cases = \"trips_toAndfrom\")\n\nef_test5 &lt;- epiflows::make_epiflows(flows = flowData5,\n                                   locations = locationData_pivoted,\n                                   duration_stay = \"avg_trip_time\",\n                                   num_cases = \"trips_toAndfrom\")\n\n\n\n\n\n\n\n\nTables\n\n\n\n\n\n\n\n\nCodeflowData\n\n# A tibble: 50 × 3\n   from_station                      to_station                   n\n   &lt;chr&gt;                             &lt;chr&gt;                    &lt;dbl&gt;\n 1 Ellis Ave & 60th St               Ellis Ave & 55th St       6927\n 2 Ellis Ave & 60th St               University Ave & 57th St  6600\n 3 Ellis Ave & 55th St               Ellis Ave & 60th St       6349\n 4 University Ave & 57th St          Ellis Ave & 60th St       6168\n 5 Calumet Ave & 33rd St             State St & 33rd St        5417\n 6 State St & 33rd St                Calumet Ave & 33rd St     5343\n 7 DuSable Lake Shore Dr & Monroe St Streeter Dr & Grand Ave   4023\n 8 Loomis St & Lexington St          Morgan St & Polk St       3719\n 9 Morgan St & Polk St               Loomis St & Lexington St  3379\n10 University Ave & 57th St          Kimbark Ave & 53rd St     3112\n# ℹ 40 more rows\n\nCodesummary(flowData$n)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1363    1510    1878    2443    2599    6927 \n\n\n\nCodelocationData_pivoted |&gt;\n    dplyr::arrange(desc(trips_toAndfrom))\n\n# A tibble: 1,567 × 5\n   allNames              trips_toAndfrom first_date          last_date          \n   &lt;chr&gt;                           &lt;dbl&gt; &lt;dttm&gt;              &lt;dttm&gt;             \n 1 Streeter Dr & Grand …           86422 2023-01-01 00:05:43 2024-01-01 00:19:01\n 2 Kingsbury St & Kinzi…           61277 2023-01-01 01:21:59 2023-12-31 21:30:50\n 3 DuSable Lake Shore D…           60808 2023-01-01 02:12:09 2023-12-31 23:34:53\n 4 Clark St & Elm St               60552 2023-01-01 01:06:48 2023-12-31 23:29:33\n 5 Clinton St & Washing…           58278 2023-01-01 00:44:39 2023-12-31 18:03:02\n 6 Wells St & Concord Ln           57642 2023-01-01 01:15:27 2023-12-31 23:51:50\n 7 Michigan Ave & Oak St           54000 2023-01-01 00:59:17 2023-12-31 23:09:35\n 8 Wells St & Elm St               52315 2023-01-01 00:59:22 2023-12-31 23:51:48\n 9 DuSable Lake Shore D…           48833 2023-01-01 00:14:47 2023-12-31 16:49:56\n10 Theater on the Lake             48349 2023-01-01 03:14:22 2023-12-31 22:53:53\n# ℹ 1,557 more rows\n# ℹ 1 more variable: avg_trip_time &lt;dbl&gt;\n\n\n\n\n\n\n\n\n1-50\n1-10\n11-20\n21-30\n31-40\n41-50"
  },
  {
    "objectID": "index.html#conclusion",
    "href": "index.html#conclusion",
    "title": "Case study: Bike-sharing program in the chicago area",
    "section": "Conclusion",
    "text": "Conclusion\nThere isn’t much of a difference between annual subscriber and casual trip behavior other than trip frequency. Annual subs are people who ride more frequently. Looking into where the main activity is, you can see that they are near stores, schools, and such."
  }
]