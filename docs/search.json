[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bike_Share",
    "section": "",
    "text": "BikeShare\n\n\n\n\n\n\n\n\n\n\n\nEric Mossotti\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "bikeShare.html",
    "href": "bikeShare.html",
    "title": "BikeShare",
    "section": "",
    "text": "Bike-share research (bikeshare-research.org)\nDivvy - Wikipedia\nCycling in Chicago - Wikipedia\nHome | Divvy Bikes\nDivvy Membership & Pass Options | Divvy Bikes\n(‚ÄúData License Agreement | Divvy Bikes,‚Äù n.d.)\n\nData License Agreement | Divvy Bikes\n\n(‚ÄúMOTIVATE,‚Äù n.d.)\n\nMOTIVATE (motivateco.com)\n\n(‚ÄúIndex of Bucket \"Divvy-Tripdata\",‚Äù n.d.)\n\nIndex of bucket ‚Äúdivvy-tripdata‚Äù\n\n(‚ÄúWhy DuckDB,‚Äù n.d.)\n\nWhy DuckDB?\n\n(‚ÄúR for Data Science (2e) - 22¬† Arrow,‚Äù n.d.)\n\nR for Data Science: Chapter 22: Arrow\n\n(‚ÄúGreat-Circle Distance - Wikipedia,‚Äù n.d.)\n\nhttps://en.wikipedia.org/wiki/Great-circle_distance\n\n(‚ÄúAverage Bicycle Speed  How Fast Do Cyclists Ride and What Affects Their Pace - BikingulTimate.com (UPDATE üëç)‚Äù 2024)\n\nhttps://bikingultimate.com/average-bicycle-speed-how-fast-do-cyclists-ride-and-what-affects-their-pace/\n\n\nObjective:"
  },
  {
    "objectID": "bikeShare.html#filtering-data-smartly",
    "href": "bikeShare.html#filtering-data-smartly",
    "title": "BikeShare",
    "section": "Filtering Data, Smartly",
    "text": "Filtering Data, Smartly\nTo ensure the conclusions are accurate, outliers should be filtered. Negative and very low trip times might skew trends. The underlying reason for very low trip times is somewhat of an unknown. Perhaps people often change their minds?\n\n\nCode\n# So you don't have to re-download or re-filter everything after making further adjustments.\n\ntblPath &lt;- \"db/data.db\"\ntblPath_fltrd &lt;- \"db/data_fltrd.db\"\n\n\nif(exists(\"dbconn\") == FALSE) {\n    dbconn &lt;- DBI::dbConnect(duckdb::duckdb(),\n                             dbdir = tblPath,\n                             read_only = FALSE,\n                             check_from = FALSE)\n}\n\n\nif (duckdb::dbExistsTable(dbconn,\n                          \"tblPath_fltrd\") == FALSE) {\n    source(\"filterDatabase.R\")\n    filterDatabase()\n    }\n\n# To verify the new filtered table exists.\nduckdb::dbListTables(dbconn)\n\n\n[1] \"db/data.db\"       \"db/data_fltrd.db\"\n\n\n\n\nCode\n#|eval: FALSE\n\n# If you need to drop any tables\nsource(\"duckDrops.R\")\n\n\nRemove which tables? (separate by comma)\n\n\nSo this should have removed the major, potentially, non-natural outliers from the dataset which are due to errors (including user errors).\nVerify the tables that now exist:\n\n\nCode\nduckdb::dbListTables(dbconn)\n\n\n[1] \"db/data.db\"       \"db/data_fltrd.db\""
  },
  {
    "objectID": "bikeShare.html#visualizations",
    "href": "bikeShare.html#visualizations",
    "title": "BikeShare",
    "section": "",
    "text": "for quick reference with using Tsibble syntax\n\n\nCode\n#|label: 'grouped tibb'\n\ngrouped_byDay &lt;- dplyr::tbl(dbconn_fltrd,\n                            tblPath_fltrd) |&gt;\n    dplyr::select(started_at,\n                  member_casual) |&gt;\n    dplyr::collect() |&gt;\n    dplyr::mutate(started_at = as.Date(started_at)) |&gt;\n    dplyr::group_by(started_at,\n                    member_casual) |&gt;\n    dplyr::summarize(n = dplyr::n(),\n                     sdev = stats::sd(n))\n\n\n\n\nCode\n#|label: 'to grouped tsibb'\n\n# tsibble, time-series table/tibble seems to make time series plots more straightforward\ngrouped_tsi &lt;- grouped_byDay |&gt;\n    tsibble::as_tsibble(key = c(member_casual,\n                                n),\n                        index = started_at) |&gt;\n    dplyr::arrange(started_at)\n\n\n\n\nCode\n#|label: \"map query setup\"\n\n# chicago starting coordinates for leaflet, setView\nchicago &lt;- maps::us.cities |&gt;\n    dplyr::select(\"name\",\n                  \"long\",\n                  \"lat\") |&gt;\n    dplyr::filter(name == \"Chicago IL\")\n\n# full dataset coordinates, might need to sample\ncoordQry &lt;- dplyr::tbl(dbconn_fltrd,\n                       tblPath_fltrd) |&gt;\n    dplyr::select(start_lng,\n                  start_lat) |&gt;\n    dplyr::add_count(start_lng,\n                     start_lat) |&gt;\n    dplyr::distinct() |&gt;\n    dplyr::arrange(desc(n)) |&gt;\n    dplyr::collect()\n\n\ncoordQry_small &lt;- dplyr::tbl(dbconn_fltrd,\n                             tblPath_fltrd) |&gt;\n    dplyr::select(start_lng,\n                  start_lat) |&gt;\n    dplyr::add_count(start_lng,\n                     start_lat) |&gt;\n    dplyr::distinct() |&gt;\n    dplyr::arrange(desc(n)) |&gt;\n    dplyr::collect() |&gt;\n    dplyr::slice_head(n = 50)\n\n\n\n\nCode\n#|label: \"mapview\"\n\ncoordQry_small |&gt;\n    sf::st_as_sf(coords = c(1:2),\n                crs = 4326) |&gt;\n    mapview::mapview()\n\n\n\n\n\n\n\n\nCode\nojs_define(js_tsi = grouped_tsi)\n\n\n\n\nCode\njsData = transpose(js_tsi)\n\n\n\n\n\n\n\n\n\n\nCode\nPlot.plot({\n    grid: true,\n    color: {legend: true},\n    marks: [\n        Plot.dot(jsData, {x: 'started_at', y: 'n', fill: 'member_casual'})\n]\n})\n\n\n\n\n\n\n\n\n\n\nCode\nPlot.lineY(jsData, {x: \"started_at\", y: \"n\"}).plot()\n\n\n\n\n\n\n\n\n\nCode\nInputs.table(jsData, {\nrows: 20\n})\n\n\n\n\n\n\n\n\n\nCode\n#|label: 'grouped stats tibb'\n\n# Need a useful data frame for basic aggregations\ngroupedStats_byDay &lt;- dplyr::tbl(dbconn_fltrd,\n                                 tblPath_fltrd) |&gt;\n    dplyr::select(started_at,\n                  member_casual,\n                  rideable_type,\n                  trip_time,\n                  miles,\n                  mph) |&gt;\n    dplyr::collect() |&gt;\n    dplyr::mutate(started_at = as.Date(started_at)) |&gt;\n    dplyr::group_by(started_at,\n                    member_casual,\n                    rideable_type) |&gt;\n    dplyr::summarize(n = dplyr::n(),\n                     trip_time_Mean = mean(trip_time),\n                     trip_time_stDev = stats::sd(trip_time),\n                     miles_Mean = mean(miles),\n                     miles_stDev = stats::sd(miles),\n                     mph_Mean = mean(miles),\n                     mph_stDev = stats::sd(mph))\n\n\n\n\nCode\n#|label: 'grouped stats tsibb'\n\n# Would prefer to work with a tsibble for time-series data\ngroupedStats_tsib &lt;- groupedStats_byDay |&gt;\n    tsibble::as_tsibble(key = c(member_casual:mph_stDev),\n                        index = started_at) |&gt;\n    dplyr::arrange(started_at)\n\n\n\n\nCode\n# A solution to help visualize these mutli-dimensional relationships of membership and bike type to time.\ngroupedStats_tsib |&gt;\n    ggplot2::ggplot(ggplot2::aes(x = started_at,\n                                 y = trip_time_Mean,\n                                 color = trip_time_Mean)) +\n    ggplot2::geom_count(size = 3) +\n    ggplot2::facet_wrap(~member_casual+rideable_type) +\n    ggplot2::scale_x_date(date_minor_breaks = \"days\",\n                          date_labels = \"%b\",\n                          breaks = \"months\") +\n    ggplot2::theme(axis.text.x = ggplot2::element_text(angle = 45,\n                                                       hjust = 1),\n                   panel.background = ggplot2::element_rect(fill = 'grey20'),\n                   panel.grid.major.y = ggplot2::element_line(\n                       linetype = 'dashed',\n                       color = 'grey30'),\n                   panel.grid.major.x = ggplot2::element_line(\n                       linetype = 'dotted',\n                       color = 'grey30'),\n                   panel.grid.minor = ggplot2::element_blank(),\n                   strip.background.x = ggplot2::element_rect(\n                       fill = 'yellowgreen')) +\n    ggplot2::scale_color_distiller(palette = 'Spectral')\n\n\n\n\n\n\n\n\n\n\n\nCode\n#|label: \"dygraph\"\n\nsumDF &lt;- groupedStats_byDay |&gt;\n    dplyr::select(started_at,\n                  n) |&gt;\n    dplyr::group_by(started_at,\n                    member_casual) |&gt;\n    dplyr::summarise(\"count\" = sum(n)) |&gt;\n    tidyr::pivot_wider(names_from = member_casual,\n                       values_from = count)\n\ndygraphs::dygraph(sumDF, main = \"ever\",\n                  ylab = \"what\") |&gt;\n    dygraphs::dyGroup(name = c(\"member\", \"casual\"),\n                       color = c(\"green\", \"red\"))\n\n\n\n\n\n\nThese maps track bike-sharing activity going on worldwide and in Chicago.\nBike Share Map (‚ÄúBike Share Map: Chicago (Divvy),‚Äù n.d.)\nCityBikes: bike sharing networks around the world (‚ÄúCityBikes: Bike Sharing Networks Around the World,‚Äù n.d.)\n\n\nCode\n#|label: 'drops duckDB tables'\n\n# a chunk for easily dropping either all (default) or specific tables from the ddb. Also made it so you can see the current tables and then the tables after running the chunk.\n\npaths &lt;- duckdb::dbListTables(dbconn)\n\npaths[]\n\ndrops_tables &lt;- function(path) {\n    \n    duckdb::dbRemoveTable(dbconn, \n                          path)\n}\n\npaths[] |&gt; purrr::walk(drops_tables)\n\npaths &lt;- duckdb::dbListTables(dbconn)\n\npaths[]\n\n\n\n\nCode\n#|label: 'duckDB Shutdown'\nduckdb::dbDisconnect(dbconn_fltrd, shutdown = TRUE)\nduckdb::dbDisconnect(dbconn, shutdown = TRUE)"
  },
  {
    "objectID": "bikeShare.html#semi-parametric-and-non-parametric-methods",
    "href": "bikeShare.html#semi-parametric-and-non-parametric-methods",
    "title": "BikeShare",
    "section": "Semi-parametric and non-parametric methods",
    "text": "Semi-parametric and non-parametric methods\nDescription\n\nTo predict the outcome variable using independent variables\n\noutcome variable(s) to test\n\nmember_casual\n\n\n\nStatistical methods\n\nBinary Logistic regression analysis\n\nData type\n\nOutcome variable:\n\nCategorical (&gt;= 2 categories)\n\nmember_casual\nrideable_type\n\n\nIndependent variable(s):\n\nCategorical (&gt;= 2 categories) or\n\nhour\n\n24 categories\n\nday_of_week\n\n7 categories\n\nmonth\n\n12 categories\n\nholiday\n\n2 categories (yes or no?)\n\nmember_casual\n\n2 categories\n\nrideable_type\n\n2 categories\n\n\nContinuous or\n\ntrip_time\nmph\nmiles\ngeospatial location\n\nboth\n\nmember_causal\nrideable_type\nhour\nday\ntrip_time\nmph\nmiles\ngeospatial location\n\n\n\n\nMean &gt; Median\n‚ÄúRight-Skewed Histogram‚Äù\n- The right side of the histogram plot has lower frequencies than the left.\nUnimodal\n\n\nCode\nlatsTable &lt;- dplyr::tbl(dbconn,\n                        tblPath_fltrd,\n                        check_from = FALSE) |&gt;\n    dplyr::select(start_lat) |&gt;\n    dplyr::collect() |&gt;\n\n    # because 5 digits after decimal place gives down to 1.1m accuracy\n    # 4 digits are accurate up to 11m\n    dplyr::mutate(start_lat = signif(start_lat, digits = 5),\n                  .keep = \"none\") |&gt;\n    table() |&gt;\n    as.data.frame()\n\n\n\n\nCode\nlongsTable &lt;- dplyr::tbl(dbconn,\n                       tblPath_fltrd,\n                       check_from = FALSE) |&gt;\n    dplyr::select(start_lng) |&gt;\n    dplyr::collect() |&gt;\n    # because 5 digits after decimal place gives down to 1.1m accuracy\n    # 4 digits are accurate up to 11m\n    dplyr::mutate(start_lng = signif(start_lng, digits = 5),\n                  .keep = \"none\") |&gt;\n    table() |&gt;\n    as.data.frame()\n\n\n\n\nCode\nlatlngTable &lt;- dplyr::tbl(dbconn,\n                       tblPath_fltrd,\n                       check_from = FALSE) |&gt;\n    dplyr::select(start_lat, start_lng) |&gt;\n    # because 5 digits after decimal place gives down to 1.1m accuracy\n    # 4 digits are accurate up to 11m\n    dplyr::collect() |&gt;\n    dplyr::mutate(start_lat = round(start_lat, digits = 3),\n                  start_lng = round(start_lng, digits = 3),\n                  .keep = \"none\") |&gt;\n    dplyr::group_by(start_lat, start_lng) |&gt;\n    #dplyr::collect() |&gt;\n    table() |&gt;\n    as.data.frame()\n\nlatlngTable |&gt;\n    head(n = 10)\n\n\n   start_lat start_lng Freq\n1     41.649   -87.844    0\n2      41.65   -87.844    0\n3     41.652   -87.844    0\n4     41.654   -87.844    0\n5     41.655   -87.844    0\n6     41.657   -87.844    0\n7     41.658   -87.844    0\n8     41.659   -87.844    0\n9      41.66   -87.844    0\n10    41.666   -87.844    0\n\n\n\n\nCode\n# Trying to get clues as to how we should alter the table. \ncoordsTable &lt;- dplyr::tbl(dbconn,\n                       tblPath_fltrd,\n                       check_from = FALSE) |&gt;\n    dplyr::select(start_lat, \n                  start_lng) |&gt;\n    # because 5 digits after decimal place gives down to 1.1m accuracy\n    # 4 digits are accurate up to 11m\n   # dplyr::filter(start_lat, signif(start_lat, digits = 7) == TRUE) |&gt;\n    #dplyr::collect() |&gt;\n    dplyr::mutate(start_lat = as.character(start_lat),\n                  start_lng = as.character(start_lng),\n                  .keep = \"none\") |&gt;\n    dplyr::filter(stringr::str_length(start_lat) &gt;= 8,\n                  # 9 to account for negative symbol in lng\n                  stringr::str_length(start_lng) &gt;= 9) |&gt;\n    dplyr::group_by(start_lat, \n                    start_lng) |&gt;\n    dplyr::summarise(n = dplyr::n()) |&gt;\n    dplyr::arrange(n) |&gt;\n    dplyr::collect() |&gt;\n    as.data.frame()\n\n\n\n\nCode\n# Trying to get clues as to how we should alter the table. \ncoordsTable2 &lt;- dplyr::tbl(dbconn,\n                           tblPath_fltrd,\n                           check_from = FALSE) |&gt;\n    dplyr::select(start_lat, start_lng) |&gt;\n    # because 5 digits after decimal place gives down to 1.1m accuracy\n    # 4 digits are accurate up to 11m\n    dplyr::mutate(start_lat = round(start_lat, digits = 5),\n                  start_lng = round(start_lng, digits = 5),\n                  start_lat = as.character(start_lat),\n                  start_lng = as.character(start_lng),\n                  .keep = \"none\") |&gt;\n    dplyr::filter(stringr::str_length(start_lat) &gt;= 8,\n                  stringr::str_length(start_lng) &gt;= 9) |&gt;\n    dplyr::group_by(start_lat, start_lng) |&gt;\n    dplyr::summarise(n = dplyr::n()) |&gt;\n  #  dplyr::filter(n &gt;= 47) |&gt;\n    dplyr::arrange(n) |&gt;\n    dplyr::collect() |&gt;\n    # seems base R's table() is a lot slower than dplyr\n    #table() |&gt;\n    as.data.frame()\n\ncoordsTable2 |&gt;\n    head(n = 10)\n\n\n   start_lat start_lng n\n1   41.78875 -87.60107 1\n2   41.88447 -87.62781 1\n3   41.79143 -87.60007 1\n4   41.90281 -87.63425 1\n5   41.89901 -87.63013 1\n6   41.87766 -87.63505 1\n7   41.88399 -87.62444 1\n8   41.88609 -87.62514 1\n9   41.87464 -87.64954 1\n10  41.88916 -87.62708 1\n\n\nAssuming their plans to expand the ‚Äúover 800 stations‚Äù by 250, I‚Äôd like to set the parameters to achieve around ~1050 rows. So I set n to include groups with at least 47 riders.\n\n\nCode\n# according to this, there are 1487 unique station id's in the dataset now\ndplyr::tbl(dbconn,\n           tblPath_fltrd,\n           check_from = FALSE) |&gt;\n    dplyr::select(start_station_name) |&gt;\n    dplyr::distinct(start_station_name) |&gt;\n    dplyr::summarise(n = dplyr::n())\n\n\n# Source:   SQL [1 x 1]\n# Database: DuckDB v0.10.0 [ecmos@Windows 10 x64:R 4.3.3/db/data.db]\n      n\n  &lt;dbl&gt;\n1  1487\n\n\nThe 5th decimal place is accurate up to 1.1m, so there ends up being groupings for bikes rented from stations located along a street or at intersections of two streets. You can tell by the start_station_name. If it‚Äôs an intersection it will say something like ‚Äú1st St & 2nd St.‚Äù. If it‚Äôs one street, it will likely just say ‚Äú1st St‚Äù. There are likely some exceptions to that rule. That helps explain why the geospatial coordinates slightly differ for entries with the same stations and IDs.\n\n\nCode\nmapTest &lt;- dplyr::tbl(dbconn,\n           tblPath_fltrd,\n           check_from = FALSE) |&gt;\n    dplyr::select(start_station_id,\n                  start_station_name,\n                  start_lat,\n                  start_lng) |&gt;\n    dplyr::group_by(start_station_name,\n                    start_station_id,\n                    start_lat,\n                    start_lng) |&gt;\n    dplyr::summarise(n = dplyr::n()) |&gt;\n    dplyr::filter(stringr::str_detect(\n        start_station_name,\n        \"Ogden Ave & Chicago Ave\") == TRUE,\n        n &gt; 1) |&gt;\n    dplyr::arrange(start_station_name, \n                   start_station_id, \n                   start_lat,\n                   start_lng,\n                   n) |&gt;\n    dplyr::collect()\n\nmapTest |&gt;\n    dplyr::ungroup() |&gt;\n    dplyr::select(start_lng, start_lat, n) |&gt;\n   # dplyr::group_by(start_lng, start_lat) |&gt;\n    sf::st_as_sf(coords = c(1:2),\n             crs = 4326) |&gt;\n    mapview::mapview()\n\n\nA quick test of the data:\n\n\nCode\ncoordsDF &lt;- mapTest |&gt;\n    dplyr::ungroup() |&gt;\n    dplyr::select(start_lng,\n                  start_lat) |&gt;\n    dplyr::add_count(start_lng,\n                     start_lat) |&gt;\n    dplyr::slice_head(n = 100)\n\ncoordsDF |&gt;\n    sf::st_as_sf(coords = c(1:2),\n                 crs = 4326) |&gt;\n    mapview::mapview()\n\n\n\n\nCode\n# according to this, there are 1428 unique station IDs in the dataset now\ncountStations &lt;- dplyr::tbl(dbconn,\n                            tblPath,\n                            check_from = FALSE) |&gt;\n    dplyr::select(start_station_id, start_station_name) |&gt;\n    #dplyr::distinct(dplyr::pick(start_station_id, start_station_name)) |&gt;\n    dplyr::group_by(start_station_id,\n                    start_station_name) |&gt;\n    dplyr::summarise(n = dplyr::n()) |&gt;\n    dplyr::filter(n &gt; 0) |&gt;\n    dplyr::collect()\n\nsummary(countStations)\n\n\n\n\nCode\nmemberCasuals_monthly  &lt;- dplyr::tbl(dbconn,\n           tblPath_fltrd) |&gt;\n    dplyr::select(started_at,\n                  member_casual) |&gt;\n    dplyr::mutate('month' = lubridate::month(started_at)) |&gt;\n    dplyr::group_by(month,\n                    member_casual) |&gt;\n    dplyr::summarize(\"riderCount\" = dplyr::n()) |&gt;\n    dplyr::arrange(month)\n\ndplyr::collect(memberCasuals_monthly) |&gt;\n    ggplot2::ggplot() +\n    ggplot2::geom_col(mapping = ggplot2::aes(x = factor(month),\n                                             y = riderCount,\n                                             fill = member_casual),\n                      color = \"black\",\n                      position = 'dodge2') +\n    ggplot2::scale_x_discrete(labels = month.abb,\n                              name = \"Month\") +\n    ggplot2::scale_fill_brewer(palette = 'Set2') +\n    ggplot2::theme_dark() +\n    ggplot2::labs(\n    title = \"Monthly Ridership: Members vs Casuals\",\n    subtitle = \"(Jan-Dec 2023)\",\n    caption = \"Data from cyclistic database.\",\n    tag = \"Figure 1.b\")\n\n\n\n\n\n\n\n\n\nfor quick reference with using Tsibble syntax\n\n\nCode\n#|label: 'grouped tibb'\n\ngrouped_byDay &lt;- dplyr::tbl(dbconn,\n                            tblPath_fltrd) |&gt;\n    dplyr::select(started_at,\n                  member_casual) |&gt;\n    dplyr::collect() |&gt;\n    dplyr::mutate(started_at = as.Date(started_at)) |&gt;\n    dplyr::group_by(started_at,\n                    member_casual) |&gt;\n    dplyr::summarize(n = dplyr::n(),\n                     sdev = stats::sd(n))\n\n\n\n\nCode\n#|label: 'to grouped tsibb'\n\n# tsibble, time-series table/tibble seems to make time series plots more straightforward\ngrouped_tsi &lt;- grouped_byDay |&gt;\n    tsibble::as_tsibble(key = c(member_casual,\n                                n),\n                        index = started_at) |&gt;\n    dplyr::arrange(started_at)\n\n\n\n\nCode\n#|label: \"map query setup\"\n\n# chicago starting coordinates for leaflet, setView\nchicago &lt;- maps::us.cities |&gt;\n    dplyr::select(\"name\",\n                  \"long\",\n                  \"lat\") |&gt;\n    dplyr::filter(name == \"Chicago IL\")\n\n# full dataset coordinates, might need to sample\ncoordQry &lt;- dplyr::tbl(dbconn,\n                       tblPath_fltrd) |&gt;\n    dplyr::select(start_lng,\n                  start_lat) |&gt;\n    dplyr::add_count(start_lng,\n                     start_lat) |&gt;\n    dplyr::distinct() |&gt;\n    dplyr::arrange(desc(n)) |&gt;\n    dplyr::collect()\n\n\ncoordQry_small &lt;- dplyr::tbl(dbconn,\n                             tblPath_fltrd) |&gt;\n    dplyr::select(start_lng,\n                  start_lat) |&gt;\n    dplyr::add_count(start_lng,\n                     start_lat) |&gt;\n    dplyr::distinct() |&gt;\n    dplyr::arrange(desc(n)) |&gt;\n    dplyr::collect() |&gt;\n    dplyr::slice_head(n = 50)\n\n\n\n\nCode\n#|label: \"mapview\"\n#|eval: false\n\n\ncoordQry_small |&gt;\n    sf::st_as_sf(coords = c(1:2),\n                crs = 4326) |&gt;\n    mapview::mapview()\n\n\n\n\n\n\n\n\nCode\nojs_define(js_tsi = grouped_tsi)\n\n\n\n\nCode\njsData = transpose(js_tsi)\n\n\n\n\n\n\n\n\n\n\nCode\nPlot.plot({\n    grid: true,\n    color: {legend: true},\n    marks: [\n        Plot.dot(jsData, {x: 'started_at', y: 'n', fill: 'member_casual'})\n]\n})\n\n\n\n\n\n\n\n\n\n\nCode\nPlot.lineY(jsData, {x: \"started_at\", y: \"n\"}).plot()\n\n\n\n\n\n\n\n\n\nCode\nlocsStart &lt;- dplyr::tbl(dbconn,\n                      tblPath_fltrd) |&gt;\n    dplyr::select(start_station_name\n                  #start_lng,\n                  #start_lat\n                  ) |&gt;\n    dplyr::distinct() |&gt;\n    dplyr::arrange(start_station_name) |&gt;\n    dplyr::rename(\"station_name\" = start_station_name\n                 # \"lng\" = start_lng,\n                 # \"lat\" = start_lat\n                 )\n\nlocsEnd &lt;- dplyr::tbl(dbconn,\n                      tblPath_fltrd) |&gt;\n    dplyr::select(end_station_name\n                  #end_lng,\n                  #end_lat\n                  ) |&gt;\n    dplyr::distinct() |&gt;\n    dplyr::arrange(end_station_name) |&gt;\n    dplyr::rename(\"station_name\" = end_station_name\n                  #\"lng\" = end_lng,\n                  #\"lat\" = end_lat\n                  )\n\nlocs &lt;- locsStart |&gt;\n    dplyr::left_join(locsEnd) |&gt;\n    dplyr::arrange(station_name)\n\nlocs\n\n\n# Source:     SQL [?? x 1]\n# Database:   DuckDB v0.10.0 [ecmos@Windows 10 x64:R 4.3.3/db/data.db]\n# Ordered by: station_name\n   station_name              \n   &lt;chr&gt;                     \n 1 2112 W Peterson Ave       \n 2 410                       \n 3 63rd St Beach             \n 4 900 W Harrison St         \n 5 Aberdeen St & Jackson Blvd\n 6 Aberdeen St & Monroe St   \n 7 Aberdeen St & Randolph St \n 8 Ada St & 113th St         \n 9 Ada St & Washington Blvd  \n10 Adler Planetarium         \n# ‚Ñπ more rows\n\n\n\n\nCode\nflowData &lt;- dplyr::tbl(dbconn,\n                       tblPath_fltrd) |&gt;\n    dplyr::select(start_station_name,\n                  end_station_name) |&gt;\n    dplyr::group_by(start_station_name,\n                    end_station_name) |&gt;\n    dplyr::summarize(n = n()) |&gt;\n    dplyr::ungroup() |&gt;\n    dplyr::arrange(desc(n)) |&gt;\n    dplyr::rename(\"from_station\" = start_station_name,\n                  \"to_station\" = end_station_name) |&gt;\n    dplyr::collect() |&gt;\n    dplyr::slice_head(n = 50)\n\nflowData\n\n\n# A tibble: 50 √ó 3\n   from_station                      to_station                   n\n   &lt;chr&gt;                             &lt;chr&gt;                    &lt;dbl&gt;\n 1 Ellis Ave & 60th St               Ellis Ave & 55th St       6927\n 2 Ellis Ave & 60th St               University Ave & 57th St  6600\n 3 Ellis Ave & 55th St               Ellis Ave & 60th St       6349\n 4 University Ave & 57th St          Ellis Ave & 60th St       6168\n 5 Calumet Ave & 33rd St             State St & 33rd St        5417\n 6 State St & 33rd St                Calumet Ave & 33rd St     5343\n 7 DuSable Lake Shore Dr & Monroe St Streeter Dr & Grand Ave   4023\n 8 Loomis St & Lexington St          Morgan St & Polk St       3719\n 9 Morgan St & Polk St               Loomis St & Lexington St  3379\n10 University Ave & 57th St          Kimbark Ave & 53rd St     3112\n# ‚Ñπ 40 more rows\n\n\nCode\nsummary(flowData$n)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1363    1510    1878    2443    2599    6927 \n\n\n\n\nCode\nlocationData &lt;- dplyr::tbl(dbconn,\n                           tblPath_fltrd) |&gt;\n    dplyr::select(start_station_name,\n                  started_at,\n                  ended_at,\n                  trip_time) |&gt;\n    dplyr::group_by(start_station_name) |&gt;\n    dplyr::mutate(\"trip_time\" = round(trip_time,\n                                      digits = 0)) |&gt;\n    dplyr::summarize(\"trip_count\" = dplyr::n(),\n                     \"first_date\" = min(started_at),\n                     \"last_date\" = max(ended_at),\n                     \"avg_trip_time\" = mean(trip_time)\n                     ) |&gt;\n    dplyr::rename(\"station_name\" = start_station_name) |&gt;\n    dplyr::arrange(station_name) |&gt;\n    dplyr::collect()\n\nlocationData\n\n\n# A tibble: 1,487 √ó 5\n   station_name trip_count first_date          last_date           avg_trip_time\n   &lt;chr&gt;             &lt;dbl&gt; &lt;dttm&gt;              &lt;dttm&gt;                      &lt;dbl&gt;\n 1 2112 W Pete‚Ä¶        546 2023-01-01 14:38:58 2023-12-31 11:40:34         15.8 \n 2 410                   3 2023-04-27 08:02:30 2023-04-27 08:46:00         12.7 \n 3 63rd St Bea‚Ä¶        648 2023-01-06 12:59:47 2023-12-29 13:31:47         30.6 \n 4 900 W Harri‚Ä¶      10925 2023-01-01 02:18:29 2024-01-01 00:06:30          9.08\n 5 Aberdeen St‚Ä¶      12888 2023-01-01 00:56:42 2023-12-31 22:15:30          9.85\n 6 Aberdeen St‚Ä¶       8486 2023-01-01 08:08:19 2023-12-31 20:06:36         12.0 \n 7 Aberdeen St‚Ä¶       9270 2023-01-01 12:39:37 2023-12-31 14:40:04         11.6 \n 8 Ada St & 11‚Ä¶         19 2023-05-04 18:51:45 2023-11-29 17:03:52         13.8 \n 9 Ada St & Wa‚Ä¶       5874 2023-01-01 06:56:37 2023-12-31 16:10:44         10.8 \n10 Adler Plane‚Ä¶      12354 2023-01-01 07:42:32 2023-12-31 16:10:21         25.0 \n# ‚Ñπ 1,477 more rows\n\n\n\n\nCode\nmergd &lt;- merge(x = locationData,\n               y = locs,\n               by.x = \"station_name\",\n               by.y = \"station_name\",\n               sort = FALSE)\n\nmergd &lt;- mergd |&gt;\n    dplyr::mutate(avg_trip_time = round(avg_trip_time,\n                                    digits = 0)) |&gt;\n    dplyr::arrange(desc(trip_count)) |&gt;\n    dplyr::slice_head(n = 25)    \n\nmergd\n\n\n                         station_name trip_count          first_date\n1             Streeter Dr & Grand Ave      43175 2023-01-01 00:12:40\n2            Kingsbury St & Kinzie St      30915 2023-01-01 02:21:08\n3                   Clark St & Elm St      30597 2023-01-01 01:10:53\n4        Clinton St & Washington Blvd      28743 2023-01-01 15:28:03\n5  DuSable Lake Shore Dr & North Blvd      28386 2023-01-01 02:12:09\n6               Wells St & Concord Ln      28357 2023-01-01 01:15:27\n7               Michigan Ave & Oak St      26861 2023-01-01 00:59:17\n8                   Wells St & Elm St      25948 2023-01-01 00:59:22\n9   DuSable Lake Shore Dr & Monroe St      25055 2023-01-01 00:14:47\n10                Theater on the Lake      23893 2023-01-01 03:14:22\n11            Clinton St & Madison St      23143 2023-01-01 01:58:28\n12               Broadway & Barry Ave      22867 2023-01-01 01:48:00\n13           Wilton Ave & Belmont Ave      22612 2023-01-01 01:10:13\n14           University Ave & 57th St      22089 2023-01-01 16:42:37\n15                Ellis Ave & 60th St      21910 2023-01-01 12:01:37\n16            Clark St & Armitage Ave      21642 2023-01-01 00:23:47\n17                Wells St & Huron St      21416 2023-01-01 01:12:35\n18      Sheffield Ave & Fullerton Ave      21259 2023-01-01 00:28:38\n19             State St & Chicago Ave      21016 2023-01-01 01:03:50\n20             Clark St & Lincoln Ave      21011 2023-01-01 01:47:47\n21         Indiana Ave & Roosevelt Rd      20336 2023-01-01 02:13:09\n22                Canal St & Adams St      20302 2023-01-01 02:22:26\n23          Desplaines St & Kinzie St      20201 2023-01-01 04:25:20\n24              Dearborn St & Erie St      20182 2023-01-01 02:31:05\n25             Wabash Ave & Grand Ave      19992 2023-01-01 00:05:43\n             last_date avg_trip_time\n1  2024-01-01 00:19:01            24\n2  2023-12-31 21:30:50             9\n3  2023-12-31 23:29:33            11\n4  2023-12-31 18:03:02            11\n5  2023-12-31 23:34:53            19\n6  2023-12-31 19:23:30            12\n7  2023-12-31 16:48:31            23\n8  2023-12-31 23:51:48            10\n9  2023-12-31 16:49:56            25\n10 2023-12-31 22:53:53            21\n11 2023-12-31 23:56:48            11\n12 2023-12-31 21:18:45            12\n13 2023-12-31 21:21:06            11\n14 2023-12-31 05:54:17             8\n15 2023-12-31 09:22:22             6\n16 2023-12-31 17:54:10            14\n17 2023-12-31 23:51:50            10\n18 2023-12-31 23:43:00             9\n19 2023-12-31 19:48:26            11\n20 2023-12-31 21:09:23            14\n21 2023-12-31 23:12:35            16\n22 2023-12-31 07:16:45            12\n23 2023-12-31 21:04:13            10\n24 2023-12-31 18:34:38            11\n25 2023-12-31 23:24:43            13\n\n\n\n\nCode\n#|class-output: plotMod\n\nef_test &lt;- epiflows::make_epiflows(flows = flowData,\n                                   locations = mergd,\n                                   duration_stay = \"avg_trip_time\",\n                                   num_cases = \"trip_count\")\n\nepiflows::vis_epiflows(ef_test)\n\n\n\n\n\n\nThese maps track bike-sharing activity going on worldwide and in Chicago.\nBike Share Map (‚ÄúBike Share Map: Chicago (Divvy),‚Äù n.d.)\nCityBikes: bike sharing networks around the world (‚ÄúCityBikes: Bike Sharing Networks Around the World,‚Äù n.d.)\n\n\nCode\n#|label: 'drops duckDB tables'\nsource(\"duckDrops.R\")\n\n\n\n\nCode\n#|label: 'duckDB Shutdown'\nduckdb::dbDisconnect(dbconn, shutdown = TRUE)\nunlink(\"db\",\n       recursive = TRUE)"
  }
]