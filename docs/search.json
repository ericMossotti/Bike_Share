[
  {
    "objectID": "index.html#import-and-project-design",
    "href": "index.html#import-and-project-design",
    "title": "Case study: Bike-sharing program in the chicago area",
    "section": "Import and Project Design",
    "text": "Import and Project Design\nData source for this data analysis was obtained from Divvy Data. (“Divvy Data,” n.d.)\nThinking ahead with reproducibility in mind, should cover most use cases for tinkering and testing. I have found it helpful to reduce the need to re-download files and re-process all over again if all one needs to do is reconnect to the database that has already been written.\nAs a counterpart to the if-else design decision at the top of the project, I’ve condensed the initial download, import and cleaning steps inside of an R-script.\nChoosing a persistent DuckDB database filesystem (as opposed to in-memory) was intentional as I wouldn’t lose the progress I’ve made when tinkering over multiple days. It seems just as fast as the in-memory database but also seems to reduce RAM needed in tinkering. (“Why DuckDB,” n.d.)"
  },
  {
    "objectID": "index.html#hidden-duplicate-observations",
    "href": "index.html#hidden-duplicate-observations",
    "title": "Case study: Bike-sharing program in the chicago area",
    "section": "Hidden Duplicate Observations?",
    "text": "Hidden Duplicate Observations?\nNow to go a little deeper, we can check for duplicates. It might not necessarily be the case that each observation (obs) is unique even if all the Rider IDs are, technically, unique. Of the other columns, it seems that the start_time, end_time, start_station, and end_station, if combined, could show if there are possibly hidden duplicated observations. We started with 5,719,877 observations (obs) for dates spanning January to December, 2023, then removed 1,388,170 incomplete obs.\nI assumed that having the same times/dates and stations for two different ride IDs was a mistake. Although, I do not know how that error would happen, I could have assumed one person could check out multiple bikes at once. In that instance, each bike would be assigned a unique ride_id. That, however, has only happened 18 times over a year. Since it’s only one copy every time, that also raises a red flag in my mind. I did not notice any other correlations with station_id/name, member_casual, or ride_type for those particular duplicated data.\nBy applying distinct() on dupeTable, we see the only distinct value, ‘n’, is 2. I conclude that, of the duplicates, each has a minimum and maximum of 1 extra copy. Number of rows in the dupeTable is 36. Because each duplicated observation has one duplicate, “n = 2”, expected removed nobs is 18. The issue is that we need to get rid of not all 36 rows, but just one extra duplicate observation from each. This will result in the expected 18 obs.\nThe count of distinct n-values for the un-duplicated table was indeed 18. So now, it is time to run a count of how rows/observations are in the dataset. There is a difference, though, concerning the correct amount. The incorrect number of observations (nobs) was 4,331,707. The correct nobs after removing duplicated obs was 4,331,689. In short, 18 additional obs were removed.\nWe can now add the processed table to our database. Might be a good idea to verify the table is where it should be."
  },
  {
    "objectID": "index.html#outlier-filter",
    "href": "index.html#outlier-filter",
    "title": "Case study: Bike-sharing program in the chicago area",
    "section": "Outlier Filter",
    "text": "Outlier Filter\nTo ensure the conclusions are accurate, outliers should be filtered. Negative and very low trip times might skew trends. The underlying reason for very low trip times is somewhat of an unknown. Perhaps people often change their minds?\nAs in the first part, an if-else code-chunk design was chosen because it makes testing easier. It’s not required but is nice-to-have. Removing the nonsensical outliers, on the other hand, is required. This code chunk accomplishes both, regardless if you are just testing and already have the filtered database table or if you still need to create it. A database filtering script was used to make the code chunk easier to follow. We then verify the table does exist now along with all other tables we previously created.\nSo this should have removed outliers from the dataset which don’t really serve the scope of this analysis. Some could have also been erroneous data."
  }
]