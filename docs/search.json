[
  {
    "objectID": "index.html#stakeholders",
    "href": "index.html#stakeholders",
    "title": "Bike-Sharing in the Streets of Chicago",
    "section": "\n1.1 Stakeholders",
    "text": "1.1 Stakeholders\nStarting off, the stakeholders can be identified as Divvy, Lyft, and the City of Chicago Department of Transportation. As such this analysis is meant to provide stakeholders with data-based guidance to better server the residents of Chicago and users of the Divvy bike sharing service. Improving air quality, promoting economic recovery, and reducing traffic congestion were among the rationale for Divvy’s implementation. (“About Divvy: Company & History | Divvy Bikes,” n.d.)"
  },
  {
    "objectID": "index.html#source",
    "href": "index.html#source",
    "title": "Bike-Sharing in the Streets of Chicago",
    "section": "\n1.2 Source",
    "text": "1.2 Source\nThe raw 2023 dataset was directly sourced from Divvy Data. (“Divvy Data,” n.d.)"
  },
  {
    "objectID": "index.html#design",
    "href": "index.html#design",
    "title": "Bike-Sharing in the Streets of Chicago",
    "section": "\n1.3 Design",
    "text": "1.3 Design\nAnother worthy goal of this analysis is to achieve reproducibility and efficiency. To help future analyst teams move this research forward, this project tried to provide adequate code documentation and best practices with regards to clean code and modularity.\nFor example, it was helpful to incorporate certain design decisions to eliminate re-downloading and re-processing of data. For users conducting analysis over the course of days-months on this dataset, following initial download and processing, it is now possible to simply reconnect to the single database file which contains all of the original data including tables written throughout the course of the analysis.\nThe underlying code incorporates an if-else decision, which includes a source code script that handles the initial processing and establishes the database filesystem. Choosing a persistent DuckDB filesystem (as opposed to purely in-memory) seemed optimal in terms of the simplicity and cost-effectiveness of SQL database queries while retaining progress made over extended periods. (“Why DuckDB,” n.d.)"
  },
  {
    "objectID": "index.html#duplicates",
    "href": "index.html#duplicates",
    "title": "Bike-Sharing in the Streets of Chicago",
    "section": "\n2.1 Duplicates",
    "text": "2.1 Duplicates\n\n\n\nCode to Remove Duplicates\n\n\n\n\n\n\n\n\nFirst, create a table containing the duplicated observations.# This is a separate table used to analyze the observations \n#  returned as not distinct (n &gt; 1). \n#   This adds an extra column, labeled \"n\".\ndupeTable &lt;- dplyr::tbl(dbconn,\n                        tblPath,\n                        check_from = FALSE) |&gt;\n    dplyr::select(started_at:end_station_name) |&gt;\n    # Counts of unique rows added for column 'n'\n    dplyr::add_count(started_at,\n                     ended_at,\n                     start_station_name,\n                     end_station_name) |&gt;\n    # Only observations that have been duplicated \n    #  1 or more times are shown.\n    dplyr::filter(n &gt; 1) |&gt;\n    # We want to see all rows, \n    #  not just one row for each obs.\n    dplyr::ungroup() |&gt;\n    dplyr::arrange(started_at) |&gt;\n    dplyr::collect()\n\n\n\nRecord a count of distinct duplicates and total observations.distinctCopiesCount &lt;- dupeTable |&gt;\n    dplyr::distinct(n) |&gt;\n    as.integer() \n\nduplicateObs &lt;- length(dupeTable[[1]])\n\n\n\nCreate a table of the now unduplicated observations we saw before.# The issue is, we need to get rid of not all of these rows,\n#  but just the extra duplicate observations. \n\n# If there were 2 rows of duplicates, \n#  we would want to end up with 1 row after \n#   removing the extras.\nundupedTable &lt;- dupeTable |&gt;\n    dplyr::distinct(started_at,\n                     start_station_name,\n                     ended_at,\n                     end_station_name,\n                     #.keep_all = TRUE\n)\n\n\n\nRecord a count of the incorrect observations.# Run an incorrect count on how many rows or observations \n#  there are in the dataset.\ncount_incorrectDists &lt;- dplyr::tbl(dbconn,\n                                   tblPath,\n                                   check_from = FALSE) |&gt;\n    dplyr::distinct(dplyr::pick(\"ride_id\")) |&gt;\n    dplyr::count(name = \"Incorrect Distinct Observations\") |&gt;\n    dplyr::collect() |&gt;\n    as.integer()\n\n\n\nRecord a count of the correct observations.# For the correct count of obs\ncount_correctDists &lt;- dplyr::tbl(dbconn,\n                                 tblPath,\n                                 check_from = FALSE) |&gt;\n    dplyr::distinct(\n        dplyr::pick(\n            \"started_at\",\n            \"start_station_name\",\n            \"ended_at\",\n            \"end_station_name\")) |&gt;\n    dplyr::count() |&gt;\n    dplyr::collect() |&gt;\n    as.integer()\n\n\n\nLastly, write the unduplicated data to the database.dupelessPath &lt;- \"db/dupeless.db\"\n \ndplyr::tbl(dbconn,\n           tblPath,\n           check_from = FALSE) |&gt;\n    dplyr::select(ride_id:trip_time) |&gt;\n    dplyr::distinct(started_at,\n                    start_station_name,\n                    ended_at,\n                    end_station_name,\n                    .keep_all = TRUE) |&gt;\n    dplyr::arrange(started_at) |&gt;\n    dplyr::collect() |&gt;\n    duckdb::dbWriteTable(conn = dbconn,\n                         name = dupelessPath,\n                         overwrite = TRUE,\n                         check_from = FALSE)\n\n\n\nCodegtDupes &lt;- dupeTable |&gt;\ndplyr::group_by(started_at) |&gt;\ngt::gt(\nrowname_col = \"row\",\ngroupname_col = \"started_at\",\nrow_group_as_column = TRUE\n) |&gt;\ngt::tab_style(\nstyle = list(\ngt::cell_text(weight = \"bold\", align = \"center\"),\ngt::cell_borders(sides = c(\"bottom\"))\n),\nlocations = gt::cells_column_labels(gt::everything())\n) |&gt;\ngt::tab_style(\nstyle = list(\ngt::cell_borders(sides = c(\"left\", \"right\"), color = \"transparent\"),\ngt::cell_text(align = \"center\", v_align = \"middle\")\n),\nlocations = gt::cells_body(gt::everything())\n) |&gt;\ngt::data_color(\ncolumns = start_station_name,\ntarget_columns = gt::everything(),\nmethod = \"auto\",\npalette = \"basetheme::brutal\"\n) |&gt;\ngt::tab_header(title = \"A view of duplicated observations\", subtitle = \"Grouping follows 'started_at'\") |&gt;\ngt::tab_options(\nheading.title.font.weight = \"bolder\",\nheading.subtitle.font.weight = \"lighter\",\nheading.align = \"center\",\ntable.background.color = \"transparent\",\ntable.font.color = \"SeaShell\",\ntable.font.size = gt::pct(75),\n)\n\n\n\nCodegt_undupes &lt;- undupedTable |&gt;\ndplyr::collect() |&gt;\ndplyr::group_by(started_at) |&gt;\ngt::gt(\nrowname_col = \"row\",\ngroupname_col = \"started_at\",\nrow_group_as_column = TRUE\n) |&gt;\ngt::fmt_number(decimals = 0) |&gt;\n\ngt::tab_style(\nstyle = list(\ngt::cell_text(weight = \"bold\", align = \"center\"),\ngt::cell_borders(sides = c(\"bottom\"))\n),\nlocations = gt::cells_column_labels(gt::everything())\n) |&gt;\ngt::tab_style(\nstyle = list(\ngt::cell_borders(sides = c(\"left\", \"right\")),\ngt::cell_text(align = \"center\", v_align = \"middle\")\n),\nlocations = gt::cells_body(gt::everything())\n) |&gt;\ngt::data_color(\ncolumns = start_station_name,\ntarget_columns = gt::everything(),\nmethod = \"auto\",\npalette = \"basetheme::brutal\"\n) |&gt;\n#gt::tab_source_note(gt::md(\"**Source**: Divvy Data\")) |&gt;\ngt::tab_header(title = \"After duplicates were removed\", subtitle = \"Same grouping)\") |&gt;\ngt::tab_options(\nheading.title.font.weight = \"bolder\",\nheading.subtitle.font.weight = \"lighter\",\nheading.align = \"center\",\ntable.background.color = \"transparent\",\ntable.font.color = \"SeaShell\",\ntable.font.size = gt::pct(75)\n)\n\n\n\nCode# To see the history of obs in our dataset.\nsummaryProcessTable &lt;- tidyr::tribble(\n    ~ \"Observations\",\n    ~ \"Counts\",\n    \"Original   \",\n    original_nobs,\n    \"Processed   \",\n    count_incorrectDists,\n    \"Duplicates   \",\n    (count_incorrectDists - count_correctDists),\n    \"Total Corrected   \",\n    count_correctDists ) |&gt;\n    gt::gt(rownames_to_stub = FALSE) |&gt;\n    gt::tab_header(title = \"Tallying Observations\") |&gt;\n    gt::tab_footnote(\n        footnote = gt::md(\"Row counts throughout the cleaning steps.\"),\n        locations = gt::cells_column_labels(columns = Counts)\n    ) |&gt;\n    gt::tab_style(\n        style = list(\n            gt::cell_borders(sides = \"bottom\"),\n            gt::cell_text(\n                align = \"left\",\n                stretch = \"semi-expanded\",\n                whitespace = \"break-spaces\"\n            )\n        ),\n        locations = gt::cells_body(gt::everything())\n    ) |&gt;\n    gt::tab_style(\n        gt::cell_text(\n            align = \"center\",\n            stretch = \"semi-expanded\",\n            whitespace = \"break-spaces\"),\n        locations = list(\n            gt::cells_title(groups = c(\"title\", \"subtitle\")),\n            gt::cells_column_labels(gt::everything())\n            )\n        ) |&gt;\n    gt::tab_options(quarto.use_bootstrap = TRUE,\n                    column_labels.font.weight = \"bold\",\n                    table.background.color = \"transparent\",\n                    table.font.color = \"SeaShell\",\n                    row.striping.background_color = \"gray10\",\n                    row.striping.include_table_body = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\nNow the question arises: How does one check the data for duplicates? That is what this section covers, but also how to only remove what is needed. Care is needed because it might not necessarily be the case that each observation or obs is unique because one of the columns contains all unique values.\nAll of the values in the ride_id column were found to be unique. However, not all of the observations, or rows, were found to be truly unique. Other columns such as start_time, end_time, start_station, and end_station were used to verify if an observation was truly unique. These would have more granular information, down to the second, a given trip’s starting and ending time and location. In this analysis, it was assumed that observations having the same starting and ending date-time and station despite having two different rider IDs was a mistake.\n\n\n\n\n\nTable 2: Duplicates Table\n\n\n\n\n\n\n\nA view of duplicated observations\n\n\nGrouping follows 'started_at'\n\n\n\nended_at\nstart_station_name\nstart_station_id\nend_station_name\nn\n\n\n\n\n2023-02-19 12:10:52\n2023-02-19 12:24:04\nOrleans St & Merchandise Mart Plaza\nTA1305000022\nGreen St & Randolph St*\n2\n\n\n2023-02-19 12:24:04\nOrleans St & Merchandise Mart Plaza\nTA1305000022\nGreen St & Randolph St*\n2\n\n\n2023-04-15 15:56:18\n2023-04-15 16:01:54\nKedzie Ave & 45th St\n342\nFairfield Ave & 44th St\n2\n\n\n2023-04-15 16:01:54\nKedzie Ave & 45th St\n342\nFairfield Ave & 44th St\n2\n\n\n2023-04-21 09:45:26\n2023-04-21 10:01:23\nMichigan Ave & 8th St\n623\nWentworth Ave & Cermak Rd*\n2\n\n\n2023-04-21 10:01:23\nMichigan Ave & 8th St\n623\nWentworth Ave & Cermak Rd*\n2\n\n\n2023-07-08 18:22:07\n2023-07-08 18:37:31\nMilwaukee Ave & Grand Ave\n13033\nBissell St & Armitage Ave*\n2\n\n\n2023-07-08 18:37:31\nMilwaukee Ave & Grand Ave\n13033\nBissell St & Armitage Ave*\n2\n\n\n2023-07-08 18:58:14\n2023-07-08 19:08:47\nClark St & Schiller St\nTA1309000024\nBissell St & Armitage Ave*\n2\n\n\n2023-07-08 19:08:47\nClark St & Schiller St\nTA1309000024\nBissell St & Armitage Ave*\n2\n\n\n2023-07-09 18:00:17\n2023-07-09 18:40:49\nWells St & Hubbard St\nTA1307000151\nFort Dearborn Dr & 31st St*\n2\n\n\n2023-07-09 18:40:49\nWells St & Hubbard St\nTA1307000151\nFort Dearborn Dr & 31st St*\n2\n\n\n2023-07-10 20:10:41\n2023-07-10 20:19:59\nClark St & Newport St\n632\nLincoln Ave & Roscoe St*\n2\n\n\n2023-07-10 20:19:59\nClark St & Newport St\n632\nLincoln Ave & Roscoe St*\n2\n\n\n2023-07-15 10:48:09\n2023-07-15 10:58:22\nRacine Ave & Wrightwood Ave\nTA1309000059\nBissell St & Armitage Ave*\n2\n\n\n2023-07-15 10:58:22\nRacine Ave & Wrightwood Ave\nTA1309000059\nBissell St & Armitage Ave*\n2\n\n\n2023-07-15 19:38:51\n2023-07-15 19:55:04\nAvondale Ave & Irving Park Rd\n15624\nPublic Rack - Hamlin Ave & Fullerton Ave\n2\n\n\n2023-07-15 19:55:04\nAvondale Ave & Irving Park Rd\n15624\nPublic Rack - Hamlin Ave & Fullerton Ave\n2\n\n\n2023-07-23 11:41:36\n2023-07-23 12:07:13\nBurnham Harbor\n15545\nFort Dearborn Dr & 31st St*\n2\n\n\n2023-07-23 12:07:13\nBurnham Harbor\n15545\nFort Dearborn Dr & 31st St*\n2\n\n\n2023-07-25 18:08:47\n2023-07-25 18:21:45\nWabash Ave & Roosevelt Rd\nTA1305000002\nWentworth Ave & Cermak Rd*\n2\n\n\n2023-07-25 18:21:45\nWabash Ave & Roosevelt Rd\nTA1305000002\nWentworth Ave & Cermak Rd*\n2\n\n\n2023-07-26 21:10:55\n2023-07-26 21:28:44\nBurnham Harbor\n15545\nFort Dearborn Dr & 31st St*\n2\n\n\n2023-07-26 21:28:44\nBurnham Harbor\n15545\nFort Dearborn Dr & 31st St*\n2\n\n\n2023-08-05 19:40:37\n2023-08-05 20:08:35\nMorgan St & Lake St*\nchargingstx4\nMorgan St & Lake St*\n2\n\n\n2023-08-05 20:08:35\nMorgan St & Lake St*\nchargingstx4\nMorgan St & Lake St*\n2\n\n\n2023-08-12 17:46:53\n2023-08-12 17:57:40\nCalumet Ave & 18th St\n13102\nWentworth Ave & Cermak Rd*\n2\n\n\n2023-08-12 17:57:40\nCalumet Ave & 18th St\n13102\nWentworth Ave & Cermak Rd*\n2\n\n\n2023-08-17 12:23:50\n2023-08-17 12:37:45\nDuSable Lake Shore Dr & Monroe St\n13300\nStreeter Dr & Grand Ave\n2\n\n\n2023-08-17 12:37:45\nDuSable Lake Shore Dr & Monroe St\n13300\nStreeter Dr & Grand Ave\n2\n\n\n2023-09-03 14:55:59\n2023-09-03 15:58:21\nBissell St & Armitage Ave*\nchargingstx1\nBissell St & Armitage Ave*\n2\n\n\n2023-09-03 15:58:21\nBissell St & Armitage Ave*\nchargingstx1\nBissell St & Armitage Ave*\n2\n\n\n2023-09-25 17:38:05\n2023-09-25 17:52:29\nFairbanks Ct & Grand Ave\nTA1305000003\nDuSable Lake Shore Dr & North Blvd\n2\n\n\n2023-09-25 17:52:29\nFairbanks Ct & Grand Ave\nTA1305000003\nDuSable Lake Shore Dr & North Blvd\n2\n\n\n2023-10-10 13:22:51\n2023-10-10 13:29:37\nLoomis St & Lexington St\n13332\nMorgan St & Polk St\n2\n\n\n2023-10-10 13:29:37\nLoomis St & Lexington St\n13332\nMorgan St & Polk St\n2\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlthough, it is actually unknown how that error would happen. It could have been assumed that one person checked out multiple bikes at once. In that instance, each bike would be assigned a unique ride_id. That, however, has only happened 18 times over a year. Since it is only one copy every time, red flags were raised. Perhaps trips could be grouped where one person pays for a other riders? If that were the case, then why is there always one duplicate?\nIn Table 2, duplicate observations were listed and grouped by color. Contrast this with Table 3, which has removed the extra copy of the observation while preserving the observation itself. Of the duplicates, each has one extra copy. It is noted that the number of rows in the duplicates table is 36. Each duplicated observation has one duplicate, where n is always the value 2. So the expected removed obs is 18. A complication involved how to remove not all obs but just one extra duplicate observation from each.\n\n\n\n\n\nTable 3: Un-duplicated Table\n\n\n\n\n\n\n\nAfter duplicates were removed\n\n\nSame grouping)\n\n\n\nstart_station_name\nended_at\nend_station_name\n\n\n\n\n2023-02-19 12:10:52\nOrleans St & Merchandise Mart Plaza\n2023-02-19 12:24:04\nGreen St & Randolph St*\n\n\n2023-04-15 15:56:18\nKedzie Ave & 45th St\n2023-04-15 16:01:54\nFairfield Ave & 44th St\n\n\n2023-04-21 09:45:26\nMichigan Ave & 8th St\n2023-04-21 10:01:23\nWentworth Ave & Cermak Rd*\n\n\n2023-07-08 18:22:07\nMilwaukee Ave & Grand Ave\n2023-07-08 18:37:31\nBissell St & Armitage Ave*\n\n\n2023-07-08 18:58:14\nClark St & Schiller St\n2023-07-08 19:08:47\nBissell St & Armitage Ave*\n\n\n2023-07-09 18:00:17\nWells St & Hubbard St\n2023-07-09 18:40:49\nFort Dearborn Dr & 31st St*\n\n\n2023-07-10 20:10:41\nClark St & Newport St\n2023-07-10 20:19:59\nLincoln Ave & Roscoe St*\n\n\n2023-07-15 10:48:09\nRacine Ave & Wrightwood Ave\n2023-07-15 10:58:22\nBissell St & Armitage Ave*\n\n\n2023-07-15 19:38:51\nAvondale Ave & Irving Park Rd\n2023-07-15 19:55:04\nPublic Rack - Hamlin Ave & Fullerton Ave\n\n\n2023-07-23 11:41:36\nBurnham Harbor\n2023-07-23 12:07:13\nFort Dearborn Dr & 31st St*\n\n\n2023-07-25 18:08:47\nWabash Ave & Roosevelt Rd\n2023-07-25 18:21:45\nWentworth Ave & Cermak Rd*\n\n\n2023-07-26 21:10:55\nBurnham Harbor\n2023-07-26 21:28:44\nFort Dearborn Dr & 31st St*\n\n\n2023-08-05 19:40:37\nMorgan St & Lake St*\n2023-08-05 20:08:35\nMorgan St & Lake St*\n\n\n2023-08-12 17:46:53\nCalumet Ave & 18th St\n2023-08-12 17:57:40\nWentworth Ave & Cermak Rd*\n\n\n2023-08-17 12:23:50\nDuSable Lake Shore Dr & Monroe St\n2023-08-17 12:37:45\nStreeter Dr & Grand Ave\n\n\n2023-09-03 14:55:59\nBissell St & Armitage Ave*\n2023-09-03 15:58:21\nBissell St & Armitage Ave*\n\n\n2023-09-25 17:38:05\nFairbanks Ct & Grand Ave\n2023-09-25 17:52:29\nDuSable Lake Shore Dr & North Blvd\n\n\n2023-10-10 13:22:51\nLoomis St & Lexington St\n2023-10-10 13:29:37\nMorgan St & Polk St\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe count of distinct n-values for the un-duplicated table was 18. Then a count of observations in the dataset were recorded. The incorrect number of obs was 4,331,707. The correct obs after removing duplicated obs was 4,331,689. In short, 18 additional obs were removed. Table 4\n\n\n\n\nTable 4: Observation Processing History\n\n\n\n\n\n\n\nTallying Observations\n\n\nObservations\nCounts1\n\n\n\n\n\nOriginal\n5719877\n\n\nProcessed\n4331707\n\n\nDuplicates\n18\n\n\nTotal Corrected\n4331689\n\n\n\n\n1 Row counts throughout the cleaning steps."
  },
  {
    "objectID": "index.html#outliers",
    "href": "index.html#outliers",
    "title": "Bike-Sharing in the Streets of Chicago",
    "section": "\n2.2 Outliers",
    "text": "2.2 Outliers\nObservations that were deemed erroneous or not useful for identifying trends in how the service is utilized by members and casual users were filtered out. It is good practice to keep note of these errors as they might explain some differences in how members and casual use the service.\nIt was determined that trips with negative minutes were errors. Trips less than 1 minute but positive were noted, but removed as they might skew the statistics we derive from this data too much later on. The underlying reason for low trip times could be from people just wanting to try it out briefly before committing or quickly realized they did not like it. There were observations that did not make much sense, but most of the data was retained.\nAs in the first part, an if-else decision was chosen because it makes testing easier. An external database filtering script was chosen to shorten the code needed inside the main Quarto document itself. The data from which the rest of the tables are based on are used for the rest of the analysis. script\n\n\n\n\n\n\n\nFilter the database\n\n\n\n\n\nThis would execute if the if-else conditions were met to filter the db/data.db database tablefilterDatabase &lt;- function(conxn = dbconn,\n                           path1 = dupelessPath,\n                           path2 = tblPath_fltrd) {\n    dplyr::tbl(conxn, path1, check_from = FALSE) |&gt;\n        dplyr::filter(trip_time &gt; 1, trip_time &lt; 480, rideable_type != \"docked_bike\") |&gt;\n        dplyr::collect() |&gt;\n        # Might as well calculate distance traveled while at it.\n        dplyr::mutate(\n            miles = geosphere::distGeo(\n                p1 = cbind(start_lng, start_lat),\n                p2 = cbind(end_lng, end_lat)\n            ) / 1000 * 0.62137119,\n            mph = (miles / (trip_time / 60))\n        ) |&gt;\n        # It's nonsensical to rent a bike for distances easily walked.\n        dplyr::filter(miles &gt; 0.1, # Seems that pro cyclists average around 20 mph,\n                      # so I set that as the ceiling.\n                      mph &lt;= 20, # To account for time spent idling, stoplights and\n                      # traffic.\n                      mph &gt; 1) |&gt;\n        duckdb::dbWriteTable(\n            conn = conxn,\n            name = path2,\n            overwrite = TRUE,\n            check_from = FALSE\n        )\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFilter Database\n\n\n\n\n\n\n\n\nFirst, if you happen to be re-using this code - this is so you do not have to re-download or re-filter after making further adjustments.tblPath &lt;- \"db/data.db\"\ndupelessPath &lt;- \"db/dupeless.db\"\ntblPath_fltrd &lt;- \"db/data_fltrd.db\"\n\nif (exists(\"dbconn\") == FALSE && dir.exists(\"db\") == TRUE) {\n    dbconn &lt;- DBI::dbConnect(\n        duckdb::duckdb(),\n        dbdir = tblPath,\n        read_only = FALSE,\n        check_from = FALSE\n    )\n}\n\nif (duckdb::dbExistsTable(dbconn,\n                          \"tblPath_fltrd\") == FALSE) {\n    source(\"filterDatabase.R\")\n    filterDatabase()\n}"
  },
  {
    "objectID": "index.html#generating-frequency-tables",
    "href": "index.html#generating-frequency-tables",
    "title": "Bike-Sharing in the Streets of Chicago",
    "section": "\n3.1 Generating Frequency Tables",
    "text": "3.1 Generating Frequency Tables\nThe rationale for frequency tables is to gain quick insights into the data. It is taking a methodical approach that, while simple, provides a crucial pathway towards understanding the bigger picture."
  },
  {
    "objectID": "index.html#sec-epiflow",
    "href": "index.html#sec-epiflow",
    "title": "Bike-Sharing in the Streets of Chicago",
    "section": "\n3.2 Traffic Flow",
    "text": "3.2 Traffic Flow\n\nFor an interesting birds-eye-view of trip behaviors, see Figure 16. This is called an epiflow diagram. It is interactive, so circles (nodes) and the lines connecting the nodes can be clicked for more information. There is also a drop-down window to further explore the data. The top 34 most traveled stations were used for this visual network diagram. The line thickness roughly corresponds with amount of trips taken, so thicker lines means more trips between nodes. (Moraga et al., n.d.)\nThese are the most active stations. Luckily, there might be a way to further explore the typical high traffic station locations and why they are high traffic. Section 3.3\n\n\n\n\nCreating an Epiflow\n\n\n\n\n\n\n\n\nFirst, creates the frequency of trips taken to and from pairs of stations. We are only going to look deeper into the top 50 most traveled pairs.flowData &lt;- dplyr::tbl(dbconn,\n                       tblPath_fltrd) |&gt;\n    dplyr::select(start_station_name,\n                  end_station_name) |&gt;\n    dplyr::group_by(start_station_name,\n                    end_station_name) |&gt;\n    dplyr::summarize(n = n()) |&gt;\n    dplyr::ungroup() |&gt;\n    dplyr::arrange(desc(n)) |&gt;\n    dplyr::rename(\"from_station\" = start_station_name,\n                  \"to_station\" = end_station_name) |&gt;\n    dplyr::collect() |&gt;\n    dplyr::slice_head(n = 50)\n\n\n\nSecond, we need statistics but also to combine the statistics for every unique station name.locationData &lt;- dplyr::tbl(dbconn,\n                           tblPath_fltrd) |&gt;\n    dplyr::select(start_station_name,\n                  end_station_name,\n                  started_at,\n                  ended_at,\n                  trip_time) |&gt;\n    dplyr::group_by(start_station_name,\n                    end_station_name\n                ) |&gt;\n    dplyr::mutate(\"trip_time\" = round(trip_time,\n                                      digits = 0)) |&gt;\n    dplyr::summarize(\n        \"trip_count\" = dplyr::n(),\n        \"first_date\" = min(started_at),\n        \"last_date\" = max(ended_at),\n        #\"avg_trip_time\" = mean(trip_time)\n    ) |&gt;\n    dplyr::ungroup() |&gt;\n    dplyr::rename(\"from_station\" = start_station_name,\n                  \"to_station\" = end_station_name\n               ) |&gt;\n    dplyr::arrange(desc(trip_count)) |&gt;\n    dplyr::collect()\n\n\n\n# Need to combine all names to single column and recalculate \n# or retain other stats.\nlocationData_pivoted &lt;- locationData |&gt;\n    tidyr::pivot_longer(cols = 1:2, \n                        values_to = \"allNames\") |&gt;\n    dplyr::group_by(allNames) |&gt;\n    dplyr::summarize(\"trips_toAndfrom\" = sum(trip_count),\n                     first_date = min(first_date),\n                     last_date = max(last_date),\n                    # avg_trip_time = mean(avg_trip_time)\n                     ) |&gt;\n    #dplyr::mutate(avg_trip_time = round(avg_trip_time,\n    #                                  digits = 0)) |&gt;\n    dplyr::arrange(trips_toAndfrom)\n\n\n\nThird, creates epiflow objects, which take in a pair of dataframes and creates the flows between them.# for all the pairs\nef_test &lt;- epiflows::make_epiflows(flows = flowData,\n                                   locations = locationData_pivoted,\n                                   #duration_stay = \"avg_trip_time\",\n                                   num_cases = \"trips_toAndfrom\")\n\n\n\n\n\n\n\n\nTables\n\n\n\n\n\n\n\n\nFirst, just a quick view of the flow data table we made earlier.flowData\n\n# A tibble: 50 × 3\n   from_station                      to_station                   n\n   &lt;chr&gt;                             &lt;chr&gt;                    &lt;dbl&gt;\n 1 Ellis Ave & 60th St               Ellis Ave & 55th St       6927\n 2 Ellis Ave & 60th St               University Ave & 57th St  6600\n 3 Ellis Ave & 55th St               Ellis Ave & 60th St       6349\n 4 University Ave & 57th St          Ellis Ave & 60th St       6168\n 5 Calumet Ave & 33rd St             State St & 33rd St        5417\n 6 State St & 33rd St                Calumet Ave & 33rd St     5343\n 7 DuSable Lake Shore Dr & Monroe St Streeter Dr & Grand Ave   4023\n 8 Loomis St & Lexington St          Morgan St & Polk St       3719\n 9 Morgan St & Polk St               Loomis St & Lexington St  3379\n10 University Ave & 57th St          Kimbark Ave & 53rd St     3112\n# ℹ 40 more rows\n\n\n\nSecond, another quick view, but for thethe location data we pivoted earlier.locationData_pivoted |&gt;\n    dplyr::arrange(desc(trips_toAndfrom))\n\n# A tibble: 1,567 × 4\n   allNames              trips_toAndfrom first_date          last_date          \n   &lt;chr&gt;                           &lt;dbl&gt; &lt;dttm&gt;              &lt;dttm&gt;             \n 1 Streeter Dr & Grand …           86422 2023-01-01 00:05:43 2024-01-01 00:19:01\n 2 Kingsbury St & Kinzi…           61277 2023-01-01 01:21:59 2023-12-31 21:30:50\n 3 DuSable Lake Shore D…           60808 2023-01-01 02:12:09 2023-12-31 23:34:53\n 4 Clark St & Elm St               60552 2023-01-01 01:06:48 2023-12-31 23:29:33\n 5 Clinton St & Washing…           58278 2023-01-01 00:44:39 2023-12-31 18:03:02\n 6 Wells St & Concord Ln           57642 2023-01-01 01:15:27 2023-12-31 23:51:50\n 7 Michigan Ave & Oak St           54000 2023-01-01 00:59:17 2023-12-31 23:09:35\n 8 Wells St & Elm St               52315 2023-01-01 00:59:22 2023-12-31 23:51:48\n 9 DuSable Lake Shore D…           48833 2023-01-01 00:14:47 2023-12-31 16:49:56\n10 Theater on the Lake             48349 2023-01-01 03:14:22 2023-12-31 22:53:53\n# ℹ 1,557 more rows\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 16: EpiFlow Network"
  },
  {
    "objectID": "index.html#sec-mapview",
    "href": "index.html#sec-mapview",
    "title": "Bike-Sharing in the Streets of Chicago",
    "section": "\n3.3 Checking the Map",
    "text": "3.3 Checking the Map\n\nThis section was made possible thanks to the latitude and longitude coordinates data provided alongside the stations names. Coming from the epiflow diagram, this should help make the data less abstract. The accordion below expands and collapses four OpenStreet maps found in the callout section below. These maps were split for viewing logistics. They contain from the epiflow in the section above. These maps are interactive, so the default views are zoomable and movable. The transparent burst buttons enable snappy zooming-in of the station groups.\n\n\n\n\nCode for Mapping\n\n\n\n\n\n\n\n\nProcessing ‘flowData’ created earlier to include geolocation data for mapview plots.# All distinct stations in one column\nnames &lt;- flowData |&gt;\n    dplyr::select(from_station,\n                  to_station) |&gt;\n    tidyr::pivot_longer(cols = 1:2,\n                        names_to = NULL,\n                        values_to = \"station_names\") |&gt;\n    dplyr::distinct()\n\n\n# The important geo-coordinates corresponding to station names\nmapData &lt;- dplyr::tbl(dbconn,\n                      tblPath_fltrd,\n                      check_from = FALSE) |&gt;\n    dplyr::select(start_station_name,\n                  start_lat,\n                  start_lng,\n                  end_station_name,\n                  end_lat,\n                  end_lng)\n\n# Filter to include all observations that match the station names listed in 'names'. We need the geo-coordinates alongside the names.\nmapData1 &lt;- mapData |&gt;\n    dplyr::collect() |&gt;\n# Filter, but through a vector of conditions.\n    dplyr::filter(start_station_name %in% names[[1]],\n                  end_station_name %in% names[[1]]) |&gt;\n    dplyr::select(start_station_name:start_lng)\n\n\n# Had to split 'mapData' into two and pivot into a single table.\nmapData2 &lt;- mapData |&gt;\n    dplyr::collect() |&gt;\n    dplyr::filter(start_station_name %in% names[[1]],\n                  end_station_name %in% names[[1]]) |&gt;\n    dplyr::select(end_station_name:end_lng)\n\n# Nice grouping\nstations_groupMap &lt;- dplyr::bind_rows(mapData1, mapData2) |&gt;\ndplyr::select(start_station_name, start_lat, start_lng) |&gt;\ndplyr::rename(\"station_names\" = start_station_name,\n\"lat\" = start_lat,\n\"lng\" = start_lng) |&gt;\ndplyr::distinct() |&gt;\ndplyr::group_by(station_names)\n\n# Setting seed for sampling\nset.seed(113)\n\n# Taking 10 random samples from each station_name group\nsampled_stations &lt;- stations_groupMap |&gt;\n    dplyr::slice_sample(n = 10) |&gt;\n    tidyr::drop_na()\n\n\n\nCreates a map coloring palette excluding grays.# All of the r-colors\nallPalette &lt;- colors()\n\n# The grays are vast so we don't want that.\ncolorfulPal &lt;- purrr::discard(allPalette, stringr::str_detect(allPalette, \"gr(a|e)y\"))\n\n# When we sample the colors, 10 should be slightly more than needed.\nn_colors &lt;- 10\n\n\n\nFirst, sourcing the script needed to generate the maps and creating the list of vectors used as input. These vectors are the slices of the top most traveled stations.slicerVector &lt;- list(c(1:9), c(10:18), c(19:27), c(28:34))\nsource(\"mapViewer.R\")\n\n\n\nThe script used to generate the maps.# I needed the stations groups' burst buttons to fit\n# the viewing window in my document and the only way I could think of is to \n#   split the stations into multiple maps. This reduces duplicate code.\n\n\n\nmapViewer &lt;- function(x) {\n    \n    nameSlice &lt;- sampled_stations |&gt;\n        dplyr::ungroup() |&gt;\n        dplyr::distinct(station_names) |&gt;\n        dplyr::slice(x)\n    \n    viewMap &lt;- sampled_stations |&gt;\n        dplyr::filter(station_names %in% nameSlice$station_names) |&gt;\n        sf::st_as_sf(coords = c(3:2), crs = 4326) |&gt;\n        mapview::mapview(\n            zcol = \"station_names\",\n            col.regions = randomColors,\n            map.types = \"OpenStreetMap\",\n            burst = TRUE,\n            legend = FALSE)\n    \n    return(viewMap)\n}\n\n\n\n\n\n\n\n\n\nBenson Ave & Church St … Ellis Ave & 60th St\n\n\n\n\n\nCodeset.seed(240)\nrandomColors &lt;- sample(colorfulPal, n_colors)\nmapViewer(slicerVector[[1]])\n\n\n\n\n\n\nFigure 17: Benson Ave & Church St - Ellis Ave & 60th St\n\n\n\n\n\n\n\n\n\nGreenview Ave & Fullteron Ave … Loomis Ave & Lexington St\n\n\n\n\n\n\n\n\n\n\n\nFigure 18: Greenview Ave & Fullteron Ave - Loomis Ave & Lexington St\n\n\n\n\n\n\n\n\n\nMichigan Ave & Oak St … State St & 33rd St\n\n\n\n\n\n\n\n\n\n\n\nFigure 19: Michigan Ave & Oak St - State St & 33rd St\n\n\n\n\n\n\n\n\n\nStreet Dr & Grand Ave … Woodlawn Ave & 55th St\n\n\n\n\n\nCodeset.seed(243)\nrandomColors &lt;- sample(colorfulPal, n_colors)\nmapViewer(slicerVector[[4]])\n\n\n\n\n\n\nFigure 20: Street Dr & Grand Ave - Woodlawn Ave & 55th St\n\n\n\n\n\n\n\n\nFor example, suppose University Ave & 57th St was selected in the epiflow. As it happens this is at the heart of the University of Chicago. So where does the traffic to and from this location usually flow to? Selecting one of the other nodes highlighted with flows directing away from the previous node, is Kimbark Ave and 53rd St. This location is seated adjacent to the Vue 53 Apartments complex in the map view."
  }
]