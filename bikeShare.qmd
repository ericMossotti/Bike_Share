---
title: "BikeShare"
author: "Eric Mossotti"

bibliography: references.bib
repo: https://github.com/ericMossotti/Bike_Share
page-layout: article
source: bikeShare.qmd

code-links:
    - text: "Project Repo"
      href: repo
      
code-fold: true
code-copy: hover
code-overflow: wrap
code-tools: true

toc: true
toc_float: false
smooth-scroll: true

echo: true

sass: styles.sass

#font: merriweather, futura
---

# Case Study: Bike-Sharing, Rethought

#### References:

-   [Bike-share research (bikeshare-research.org)](https://bikeshare-research.org/#bssid:chicago)

-   [Divvy - Wikipedia](https://en.wikipedia.org/wiki/Divvy)

-   [Cycling in Chicago - Wikipedia](https://en.wikipedia.org/wiki/Cycling_in_Chicago)

-   [Home \| Divvy Bikes](https://divvybikes.com/)

-   [Divvy Membership & Pass Options \| Divvy Bikes](https://divvybikes.com/pricing)

-   [@datalic]

    -   [Data License Agreement \| Divvy Bikes](https://divvybikes.com/data-license-agreement)

-   [@motivate]

    -   [MOTIVATE (motivateco.com)](https://motivateco.com/)

-   [@indexof]

    -   [Index of bucket "divvy-tripdata"](https://divvy-tripdata.s3.amazonaws.com/index.html)

-   [@whyduck]

    -   [Why DuckDB?](https://duckdb.org/why_duckdb)

-   [@rforda]

    -   [R for Data Science: Chapter 22: Arrow](https://r4ds.hadley.nz/arrow "Arrow")

-   [@great-ci]

    -   <https://en.wikipedia.org/wiki/Great-circle_distance>

-   [@average2024]

    -   <https://bikingultimate.com/average-bicycle-speed-how-fast-do-cyclists-ride-and-what-affects-their-pace/>

```{r, include = FALSE}

knitr::opts_chunk$set(message = FALSE, 
                      warning = FALSE)
```

## Objective:

Communicate to shareholders data based insights regarding to better inform their desire to enlist more annual subscribers.

# First Off: Import, then Process

```{r}

if(exists("dbconn") == FALSE) {
    # Script to keep this document less cluttered. 
    source("import_clean_initial.R")
    } else {
        tblPath <- "db/data.db"
        
        dbconn <- DBI::dbConnect(
        duckdb::duckdb(),
        dbdir = tblPath,
        read_only = FALSE,
        check_from = FALSE)
    }
```

Verifying DuckDB tables:

```{r}
duckdb::dbListTables(dbconn)
```

Now to go a little deeper, we can check for duplicates. It might not necessarily be the case that each observation (obs) is unique even if all the Rider IDs are, technically, unique.

```{r}
#|label: 'create dupe-table, count n distinct'

# This is a separate table used to analyze the observations returned as not distinct (n > 1). This adds an extra column labeled "n".
dupeTable <- dplyr::tbl(dbconn,
                        tblPath,
                        check_from = FALSE) |>
    dplyr::select(started_at:end_station_name) |>
    # Counts of unique rows added for column 'n'
    dplyr::add_count(started_at,
                     ended_at,
                     start_station_name,
                     end_station_name) |>
    # Only observations that have been duplicated 1 or more
    # times are shown
    dplyr::filter(n > 1) |>
    # We want to see all rows, not just one row for each obs
    dplyr::ungroup() |>
    dplyr::arrange(started_at) |>
    dplyr::collect()


gtDupes <- dupeTable |>
    dplyr::group_by(started_at) |>
    gt::gt(rowname_col = "row",
           groupname_col = "started_at",
           row_group_as_column = TRUE,
           caption = "Duplicates_Table1") |>
    gt::tab_style(
    style = list(
        gt::cell_text(weight = "bold",
                      align = "center"),
        gt::cell_borders(sides = c("left", "right"))
    ),
    locations = gt::cells_column_labels(gt::everything())
    ) |>
    gt::tab_style(
    style = list(
        gt::cell_borders(sides = c("left", "right")),
        gt::cell_text(align = "center",
                      v_align = "middle")
    ),
    locations = gt::cells_body(gt::everything())
    ) |>
    gt::data_color(columns = start_station_name,
                   target_columns = gt::everything(),
                   method = "auto",
                   palette = "basetheme::brutal") |>
    gt::data_color(columns = "n",
                   target_columns = "n",
                   colors = "purple") |>
    gt::data_color(columns = "started_at",
                   target_columns = "started_at",
                   colors = "blue") |>
    gt::tab_source_note(gt::md(
        "**Table**: Duplicates Table, **Data**: Divvy Data "))

gtDupes

```

[@divvyda]

```{r}

factorDupeTable <- dupeTable |>
    dplyr::distinct(started_at) |>
    dplyr::summarize(n = dplyr::n())


dupeTable |>
    dplyr::distinct(started_at) |>
    dplyr::summarize(n = dplyr::n())

factorDupeTable <- dupeTable |>
    dplyr::collect() |>
    dplyr::mutate(started_at = factor(started_at),
                  .keep = "all")

factorDupeTable$started_at

colorer <- scales::col_factor(
    palette = c("grey10", "grey100"),
    domain = c(factorDupeTable$started_at)
)

dupeTable |>
   # dplyr::group_by(started_at) |>
    
    flextable::flextable() |>
    flextable::theme_zebra() |>
    #flextable::highlight(j = "n",
    #                     color = "grey") |>
    flextable::bg(#i = "started_at",
                  j = "n",
                  bg = "lightgreen",
                  part = "body")
    
dupeTableFT
```

Of the other columns, it seems that the start_time, end_time, start_station, and end_station could show if there are possibly hidden duplicated observations.

We started with 5,719,877 observations (obs) for dates spanning January to December, 2023, then removed 1,388,170 incomplete obs.

I assumed that having the same times/dates and stations for two different ride IDs was a mistake. Although, I do not know how that error would happen. I could have assumed one person could check out multiple bikes at once. In that instance, each bike could be assigned a unique ride_id. That, however, has only happened 18 times over a year. Since it's only one copy every time, that also raises a red flag. I did not notice any other correlations with station_id/name, member_casual, or ride_type for those particular duplicated data.

```{r}
#|label: 'output to distinct duplicates and total obs'

cat(" Distinct copy count of dupes: ", distinctDupeCounts,
    "\n\n",
    "Total observations that have and are duplicates: ",
       length(dupeTable[[1]]))
```

By applying distinct() on dupeTable, we see the only distinct value is 2. We can safely conclude that, of the duplicates, each has a minimum and maximum of 1 extra copy.

Number of rows in the dupeTable is 36. Because each duplicated observation has one duplicate (n = 2), expected removed nobs is 18. The issue is that we need to get rid of not all 36 rows, but just the 1 extra duplicate observation from each, resulting in the expected 18.

```{r}
#|label: 'create un-duped table, count rows'

# The issue is, we need to get rid of not all of these rows, but just the extra duplicate observations. 

# If there were 2 rows of duplicates, we would want to end up with 1 row after removing the extras.
undupedTable <- dupeTable |>
    dplyr::distinct(started_at,
                     start_station_name,
                     ended_at,
                     end_station_name,
                     .keep_all = TRUE)

distinctUndupedCounts <- undupedTable |>
    dplyr::select(started_at) |>
    dplyr::distinct() |>
    dplyr::count() |>
    as.integer()

undupedTable
```

```{r}
#|label: 'output distinct obs, n'

cat("Count of distinct observations: ", 
    distinctUndupedCounts)
```

The count of observed distinct values for the un-duplicated table was indeed 18. So now, it is time to run a count of how rows/observations are in the dataset. There is a difference, though, concerning the correct amount:

```{r}
#|label: 'incorrect/correct distinct observations'

# Run an incorrect count on how many rows or observations there are in the dataset.
incorrectDistinct <- dplyr::tbl(dbconn,
                                tblPath,
                                check_from = FALSE) |>
    dplyr::distinct(dplyr::pick("ride_id")) |>
    dplyr::count(name = "Incorrect Distinct Observations") |>
    dplyr::collect() |>
    as.integer()

# For the correct count of obs
correctDistinct <- dplyr::tbl(dbconn,
                              tblPath,
                              check_from = FALSE) |>
    dplyr::distinct(
        dplyr::pick(
            "started_at",
            "start_station_name",
            "ended_at",
            "end_station_name"
        )
    ) |>
    dplyr::count() |>
    dplyr::collect() |>
    as.integer()
```

```{r}
#| label: 'duplicate correction tibb'
#| fig-cap: "Summary of observations removed by processing."
#| fig-cap-location: margin
#| column: body-outset

# To visualize a summary of what we just determined regarding obs
tibble::tibble(
    "Original Obs" = original_nobs,
    "Uncorrected Complete Obs" = incorrectDistinct,
    "Corrected Complete Obs" = correctDistinct,
    "Removed Obs" = (incorrectDistinct - correctDistinct)
) |>
    polars::as_polars_df()
```

The incorrect number of observations (nobs) was 4,331,707. The correct nobs after removing duplicated obs was 4,331,689. In short, 18 additional obs were removed.

```{r}
#| label: 'overwrite file with correct obs'
#| column: body-outset
#| 
dplyr::tbl(dbconn,
           tblPath,
           check_from = FALSE) |>
    dplyr::select(ride_id:trip_time) |>
    dplyr::distinct(started_at,
                    start_station_name,
                    ended_at,
                    end_station_name,
                    .keep_all = TRUE) |>
    dplyr::arrange(started_at) |>
    dplyr::collect() |>
    duckdb::dbWriteTable(conn = dbconn,
                         name = tblPath,
                         overwrite = TRUE,
                         check_from = FALSE)

dplyr::tbl(dbconn,
           tblPath) |>
    head()
```

## Filtering Data, Smartly

To ensure the conclusions are accurate, outliers should be filtered. Negative and very low trip times might skew trends. The underlying reason for very low trip times is somewhat of an unknown. Perhaps people often change their minds?

```{r}
#| label: "filtering db"

# So you don't have to re-download or re-filter everything after making further adjustments.

tblPath <- "db/data.db"
tblPath_fltrd <- "db/data_fltrd.db"


if(exists("dbconn") == FALSE) {
    dbconn <- DBI::dbConnect(duckdb::duckdb(),
                             dbdir = tblPath,
                             read_only = FALSE,
                             check_from = FALSE)
}


if (duckdb::dbExistsTable(dbconn,
                          "tblPath_fltrd") == FALSE) {
    source("filterDatabase.R")
    filterDatabase()
    }

# To verify the new filtered table exists.
duckdb::dbListTables(dbconn)
```

```{r}
#|eval: FALSE

# If you need to drop any tables
source("duckDrops.R")
```

So this should have removed the major, potentially, non-natural outliers from the dataset which are due to errors (including user errors).

Verify the tables that now exist:

```{r}
duckdb::dbListTables(dbconn)
```

# Frequency Tables and What They Tell Us

Now we're going to create dimension and frequency tables and add those to our database. We can retain the outliers in those tables and perhaps filter as needed later. This is a good place start. If needed, we can dive deeper into other statistical techniques or adjust parameters in these code chunks and overwrite or create new db tables.

By generating frequency tables, I quickly glean insights from the data.

```{r}
# Member_casual
dplyr::tbl(dbconn,
           tblPath_fltrd,
           check_from = FALSE) |>
    dplyr::select(member_casual) |>
    dplyr::group_by(member_casual) |>
    dplyr::summarize(n = dplyr::n()) |>
    dplyr::collect() |>
    duckdb::dbWriteTable(conn = dbconn,
                         name = "db/freq_member.db",
                         overwrite = TRUE,
                         check_from = FALSE)

dplyr::tbl(dbconn,
           "db/freq_member.db")

```

This indicates that there are nearly twice as many trips taken by annual subscribers than by casual users. So, it would seem that there is some correlation between how often a person uses the service and whether they choose to subscribe to an annual membership plan.

```{r}
# Rideable_type 
dplyr::tbl(dbconn,
           tblPath_fltrd,
           check_from = FALSE) |>
    dplyr::select(rideable_type) |>
    dplyr::group_by(rideable_type) |>
    dplyr::summarize(n = dplyr::n()) |>
    dplyr::collect() |>
    duckdb::dbWriteTable(conn = dbconn,
                         name = "db/freq_rTypes.db",
                         overwrite = TRUE,
                         check_from = FALSE)

dplyr::tbl(dbconn,
           "db/freq_rTypes.db")
```

This tells us that there are nearly twice as many trips taken with non-electric bikes as electric bikes. There are differences in cost for casual members, so that might be one determining factor. Also, perhaps the associated health benefits with exercise would help explain higher non-electric bicycle use.

```{r}
duckdb::dbListTables(dbconn)
```

```{r}

# Miles
dplyr::tbl(dbconn,
           tblPath_fltrd,
           check_from = FALSE) |>
    dplyr::select(miles) |>
    dplyr::collect() |>
    dplyr::mutate(miles = dplyr::case_when(miles >= 1 ~ round(miles,
                                                              digits = 0),
                                           miles < 1 ~ round(signif(miles, 3),
                                                             digits = 1))) |>
    dplyr::group_by(miles) |>
    dplyr::summarize(n = dplyr::n()) |>
    dplyr::arrange(miles) |>
    duckdb::dbWriteTable(conn = dbconn,
                         name = "db/freq_miles.db",
                         check_from = FALSE,
                         overwrite = TRUE)


dplyr::tbl(dbconn,
           "db/freq_miles.db") |>
    dplyr::arrange(desc(n)) |>
    head(n = 10)
```

```{r}
dplyr::tbl(dbconn,
           "db/freq_miles.db") |>
    dplyr::collect() |>
    dplyr::mutate(n = n/1000) |>
    dplyr::rename("Trips (in thousands)" = n,
                  "Miles" = miles) |>
    plot()
```

This shows that around 0.3 to 3 miles is the distance traveled on most trips taken. The fact that casual bicycles are used more often perhaps makes a little more sense given that people are not traveling long distances.

```{r}
duckdb::dbListTables(dbconn)
```

```{r}

# MPH
dplyr::tbl(dbconn,
           tblPath_fltrd,
           check_from = FALSE) |>
    dplyr::select(mph) |>
    dplyr::mutate(mph = round(mph, digits = 0)) |>
    dplyr::group_by(mph) |>
    dplyr::summarize(n = dplyr::n()) |>
    dplyr::collect() |>
    dplyr::arrange(mph) |>
    duckdb::dbWriteTable(conn = dbconn,
                         name = "db/freq_mph.db",
                         check_from = FALSE,
                         overwrite = TRUE)

dplyr::tbl(dbconn,
           "db/freq_mph.db")
```

```{r}
dplyr::tbl(dbconn,
           "db/freq_mph.db") |>
    dplyr::collect() |>
    plot()

```

The most common overall mph for trips fall in between 2-11 mph. This is what we might expect for city travel.

```{r}
duckdb::dbListTables(dbconn)
```

```{r}
# WkDay
dplyr::tbl(dbconn,
           tblPath_fltrd,
           check_from = FALSE) |>
    dplyr::select(started_at) |>
    dplyr::mutate(wkday = lubridate::wday(started_at)) |>
    dplyr::group_by(wkday) |>
    dplyr::summarize(n = dplyr::n()) |>
    dplyr::arrange(wkday) |>
    dplyr::collect() |>
    dplyr::mutate(wkday = c("Sun",
                            "Mon",
                            "Tue",
                            "Wed",
                            "Thu",
                            "Fri",
                            "Sat"),
                  wkday = forcats::as_factor(wkday),
                  wkday = forcats::fct_inorder(wkday)) |>
    duckdb::dbWriteTable(conn = dbconn,
                         name = "db/freq_wkDay.db",
                         check_from = FALSE,
                         overwrite = TRUE)

dplyr::tbl(dbconn,
           "db/freq_wkday.db") 
```

```{r}

dplyr::tbl(dbconn,
           "db/freq_wkday.db") |>
    dplyr::collect() |>
    plot()
```

There is a slight reduction in trips taken on Sundays and Mondays. This could be explained by the fact that most trips are taken by annual subscribers, possibly because they are biking to work and such. Much less people will be biking to work on Sundays. Taking 3-day weekends is common in the workplace, where most would likely opt to use up their sick/vacation days on Monday, which might explain the down tick in trips taken on Mondays. Also, those who are working Tuesdays-Saturdays could be taking Sunday and Mondays off.

```{r}

# Months
dplyr::tbl(dbconn,
           tblPath_fltrd,
           check_from = FALSE) |>
    dplyr::select(started_at) |>
    dplyr::mutate(months = lubridate::month(started_at,
                                            label = FALSE,
                                            abbr = TRUE
                                            )) |>
    dplyr::group_by(months) |>
    dplyr::summarize(n = dplyr::n()) |>
    dplyr::collect() |>
    dplyr::arrange(months) |>
    dplyr::mutate(months = c(month.abb),
                  months = forcats::as_factor(months),
                  months = forcats::fct_inorder(months)) |>
    duckdb::dbWriteTable(conn = dbconn,
                         name = "db/freq_month.db",
                         check_from = FALSE,
                         overwrite = TRUE)

dplyr::tbl(dbconn,
           "db/freq_month.db")

dplyr::tbl(dbconn,
           "db/freq_month.db") |>
    dplyr::summarize(sum = sum(n))
```

```{r}
dplyr::tbl(dbconn,
           "db/freq_month.db") |>
    dplyr::collect() |>
    plot()
```

The monthly ridership does differ substantially between certain months. Summer months (relative to North America) is when we see the largest increase in ridership. Spring and Fall seasons lie in between Summer and Winter in terms of scale of trips taken

```{r}
dplyr::tbl(dbconn,
           tblPath_fltrd) |>
    dplyr::mutate(trip_time = round(trip_time,
                                    digits = 0)) |>
    dplyr::group_by(trip_time) |>
    dplyr::summarize(n = dplyr::n()) |>
    dplyr::arrange(trip_time) |>
    dplyr::collect() |>
    duckdb::dbWriteTable(conn = dbconn,
                         name = "db/freq_tripTime.db",
                         check_from = FALSE,
                         overwrite = TRUE)

dplyr::tbl(dbconn,
           "db/freq_tripTime.db") |>
    dplyr::collect() |>
    dplyr::arrange(desc(n)) |>
    head(n = 20)

dplyr::tbl(dbconn,
           "db/freq_tripTime.db") |>
    dplyr::summarize(sum = sum(n))



```

```{r}
dplyr::tbl(dbconn,
           "db/freq_tripTime.db") |>
    dplyr::collect() |>
    plot()
```

The time riders are usually spending on these trips lies in between 5-15 minutes.

```{r}
dplyr::tbl(dbconn,
           tblPath_fltrd) |>
    dplyr::select(start_station_name) |>
    dplyr::group_by(start_station_name) |>
    dplyr::summarize(n = dplyr::n()) |>
    dplyr::arrange(start_station_name) |>
    dplyr::collect() |>
    duckdb::dbWriteTable(conn = dbconn,
                         name = "db/freq_startNames.db",
                         check_from = FALSE,
                         overwrite = TRUE)


dplyr::tbl(dbconn,
           "db/freq_startNames.db")
```

```{r}
dplyr::tbl(dbconn,
           tblPath_fltrd) |>
    dplyr::select(start_station_name,
                  end_station_name) |>
    dplyr::group_by(start_station_name,
                    end_station_name) |>
    dplyr::summarize(n = dplyr::n()) |>
    dplyr::arrange(start_station_name) |>
    dplyr::collect() |>
    duckdb::dbWriteTable(conn = dbconn,
                         name = "db/freq_pairStations.db",
                         check_from = FALSE,
                         overwrite = TRUE)

dplyr::tbl(dbconn,
           "db/freq_pairStations.db")

dplyr::tbl(dbconn,
           "db/freq_pairStations.db") |>
    dplyr::summarize(sum = sum(n))

```

```{r}
dplyr::tbl(dbconn,
           tblPath_fltrd)
```

```{r}
#|label: 'rider count by hour table'
dplyr::tbl(dbconn,
           tblPath_fltrd,
           check_from = FALSE) |>
    dplyr::select(started_at) |>
    dplyr::mutate("hour" = lubridate::hour(started_at)) |>
    dplyr::group_by(hour) |>
    dplyr::summarise("Total_Riders" = dplyr::n()) |> #or count?
    dplyr::arrange(hour) |>
    dplyr::collect() |>
    dplyr::mutate("hour" = hms::hms(hours = hour),
                  "hour" = format(strptime(hour, format = "%H"), "%r"),
                  "index" = seq(1:24)) |>
    duckdb::dbWriteTable(conn = dbconn,
                         name = "db/rideHours.db",
                         check_from = FALSE,
                         overwrite = TRUE)
```

```{r}
#| eval: true

hours_of_Riders <- dplyr::tbl(dbconn,
                              "db/rideHours.db") |>
    dplyr::collect()

x <-
    stringr::str_sub_all(hours_of_Riders[[1]],
                         start = 1,
                         end = 2) |>
    as.character() |>
    stringr::str_remove(pattern = "^0")


y <- stringr::str_sub(hours_of_Riders[[1]],
                      start = -2,
                      end = -1) |>
    stringr::str_to_lower()

simpleTimes <- stringr::str_c(x, sep = " ", y)

hours_of_Riders[[1]] <- simpleTimes

hours_of_Riders
```

::: column-page
```{r}
#| label: 'radial-column plot'
#| fig-column: page
#| fig-cap: "The time of day people tend to be riding."
#| fig-cap-location: bottom
#| title: "Time of Day and Volume of Cyclers"
#| fig-width: 15
#| fig-height: 12

hoursAnimate <- ggplot2::ggplot(data = hours_of_Riders,
                mapping = ggplot2::aes(
                    x = reorder(hour, .data$index),
                    y = Total_Riders,
                    fill = Total_Riders)) +
    ggplot2::geom_col() +
    ggplot2::coord_radial(start = 2 * pi,
                          inner.radius = .2) +
    ggplot2::xlab(NULL) +
    ggplot2::ylab(NULL) +
    ggplot2::scale_fill_distiller(palette = "Spectral",
                                  direction = 1) +
    ggplot2::labs(
        title = "Average Riders by the Hour of Day",
        subtitle = "(Jan-Dec 2023)",
        caption = "Data from cyclistic database.",
        tag = "Figure 1.c"
        ) +
    ggplot2::theme(
        title = ggplot2::element_text(
            size = 20,
            lineheight = 4,
            color = "white"
        ),
        
        text = ggplot2::element_text(color = "white"),
        
        panel.background = ggplot2::element_rect(fill = "black"),
        panel.grid.major.x = ggplot2::element_line(linewidth = 1,
                                                   color = 'grey10'),
        panel.grid.major.y = ggplot2::element_blank(),
        
        
        plot.background = ggplot2::element_rect(fill = "black"),
        
        axis.line.x = ggplot2::element_line(
            linewidth = 1,
            color = 'grey10',
            arrow = ggplot2::arrow()
        ),
        
        axis.ticks.y = ggplot2::element_blank(),
        
        axis.text.x = ggplot2::element_text(
            size = 18,
            color = "grey90",
            face = "bold"
        ),
        axis.text.y = ggplot2::element_blank(),
        
        legend.background = ggplot2::element_rect(fill = "black"),
        legend.ticks = ggplot2::element_line(color = "black",
                                             linewidth = .5),
        legend.text = ggplot2::element_text(color = 'grey80',
                                            size = 13),
        legend.position = "right",
        legend.justification = "center",
        legend.direction = "vertical",
        legend.key.size = grid::unit(1.5, "cm")
       ) +
    gganimate::transition_reveal(along = seq(length(hour)))

my_anim <- gganimate::animate(hoursAnimate,
                             renderer = gganimate::gifski_renderer())

my_anim
```
:::

# Data Modeling

**Non-parametric descriptive:**

-   median, interquartile-range (IQR)

## **Semi-parametric and non-parametric** methods

**Description**

-   To predict the outcome variable using independent variables

    -   outcome variable(s) to test

        -   member_casual

**Statistical methods**

-   Binary Logistic regression analysis

**Data type**

-   Outcome variable:

    -   Categorical (\>= 2 categories)

        -   member_casual

        -   rideable_type

-   Independent variable(s):

    -   Categorical (\>= 2 categories) or

        -   hour

            -   24 categories

        -   day_of_week

            -   7 categories

        -   month

            -   12 categories

        -   holiday

            -   2 categories (yes or no?)

        -   member_casual

            -   2 categories

        -   rideable_type

            -   2 categories

    -   Continuous or

        -   trip_time

        -   mph

        -   miles

        -   geospatial location

    -   both

        -   member_causal

        -   rideable_type

        -   hour

        -   day

        -   trip_time

        -   mph

        -   miles

        -   geospatial location

\
Mean \> Median

**"Right-Skewed Histogram"**\
- The right side of the histogram plot has lower frequencies than the left.

Unimodal

```{r}

latsTable <- dplyr::tbl(dbconn,
                        tblPath_fltrd,
                        check_from = FALSE) |>
    dplyr::select(start_lat) |>
    dplyr::collect() |>

    # because 5 digits after decimal place gives down to 1.1m accuracy
    # 4 digits are accurate up to 11m
    dplyr::mutate(start_lat = signif(start_lat, digits = 5),
                  .keep = "none") |>
    table() |>
    as.data.frame()

```

```{r}
#| eval: false


longsTable <- dplyr::tbl(dbconn,
                       tblPath_fltrd,
                       check_from = FALSE) |>
    dplyr::select(start_lng) |>
    dplyr::collect() |>
    # because 5 digits after decimal place gives down to 1.1m accuracy
    # 4 digits are accurate up to 11m
    dplyr::mutate(start_lng = signif(start_lng, digits = 5),
                  .keep = "none") |>
    table() |>
    as.data.frame()

```

```{r}


latlngTable <- dplyr::tbl(dbconn,
                       tblPath_fltrd,
                       check_from = FALSE) |>
    dplyr::select(start_lat, start_lng) |>
    # because 5 digits after decimal place gives down to 1.1m accuracy
    # 4 digits are accurate up to 11m
    dplyr::collect() |>
    dplyr::mutate(start_lat = round(start_lat, digits = 3),
                  start_lng = round(start_lng, digits = 3),
                  .keep = "none") |>
    dplyr::group_by(start_lat, start_lng) |>
    #dplyr::collect() |>
    table() |>
    as.data.frame()

latlngTable |>
    head(n = 10)
```

```{r}

# Trying to get clues as to how we should alter the table. 
coordsTable <- dplyr::tbl(dbconn,
                       tblPath_fltrd,
                       check_from = FALSE) |>
    dplyr::select(start_lat, 
                  start_lng) |>
    # because 5 digits after decimal place gives down to 1.1m accuracy
    # 4 digits are accurate up to 11m
   # dplyr::filter(start_lat, signif(start_lat, digits = 7) == TRUE) |>
    #dplyr::collect() |>
    dplyr::mutate(start_lat = as.character(start_lat),
                  start_lng = as.character(start_lng),
                  .keep = "none") |>
    dplyr::filter(stringr::str_length(start_lat) >= 8,
                  # 9 to account for negative symbol in lng
                  stringr::str_length(start_lng) >= 9) |>
    dplyr::group_by(start_lat, 
                    start_lng) |>
    dplyr::summarise(n = dplyr::n()) |>
    dplyr::arrange(n) |>
    dplyr::collect() |>
    as.data.frame()

```

```{r}

# Trying to get clues as to how we should alter the table. 
coordsTable2 <- dplyr::tbl(dbconn,
                           tblPath_fltrd,
                           check_from = FALSE) |>
    dplyr::select(start_lat, start_lng) |>
    # because 5 digits after decimal place gives down to 1.1m accuracy
    # 4 digits are accurate up to 11m
    dplyr::mutate(start_lat = round(start_lat, digits = 5),
                  start_lng = round(start_lng, digits = 5),
                  start_lat = as.character(start_lat),
                  start_lng = as.character(start_lng),
                  .keep = "none") |>
    dplyr::filter(stringr::str_length(start_lat) >= 8,
                  stringr::str_length(start_lng) >= 9) |>
    dplyr::group_by(start_lat, start_lng) |>
    dplyr::summarise(n = dplyr::n()) |>
  #  dplyr::filter(n >= 47) |>
    dplyr::arrange(n) |>
    dplyr::collect() |>
    # seems base R's table() is a lot slower than dplyr
    #table() |>
    as.data.frame()

coordsTable2 |>
    head(n = 10)
```

Assuming their plans to expand the "over 800 stations" by 250, I'd like to set the parameters to achieve around \~1050 rows. So I set n to include groups with at least 47 riders.

```{r}

# according to this, there are 1487 unique station id's in the dataset now
dplyr::tbl(dbconn,
           tblPath_fltrd,
           check_from = FALSE) |>
    dplyr::select(start_station_name) |>
    dplyr::distinct(start_station_name) |>
    dplyr::summarise(n = dplyr::n())
```

The 5th decimal place is accurate up to 1.1m, so there ends up being groupings for bikes rented from stations located along a street or at intersections of two streets. You can tell by the start_station_name. If it's an intersection it will say something like "1st St & 2nd St.". If it's one street, it will likely just say "1st St". There are likely some exceptions to that rule. That helps explain why the geospatial coordinates slightly differ for entries with the same stations and IDs.

```{r}
#| eval: false

mapTest <- dplyr::tbl(dbconn,
           tblPath_fltrd,
           check_from = FALSE) |>
    dplyr::select(start_station_id,
                  start_station_name,
                  start_lat,
                  start_lng) |>
    dplyr::group_by(start_station_name,
                    start_station_id,
                    start_lat,
                    start_lng) |>
    dplyr::summarise(n = dplyr::n()) |>
    dplyr::filter(stringr::str_detect(
        start_station_name,
        "Ogden Ave & Chicago Ave") == TRUE,
        n > 1) |>
    dplyr::arrange(start_station_name, 
                   start_station_id, 
                   start_lat,
                   start_lng,
                   n) |>
    dplyr::collect()

mapTest |>
    dplyr::ungroup() |>
    dplyr::select(start_lng, start_lat, n) |>
   # dplyr::group_by(start_lng, start_lat) |>
    sf::st_as_sf(coords = c(1:2),
             crs = 4326) |>
    mapview::mapview()
```

A quick test of the data:

```{r}
#| eval: false

coordsDF <- mapTest |>
    dplyr::ungroup() |>
    dplyr::select(start_lng,
                  start_lat) |>
    dplyr::add_count(start_lng,
                     start_lat) |>
    dplyr::slice_head(n = 100)

coordsDF |>
    sf::st_as_sf(coords = c(1:2),
                 crs = 4326) |>
    mapview::mapview()
```

```{r}
#| eval: false

# according to this, there are 1428 unique station IDs in the dataset now
countStations <- dplyr::tbl(dbconn,
                            tblPath,
                            check_from = FALSE) |>
    dplyr::select(start_station_id, start_station_name) |>
    #dplyr::distinct(dplyr::pick(start_station_id, start_station_name)) |>
    dplyr::group_by(start_station_id,
                    start_station_name) |>
    dplyr::summarise(n = dplyr::n()) |>
    dplyr::filter(n > 0) |>
    dplyr::collect()

summary(countStations)
```

```{r}
#| label: "column plot"
#| column: body-outset

memberCasuals_monthly  <- dplyr::tbl(dbconn,
           tblPath_fltrd) |>
    dplyr::select(started_at,
                  member_casual) |>
    dplyr::mutate('month' = lubridate::month(started_at)) |>
    dplyr::group_by(month,
                    member_casual) |>
    dplyr::summarize("riderCount" = dplyr::n()) |>
    dplyr::arrange(month)

dplyr::collect(memberCasuals_monthly) |>
    ggplot2::ggplot() +
    ggplot2::geom_col(mapping = ggplot2::aes(x = factor(month),
                                             y = riderCount,
                                             fill = member_casual),
                      color = "black",
                      position = 'dodge2') +
    ggplot2::scale_x_discrete(labels = month.abb,
                              name = "Month") +
    ggplot2::scale_fill_brewer(palette = 'Set2') +
    ggplot2::theme_dark() +
    ggplot2::labs(
    title = "Monthly Ridership: Members vs Casuals",
    subtitle = "(Jan-Dec 2023)",
    caption = "Data from cyclistic database.",
    tag = "Figure 1.b")
```

for quick reference with using Tsibble syntax

```{r}
#|label: 'grouped tibb'

grouped_byDay <- dplyr::tbl(dbconn,
                            tblPath_fltrd) |>
    dplyr::select(started_at,
                  member_casual) |>
    dplyr::collect() |>
    dplyr::mutate(started_at = as.Date(started_at)) |>
    dplyr::group_by(started_at,
                    member_casual) |>
    dplyr::summarize(n = dplyr::n(),
                     sdev = stats::sd(n))
```

```{r}
#|label: 'to grouped tsibb'

# tsibble, time-series table/tibble seems to make time series plots more straightforward
grouped_tsi <- grouped_byDay |>
    tsibble::as_tsibble(key = c(member_casual,
                                n),
                        index = started_at) |>
    dplyr::arrange(started_at)
```

```{r}
#|label: "map query setup"

# chicago starting coordinates for leaflet, setView
chicago <- maps::us.cities |>
    dplyr::select("name",
                  "long",
                  "lat") |>
    dplyr::filter(name == "Chicago IL")

# full dataset coordinates, might need to sample
coordQry <- dplyr::tbl(dbconn,
                       tblPath_fltrd) |>
    dplyr::select(start_lng,
                  start_lat) |>
    dplyr::add_count(start_lng,
                     start_lat) |>
    dplyr::distinct() |>
    dplyr::arrange(desc(n)) |>
    dplyr::collect()


coordQry_small <- dplyr::tbl(dbconn,
                             tblPath_fltrd) |>
    dplyr::select(start_lng,
                  start_lat) |>
    dplyr::add_count(start_lng,
                     start_lat) |>
    dplyr::distinct() |>
    dplyr::arrange(desc(n)) |>
    dplyr::collect() |>
    dplyr::slice_head(n = 50)

```

```{r}
#|label: "mapview"
#|eval: false


coordQry_small |>
    sf::st_as_sf(coords = c(1:2),
                crs = 4326) |>
    mapview::mapview()

```

```{r}

ojs_define(js_tsi = grouped_tsi)
```

```{ojs}
jsData = transpose(js_tsi)
```

::: column-page
```{ojs}
Plot.plot({
    grid: true,
    color: {legend: true},
    marks: [
        Plot.dot(jsData, {x: 'started_at', y: 'n', fill: 'member_casual'})
]
})
```
:::

```{ojs}
Plot.lineY(jsData, {x: "started_at", y: "n"}).plot()
```

```{r}

locsStart <- dplyr::tbl(dbconn,
                      tblPath_fltrd) |>
    dplyr::select(start_station_name
                  #start_lng,
                  #start_lat
                  ) |>
    dplyr::distinct() |>
    dplyr::arrange(start_station_name) |>
    dplyr::rename("station_name" = start_station_name
                 # "lng" = start_lng,
                 # "lat" = start_lat
                 )

locsEnd <- dplyr::tbl(dbconn,
                      tblPath_fltrd) |>
    dplyr::select(end_station_name
                  #end_lng,
                  #end_lat
                  ) |>
    dplyr::distinct() |>
    dplyr::arrange(end_station_name) |>
    dplyr::rename("station_name" = end_station_name
                  #"lng" = end_lng,
                  #"lat" = end_lat
                  )

locs <- locsStart |>
    dplyr::left_join(locsEnd) |>
    dplyr::arrange(station_name)

locs
```

```{r}

flowData <- dplyr::tbl(dbconn,
                       tblPath_fltrd) |>
    dplyr::select(start_station_name,
                  end_station_name) |>
    dplyr::group_by(start_station_name,
                    end_station_name) |>
    dplyr::summarize(n = n()) |>
    dplyr::ungroup() |>
    dplyr::arrange(desc(n)) |>
    dplyr::rename("from_station" = start_station_name,
                  "to_station" = end_station_name) |>
    dplyr::collect() |>
    dplyr::slice_head(n = 50)

flowData
summary(flowData$n)
```

```{r}

locationData <- dplyr::tbl(dbconn,
                           tblPath_fltrd) |>
    dplyr::select(start_station_name,
                  started_at,
                  ended_at,
                  trip_time) |>
    dplyr::group_by(start_station_name) |>
    dplyr::mutate("trip_time" = round(trip_time,
                                      digits = 0)) |>
    dplyr::summarize("trip_count" = dplyr::n(),
                     "first_date" = min(started_at),
                     "last_date" = max(ended_at),
                     "avg_trip_time" = mean(trip_time)
                     ) |>
    dplyr::rename("station_name" = start_station_name) |>
    dplyr::arrange(station_name) |>
    dplyr::collect()

locationData
```

```{r}

mergd <- merge(x = locationData,
               y = locs,
               by.x = "station_name",
               by.y = "station_name",
               sort = FALSE)

mergd <- mergd |>
    dplyr::mutate(avg_trip_time = round(avg_trip_time,
                                    digits = 0)) |>
    dplyr::arrange(desc(trip_count)) |>
    dplyr::slice_head(n = 25)    

mergd
```

```{r}
#|class-output: plotMod

ef_test <- epiflows::make_epiflows(flows = flowData,
                                   locations = mergd,
                                   duration_stay = "avg_trip_time",
                                   num_cases = "trip_count")

epiflows::vis_epiflows(ef_test)
```

These maps track bike-sharing activity going on worldwide and in Chicago.

[Bike Share Map](https://bikesharemap.com/#/8/-87.5771/41.3747/) [@bikesha]

[CityBikes: bike sharing networks around the world](https://citybik.es/) [@citybike]

```{r, eval = FALSE}
#|label: 'drops duckDB tables'
source("duckDrops.R")
```

```{r}
#|label: 'duckDB Shutdown'
duckdb::dbDisconnect(dbconn, shutdown = TRUE)
unlink("db",
       recursive = TRUE)
```
