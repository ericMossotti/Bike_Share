---
title: "Bike-Sharing in the Streets of Chicago"
author: "Eric Mossotti"
date: "05-23-2024"
date-modified: last-modified
date-format: "MMM D, YYYY"

bibliography: references.bib
repo: https://github.com/ericMossotti/Bike_Share
source: index.qmd
abstract-title: "Objective"
abstract: "Communicating reproducible, data-driven insights."
description-meta: "Communicate reproducible, data-driven insights."

code-links:
    - text: "Project Repo"
      href: https://github.com/ericMossotti/Bike_Share
code-fold: true
code-copy: hover
code-overflow: wrap
code-tools: true
code-link: true

toc: true
toc-location: left
toc-depth: 5
number-sections: true
link-external-newwindow: true

smooth-scroll: true
fig-responsive: true
echo: true

citation-location: margin
citations-hover: true
link-citations: true
csl: csl/apa.csl
zotero: true

callout-appearance: simple

license: CC BY-SA
funding: "The author(s) received no specific funding for this work."
---

```{r, include = FALSE}

knitr::opts_chunk$set(message = FALSE, 
                      warning = FALSE)
```

------------------------------------------------------------------------

# Intro

::: {#offcanvas1 .offcanvas .offcanvas-end tabindex="-1" aria-labelledby="offcanvas" style="width: auto"}
::: offcanvas-header
::: {#offcanvasLabel .h5 .offcanvas-title}
Import Processing Code
:::

::: {.btn-close type="button" data-bs-dismiss="offcanvas" ariaLabel="Close"}
:::
:::

::: offcanvas-body
::: flex-code
```{r}
#| label: importOrConnect
#| code-summary: First, we decide whether to download and do the necessary initial processing steps or skip that if we have already done this and are just tinkering around with the project. 
#| tidy: true

# Does the database directory exist? If not, means we need to import data
if (exists("dbconn") == FALSE &&
    dir.exists("db") == FALSE) {
  
  source("Scripts/importClean.R")
  
  # Else connect to the existing, persistent db  
} else {
  
  database_path <- "db/data.db"
  original_path <- "db/original_data.db"
  complete_path <- "db/complete_data.db"
  filtered_path <- "db/filtered_data.db"
  
  dbconn <- DBI::dbConnect(duckdb::duckdb(), 
                           dbdir = database_path, 
                           read_only = FALSE)
}

source("Scripts/tabler.R")
source("Scripts/plotter.R")
source("Scripts/transformData.R")
source("Scripts/chisqTest.R")

```

```{r}
#| label: importProcessScript
#| code-summary: This then would be executed if conditions were met. Usually, this would only execute if there is no db folder and associated files.
#| file: "Scripts/importClean.R"
#| eval: false 
```
:::
:::
:::

## Stakeholders

The primary stakeholders in this analysis are Divvy, Lyft (the parent company of Divvy), and the City of Chicago Department of Transportation. The analysis aims to provide these stakeholders with data-driven insights to enhance the Divvy bike-sharing service, better serving the residents of Chicago and its users. The initial rationale behind Divvy's implementation included improving air quality, promoting economic recovery, and reducing traffic congestion within the city. [@aboutdi]

## Source

::: p-1
The raw 2023 dataset was imported from Divvy Data. [@divvyda]
:::

::: d-flex
::: column-screen-inset
```{r}
#| label: tbl-raw
#| tbl-cap: Raw data
#| cap-location: top
#| tidy: true

# List of column labels to feed tabler() and add_multiple_footnotes()
location_list <- dplyr::tbl(dbconn, original_path) |>
dplyr::collect() |>
colnames() |>
as.list()

# A simple list of footnotes to feed tabler() and add_multiple_footnotes().
note_list <- list(
"Anonymized trip identifier.", 
"The bicycle type.", 
"Starting date-time (to the second).",
"Ending date-time (to the second).",
"Station name of where the trip started.",
"Station ID of where the trip started.",
"Station name of where the trip ended.",
"Station ID of where the trip ended.",
"Latitude associated with the starting location.",
"Longitude associated with the starting location.",
"Latitude associated with the ending location.",
"Longitude associated with the ending location.",
"If someone is an annual subscriber or not."
)

dplyr::tbl(dbconn, original_path) |>
dplyr::collect() |>
dplyr::slice_head(n = 10) |>
tabler(
title = "The Original Data",
source_note = gt::md("**Source**: Divvy Data"),
note_list = note_list,
location_list = location_list,
noteColumns = TRUE
) |>
gt::tab_options(
table.font.size = gt::pct(75),
footnotes.multiline = FALSE
)

```
:::
:::

::: {#offcanvas100 .offcanvas .offcanvas-start tabindex="1" aria-labelledby="offcanvas" style="width: auto"}
::: offcanvas-header
::: {.h5 .offcanvas-title}
Glimpse
:::

::: {.btn-close type="button" data-bs-dismiss="offcanvas" ariaLabel="Close"}
:::
:::

::: offcanvas-body
::: flex-code
```{r}
#| label: glimpseRaw
#| code-summary: A 'glimpse' function output of the raw data with data type information.
#| tidy: true

dplyr::tbl(dbconn, original_path) |>
dplyr::collect() |>
tibble::glimpse() 
```
:::
:::
:::

::: {.d-grid .gap-0}
::: {.btn .btn-outline-warning .mb-3 .mt-3 type="button" data-bs-toggle="offcanvas" data-bs-target="#offcanvas100" aria-controls="offcanvas"}
R console output of the raw data
:::
:::

## Design

Another worthy objective of this analysis is to achieve reproducibility and efficiency. To facilitate future research and enable subsequent analyst teams to build upon this work, the project aimed to provide adequate code documentation and adhere to best practices regarding clean and modular code.

For instance, certain design decisions were incorporated to eliminate the need for re-downloading and re-processing data. For analysts conducting analysis over an extended period, such as days or months, on this dataset, it is now possible to simply reconnect to the single database file containing all the original data, including tables generated throughout the analysis process, following the initial download and subsequent processing.

The underlying code incorporates an if-else decision, which includes a source code script responsible for handling the initial data processing and establishing the database filesystem. Opting for a persistent DuckDB filesystem (as opposed to a purely in-memory solution) appeared optimal in terms of simplicity, cost-effectiveness of SQL database queries, and retaining progress made over extended periods. [@whyduck]

To streamline the process, reduce code duplication, and maintain consistent formatting throughout the project, reusable functions were developed for generating most of the tables and figures. These functions are located in the "Scripts" folder within the working directory. Their modular design not only simplifies the implementation of formatting changes but also facilitates the integration of additional code snippets when necessary. For instance, certain plots might require limiting the range of the axes, which can be achieved by combining these functions with appropriate code addendum. By leveraging these functions, the project benefits from reduced redundancy, improved efficiency, and cohesive formatting across all visualizations and data representations.

## Initial Database Table List

```{r}
#| label: tbl-dbList
#| code-summary: These are the starting tables contained in the data/data.db file. Noting this as we will be adding many more tables in the later stages.
#| tbl-cap: Initial DB Table List
#| tidy: true

dbList <- duckdb::dbListTables(dbconn) |>
as.data.frame() |>
tabler(
title = "Database Tables",
note_list = list(gt::md("Tables in `db/data.db` at this stage")),
location_list = list("duckdb::dbListTables(dbconn)"),
noteColumns = TRUE,
source_note = gt::md("**Source**: `db/data.db`")
) |>
gt::cols_label("duckdb::dbListTables(dbconn)" = "Table Paths") |>
gt::cols_align(align = "left")

dbList
```

![A view of the filesystem directory. Notice there are not separate files for tables. Technically, data.db, in this view, does not represent a table name, but the name of the database.](images/dbFiles_start.png){.column-margin}

# Tidying

::: p-2
The starting observation count was 5,719,877. Then 1,388,170 incomplete observations were then removed by the initial processing script.
:::

::: {.d-grid .gap-0}
::: {.callout-warning icon="false" width="auto"}
Code processing steps are accessible via buttons like the one below. Drop-down code summaries and tables therein add context and transparency regarding the presented findings to enhance the reader's understanding.
:::

::: {.btn .btn-outline-danger .mb-3 type="button" data-bs-toggle="offcanvas" data-bs-target="#offcanvas1" aria-controls="offcanvas"}
Data processing documentation
:::
:::

## Duplicates

::: {#offcanvas2 .offcanvas .offcanvas-end tabindex="-1" aria-labelledby="offcanvas" style="width: auto"}
::: offcanvas-header
::: {.h5 .offcanvas-title}
Code to Remove Duplicates
:::

::: {.btn-close type="button" data-bs-dismiss="offcanvas" ariaLabel="Close"}
:::
:::

::: offcanvas-body
::: flex-code
```{r}
#| label: originalNobs
#| code-summary: First, record original observations from the raw data.
#| tidy: true

# Need to save this count for the summary table later
original_nobs <- dplyr::tbl(dbconn, original_path) |>
dplyr::collect() |>
nrow()
```

```{r}
#| label: duplicates_gt
#| code-summary: Create a table containing the duplicated observations.
#| tidy: true

# This is a separate table used to analyze the observations
# returned as not distinct (n > 1). This adds an extra column, labeled "n".
dupeTable <- dplyr::tbl(dbconn, complete_path) |>
dplyr::select(started_at:end_station_name) |>
# Counts of unique rows added for column 'n'
dplyr::add_count(started_at, ended_at, start_station_name, end_station_name) |>
# Only observations that have been duplicated 1 or more times are shown.
dplyr::filter(n > 1) |>
# To see all rows, not just one row for each obs.
dplyr::ungroup() |>
dplyr::arrange(started_at) |>
dplyr::collect()
```

```{r}
#| label: duplicateObs count
#| code-summary: Record a count of distinct duplicates and total observations.
#| tidy: true

distinctCopiesCount <- dupeTable |>
dplyr::distinct(n) |>
as.integer()

duplicateObs <- length(dupeTable[[1]])
```

```{r}
#| label: undupedTable
#| code-summary: Create a table of the now unduplicated observations seen earlier.
#| tidy: true

# The issue is, we need to get rid of not all of these rows, but just the extra duplicate observations.

# If there were 2 rows of duplicates, one would want to end up with 1 row after removing the extras.
undupedTable <- 
dupeTable |>
dplyr::distinct(started_at, 
start_station_name, 
ended_at, 
end_station_name)
```

```{r}
#| label: incorrect distinct obs count
#| code-summary: Record a count of the incorrect observations.
#| tidy: true

# Run an incorrect count on how many rows or observations there are in the dataset.
count_incorrectDists <- dplyr::tbl(dbconn, complete_path) |>
dplyr::distinct(dplyr::pick("ride_id")) |>
dplyr::count(name = "Incorrect Distinct Observations") |>
dplyr::collect() |>
as.integer()
```

```{r}
#| label: count_correctDists count
#| code-summary: Record a count of the correct observations.
#| tidy: true

# For the correct count of obs
count_correctDists <- dplyr::tbl(dbconn, complete_path) |>
dplyr::distinct(dplyr::pick(
"started_at",
"start_station_name",
"ended_at",
"end_station_name"
)) |>
dplyr::count() |>
dplyr::collect() |>
as.integer()
```

```{r}
#| label: writeUnduplicated
#| code-summary: Lastly, write the unduplicated data to the database.
#| tidy: true

if (isFALSE(duckdb::dbExistsTable(dbconn, "db/data_unduped.db"))) {
dplyr::tbl(dbconn, complete_path) |>
dplyr::distinct(started_at,
start_station_name,
ended_at,
end_station_name,
.keep_all = TRUE) |>
dplyr::arrange(started_at) |>
dplyr::collect() |>
duckdb::dbWriteTable(conn = dbconn,
name = "db/data_unduped.db",
overwrite = TRUE)
}
```
:::
:::
:::

::: {.mt-2 .mb-2}
A crucial question arises: How can one identify and handle duplicate data? This section covers the process of checking for duplicates and selectively removing them while exercising caution. It is essential to recognize that the presence of unique values in a single column does not necessarily guarantee the uniqueness of each observation or row.

While all values in the **ride_id** column were found to be unique, not all observations were truly distinct. To verify the uniqueness of each observation, additional columns such as **start_time**, **end_time**, **start_station**, and **end_station** were utilized. These columns provide more granular information, including the precise starting and ending times down to the second, as well as the starting and ending locations. It was assumed that observations with identical starting and ending date-times and stations, despite having different rider IDs, were potentially erroneous duplicates.
:::

::: {.tableScroller .p-2}
```{r}
#| label: tbl-duplicates
#| tbl-cap: Duplicates Table

gtDupes <- dupeTable |>
dplyr::group_by(started_at) |>
gt::gt(
rowname_col = "row",
groupname_col = "started_at",
row_group_as_column = TRUE
) |>
gt::tab_style(
style = list(
gt::cell_text(weight = "bold", align = "center"),
gt::cell_borders(sides = c("bottom"))
),
locations = gt::cells_column_labels(gt::everything())
) |>
gt::tab_style(
style = list(
gt::cell_borders(sides = c("left", "right"), color = "transparent"),
gt::cell_text(align = "center", v_align = "middle")
),
locations = gt::cells_body(gt::everything())
) |>
gt::data_color(
columns = start_station_name,
target_columns = gt::everything(),
method = "auto",
palette = "basetheme::brutal"
) |>
gt::tab_header(title = "A view of duplicated observations", subtitle = "Grouping follows the starting date-time value") |>
gt::tab_options(
heading.title.font.weight = "bolder",
heading.subtitle.font.weight = "lighter",
heading.align = "center",
table.background.color = "transparent",
table.font.color = "SeaShell",
table.font.size = gt::pct(75),
) |>
gt::tab_source_note(source_note = gt::md("**Source**: `db/data_complete.db`"))


gtDupes
```
:::

::: {.mt-3 .mb-3}
Although the cause of such duplication errors is unknown, it could be assumed that one person checked out multiple bikes simultaneously. In that scenario, each bike would be assigned a unique **ride_id**. However, this occurrence was relatively rare, happening only **18** times over the course of a year. Since there is only one duplicate for each instance, it raises concerns and warrants further investigation. It is possible that trips could be grouped where one person pays for another rider's fare. However, if that were the case, it raises the question of why there is always precisely one duplicate.

In @tbl-duplicates, duplicate observations are listed and grouped by color for visual clarity. In contrast, @tbl-unduplicated presents the data after removing the extra copy of each duplicate observation while preserving the unique observations. Of the duplicates identified, each had one extra copy. It was noted that the number of rows in the duplicates table is 36. Each duplicated observation has one duplicate, where **n** (the count) is always 2. Therefore, the expected number of observations to be removed was 18. A complication arose in determining how to remove not all observations but only the extra duplicate observation from each group.
:::

::: {.p-2 .tableScroller}
```{r}
#| label: tbl-unduplicated
#| tbl-cap: Un-duplicated Table

gt_undupes <- undupedTable |>
dplyr::collect() |>
dplyr::group_by(started_at) |>
gt::gt(
rowname_col = "row",
groupname_col = "started_at",
row_group_as_column = TRUE
) |>
gt::fmt_number(decimals = 0) |>
gt::tab_style(
style = list(
gt::cell_text(weight = "bold", align = "center"),
gt::cell_borders(sides = c("bottom"))
),
locations = gt::cells_column_labels(gt::everything())
) |>
gt::tab_style(
style = list(
gt::cell_borders(sides = c("left", "right")),
gt::cell_text(align = "center", v_align = "middle")
),
locations = gt::cells_body(gt::everything())
) |>
gt::data_color(
columns = start_station_name,
target_columns = gt::everything(),
method = "auto",
palette = "basetheme::brutal"
) |>
gt::tab_header(title = "After duplicates were removed", subtitle = "Same grouping") |>
gt::tab_options(
heading.title.font.weight = "bolder",
heading.subtitle.font.weight = "lighter",
heading.align = "center",
table.background.color = "transparent",
table.font.color = "SeaShell",
table.font.size = gt::pct(75)
) |>
gt::tab_source_note(source_note = gt::md("**Source**: `db/data_complete.db`"))

gt_undupes

```
:::

::: {.mb-2 .mt-2}
To ensure the accurate removal of duplicates, the count of distinct n-values (representing the number of occurrences) for the un-duplicated table was computed, confirming the expected 18 unique instances. Subsequently, the total number of observations in the dataset was recorded, initially standing at 4,331,707. After removing the identified duplicate observations, the correct count of observations was 4,331,689. In summary, 18 additional observations were successfully removed, aligning with the expected number of duplicates identified earlier. These steps are documented in @tbl-observationHistory for reference.

By carefully analyzing the count of distinct n-values and the total observation count before and after reduplication, it was ensured that only the precise number of duplicate observations was removed, preserving the integrity of the unique data while eliminating the identified duplicates. This meticulous approach to data cleaning is crucial for maintaining data quality and reliability throughout the analysis process.
:::

::: {.d-grid .gap-0}
::: {.btn .btn-outline-info .mt-4 .mb-3 type="button" data-bs-toggle="offcanvas" data-bs-target="#offcanvas2" aria-controls="offcanvas"}
Code used for handling duplicates in this section
:::
:::

## Outliers

::: {#offcanvas33 .offcanvas .offcanvas-start tabindex="-1" aria-labelledby="offcanvas" style="width: auto"}
::: offcanvas-header
::: {.h5 .offcanvas-title}
Transform and Filter the Database
:::

::: {.btn-close type="button" data-bs-dismiss="offcanvas" ariaLabel="Close"}
:::
:::

::: offcanvas-body
::: flex-code
```{r}
#| label: filterDecisions
#| code-summary: If you happen to be re-using this code - this is so you do not have to re-download or re-filter after making further adjustments.
#| tidy: true

filtered_path <- "db/filtered_data.db"

# Do we still need to filter the database?
if (duckdb::dbExistsTable(dbconn, filtered_path) == FALSE) {
source("Scripts/transFilter.R")
transFilter(conxn = dbconn, oldPath = "db/data_unduped.db", newPath = filtered_path)
}
```
:::

::: flex-code
```{r}
#| label: filterScript
#| code-summary: This would execute if the if-else conditions were met to filter the db/data.db database table
#| file: "Scripts/transFilter.R"
#| eval: false
```
:::
:::
:::

Observations deemed erroneous or irrelevant for identifying usage trends among members and casual users were filtered out. Keeping track of these errors is a good practice, as they might provide insights into the differences in how members and casuals utilize the service.

Trips with negative duration were flagged as errors and removed. Additionally, trips lasting less than a minute but greater than zero were noted and removed, as they could potentially skew the derived statistics. These extremely short trips might be attributed to users briefly trying out the service before committing or quickly realizing their dissatisfaction with it. While some observations seemed nonsensical, most of the data was retained.

Consistent with the previous approach, an **if-else** decision was employed to facilitate testing. An external database filtering script was utilized to streamline the code within the main Quarto document. The resulting filtered data served as the foundation for subsequent analysis and table generation.

::: {.p-2 .flex-code}
```{r}
#| label: countFiltered
#| code-summary: To get a count of the new total observations after filtering.
#| tidy: true

count_filtered <- dplyr::tbl(dbconn, filtered_path) |>
dplyr::select(ride_id) |>
dplyr::distinct() |>
dplyr::count() |>
dplyr::collect() |>
as.integer()

```
:::

```{r}
#| label: tbl-observationHistory
#| tbl-cap: Observation Processing History

# To see the history of obs in our dataset.
summaryProcessTable <- tidyr::tribble(
~ " ",
~ "  ",
"Original   ",
original_nobs,
"Complete Observations   ",
count_incorrectDists,
"Duplicates   ",
(count_incorrectDists - count_correctDists),
"Filtered     ",
(count_correctDists - count_filtered),
"Total Corrected   ",
count_filtered
) |>
gt::gt(rownames_to_stub = FALSE) |>
gt::tab_header(title = "Tallying Observations", 
subtitle = gt::md("Noting the change<br>in observations")) |>
gt::tab_footnote(
footnote = gt::md("Row counts throughout the cleaning steps."),
locations = gt::cells_column_labels(columns = "  ")
) |>
gt::tab_style(
style = list(
gt::cell_borders(sides = "bottom"),
gt::cell_text(
align = "left",
stretch = "semi-expanded"
)
),
locations = gt::cells_body(gt::everything())
) |>
gt::tab_style(
gt::cell_text(
align = "center",
stretch = "semi-expanded"),
locations = list(
gt::cells_title(groups = c("title", "subtitle")),
gt::cells_column_labels(gt::everything())
)
) |>
gt::fmt_number(decimals = 0) |>
gt::tab_options(
column_labels.font.weight = "bold",
table.background.color = "transparent",
table.font.color = "SeaShell",
row.striping.background_color = "gray10",
row.striping.include_table_body = TRUE
) |>
gt::tab_source_note(source_note = gt::md("**Sources**: `db/data_original.db`, `db/data_complete.db`,<br>`db/data_unduped.db`, `db/data_filtered.db`"))

summaryProcessTable
```

::: {.d-grid .gap-0}
::: {.btn .btn-outline-success .mb-3 type="button" data-bs-toggle="offcanvas" data-bs-target="#offcanvas33" aria-controls="offcanvas"}
Outlier and additional transformations
:::
:::

# Exploratory Analysis

::: {#offcanvas8979 .offcanvas .offcanvas-start tabindex="-1" aria-labelledby="offcanvas" style="width: auto"}
::: offcanvas-header
::: {.h5 .offcanvas-title}
EDA Scripts
:::

::: {.btn-close type="button" data-bs-dismiss="offcanvas" ariaLabel="Close"}
:::
:::

::: offcanvas-body
::: flex-code
```{r}
#| label: tablerScript
#| code-summary: Reduces duplicate code and improves conistency in table formatting across this report.
#| file: "Scripts/tabler.R"
#| eval: false 
```
:::

::: flex-code
```{r}
#| label: plotterScript
#| code-summary: Provides a useful script for consistent formatting of many of the plots. 
#| file: "Scripts/plotter.R"
#| eval: false 
```
:::

::: flex-code
```{r}
#| label: transformDataScript
#| eval: false
#| file: "Scripts/transformData.R"
#| code-summary: Performs many of the more backend query transformations I needed for this document. 
```
:::

::: flex-code
```{r}
#| label: chisqTestScript
#| eval: false
#| file: "Scripts/chisqTest.R"
#| code-summary: Helpfully, this contributes to the Chi-Squared tables by allowing me to add the chi-squared statistic and degrees of freedom information in the footnotes.
```
:::
:::
:::

::: {.d-grid .gap-0}
::: {.btn .btn-outline-warning .mt-3 .mb-3 type="button" data-bs-toggle="offcanvas" data-bs-target="#offcanvas8979" aria-controls="offcanvas"}
Scripts used frequently in this section
:::
:::

## Membership {#sec-membership}

::: {.d-grid .gap-3}
::: {.callout-important .mb-3 icon="false"}
<!-- -->

-   @tbl-memberTotals and @fig-totalMemberFrequency gives an idea of the membership distribution.
:::

::: {.btn-group role="group" aria-label="third"}
::: {.btn .btn-dark type="button" data-bs-toggle="offcanvas" data-bs-target="#offcanvas771" aria-controls="offcanvas"}
Database Operations
:::

::: {.btn .btn-secondary type="button" data-bs-toggle="offcanvas" data-bs-target="#offcanvas772" aria-controls="offcanvas"}
Table Preview
:::
:::
:::

::: {#offcanvas771 .offcanvas .offcanvas-start tabindex="-1" aria-labelledby="offcanvas" style="width: auto"}
::: offcanvas-header
::: {.h5 .offcanvas-title}
Database Code
:::

::: {.btn-close type="button" data-bs-dismiss="offcanvas" ariaLabel="Close"}
:::
:::

::: offcanvas-body
::: flex-code
```{r}
#| label: writeMembership
#| code-summary: Write ... to db/... .db
#| tidy: true

if (isFALSE(duckdb::dbExistsTable(dbconn, "db/membership.db"))) {
dplyr::tbl(dbconn, filtered_path) |>
dplyr::select(member_casual) |>
dplyr::arrange(member_casual) |>
dplyr::collect() |>
duckdb::dbWriteTable(conn = dbconn,
name = "db/membership.db",
overwrite = TRUE)
}
```
:::
:::
:::

::: {#offcanvas772 .offcanvas .offcanvas-start tabindex="1" aria-labelledby="offcanvas" style="width: auto"}
::: offcanvas-header
::: {.h5 .offcanvas-title}
Tables
:::

::: {.btn-close type="button" data-bs-dismiss="offcanvas" ariaLabel="Close"}
:::
:::

::: offcanvas-body
::: {layout="[[1]]"}
```{r}
#| label: tbl-kableMembership
#| tbl-cap: Kable output
#| tidy: true

dplyr::tbl(dbconn, "db/membership.db") |>
dplyr::collect() |>
head() |>
kableExtra::kable()

```
:::
:::
:::

::: column-page-inset-right

::: panel-tabset
### [Overall Frequency Analysis]{.panel-tabset-label}

::: panel-tabset
#### [Frequency Plot]{.panel-tabset-label}

```{r}
#| label: fig-totalMemberFrequency
#| fig-cap: "Plotting the overall membership status distribution"
#| fig-cap-location: top
#| tidy: true
#| fig-dpi: 150

gplot <- 
transformData(
conn = dbconn, 
path = "db/membership.db", 
select_cols = "member_casual",
group_cols = "member_casual", 
doWeights = TRUE
) |>
plotter(
x_col = member_casual, 
y_col = n, 
geomType = "column", 
title = paste0("What does the overall\nmembership distribution look like?"),
subtitle = NULL,
x_label = "Rider Groups", 
y_label = "n")

gplot

```

#### [Frequency Table]{.panel-tabset-label}

```{r}
#| label: tbl-memberTotals
#| tbl-cap: "Tabularizing membership distribution across all observations"
#| tidy: true

transformData(
conn = dbconn, 
path = "db/membership.db", 
select_cols = "member_casual",
group_cols = "member_casual", 
doWeights = TRUE
) |>
tabler (
title = gt::md("Overall membership<br>distribution"),
note_list = list(
"membership status", "observation count"),
location_list = c("member_casual", "n"),
source_note = gt::md("**Source**: `db/membership.db`"),
noteRows = TRUE,
hide_column_labels = TRUE,
row_index = 1
)
#gt::cols_label(member_casual = " ", n = " ") |>
#gt::cols_align(columns = n, align = "right") |>
#gt::cols_align(columns = member_casual, align = "left")

```
:::
:::
:::

## Cycle Types {#sec-btypes}

::: {.d-grid .gap-3}
::: {.callout-important .tableScroller .mb-3 icon="false"}
<!-- -->

-   @tbl-btypeTotal and @fig-btypeTotal gives an idea of the *rideable_type* parameter's distribution.

-   @tbl-btypeGroups and @fig-btypeGroups summarizes the bicycle type frequencies by membership status.

-   @tbl-btypeChiSquare presents a Chi-Square analysis of bicycle type usage among casual users and members. There is a statistically significant association between bicycle type and membership status ($p < 0.001$).

    -   Members show a higher preference for classic bikes ($65 \%$) compared to casual users (59%). Casual users have a higher proportion of electric bike usage ($41 \%$) compared to members ($35 \%$).

    -   Both casual users and members prefer classic bikes over electric bikes.

    -   The very low p-value ($< 0.001$) indicates strong evidence against the null hypothesis of no association between bicycle type and membership status. This suggests that the choice of bicycle type is not independent of membership status.

    -   The large $\chi^2$ value ($14762.37$) with just 1 degree of freedom (calculated as $[rows - 1] * [columns - 1]$) results in the very small p-value ($< 0.001$). This combination strongly suggests that the difference in bike type preference between casual users and members is not due to random chance. However, with such a large sample size (nearly 4 million total users), even small differences can produce statistically significant results.

-   @tbl-btypeModel presents the results of a binary logistic regression analyzing the relationship between bicycle type and membership status. The analysis compares two types of bicycles: classic bikes and electric bikes, with classic bikes serving as the reference category.

    -   The odds of membership for users of electric bikes were $0.76$ times the odds for users of classic bikes. Statistical Significance: The difference in membership likelihood between electric and classic bike users is highly statistically significant ($p < 0.001$).
:::

::: {.btn-group role="group" aria-label="third"}
::: {.btn .btn-dark type="button" data-bs-toggle="offcanvas" data-bs-target="#offcanvas771" aria-controls="offcanvas"}
Database Operations
:::

::: {.btn .btn-secondary type="button" data-bs-toggle="offcanvas" data-bs-target="#offcanvas772" aria-controls="offcanvas"}
Table Preview
:::
:::
:::

::: {#offcanvas871 .offcanvas .offcanvas-start tabindex="-1" aria-labelledby="offcanvas" style="width: auto"}
::: offcanvas-header
::: {.h5 .offcanvas-title}
DB Operations
:::

::: {.btn-close type="button" data-bs-dismiss="offcanvas" ariaLabel="Close"}
:::
:::

::: offcanvas-body
::: flex-code
```{r}
#| label: write bType to duckdb
#| code-summary: Write bType.db to the database.
#| tidy: true

if (isFALSE(duckdb::dbExistsTable(dbconn, "db/bType.db"))) {
dplyr::tbl(dbconn, filtered_path) |>
dplyr::select(rideable_type, member_casual) |>
dplyr::arrange(rideable_type, member_casual) |>
dplyr::collect() |>
dplyr::mutate(rideable_type = forcats::as_factor(rideable_type)) |>
duckdb::dbWriteTable(conn = dbconn,
name = "db/bType.db",
overwrite = TRUE)
}
```
:::

::: flex-code
```{r}
#| label: btypeTransform
#| code-summary: Transform and write as a weighted binary table for modeling.
#| tidy: true

if (isFALSE(duckdb::dbExistsTable(dbconn, "db/bType_wb.db"))) {
transformData(
conn = dbconn,
path = "db/bType.db",
select_cols = c("rideable_type", "member_casual"),
group_cols = c("rideable_type", "member_casual"),
binary_col = "member_casual",
zero_val = "casual",
one_val = "member",
doWeights = TRUE
) |>
duckdb::dbWriteTable(conn = dbconn,
name = "db/bType_wb.db",
overwrite = TRUE)
}

```
:::
:::
:::

::: {#offcanvas872 .offcanvas .offcanvas-start tabindex="1" aria-labelledby="offcanvas" style="width: auto"}
::: offcanvas-header
::: {.h5 .offcanvas-title}
Tables
:::

::: {.btn-close type="button" data-bs-dismiss="offcanvas" ariaLabel="Close"}
:::
:::

::: offcanvas-body
::: {layout="[[1,2]]"}
```{r}
#| label: tbl-kableBtype
#| tbl-cap: Kable output
#| tidy: true

dplyr::tbl(dbconn, "db/bType.db") |>
dplyr::collect() |>
head() |>
kableExtra::kable()

```

```{r}
#| label: tbl-kableBtypeW
#| tbl-cap: Kable output
#| tidy: true

dplyr::tbl(dbconn, "db/bType_wb.db") |>
dplyr::collect() |>
head() |>
kableExtra::kable()

```
:::
:::
:::

::: panel-tabset
### [Overall Frequency Analysis]{.panel-tabset-label}

::: panel-tabset
#### Frequency [Plot]{.panel-tabset-label}

```{r}
#| label: fig-btypeTotal
#| fig-cap: Bicycle type frequencies 
#| tidy: true
#| fig-dpi: 150

gplot <- 
transformData(
conn = dbconn, 
path = "db/bType.db", 
select_cols = "rideable_type",
group_cols = "rideable_type", 
doWeights = TRUE
) |>
plotter(
x_col = rideable_type, 
y_col = n, 
geomType = "column", 
title = "Bicycle Type", 
x_label = "Type", 
y_label = "n")

gplot

```

#### [Frequency Table]{.panel-tabset-label}

```{r}
#| label: tbl-btypeTotal
#| tbl-cap: Cycle Type Total Frequency
#| tidy: true

transformData(
conn = dbconn, 
path = "db/bType.db", 
select_cols = "rideable_type",
group_cols = "rideable_type", 
doWeights = TRUE
) |>
tabler( 
title = "Bicycle Types", 
note_list = list("bicycle type","observation count"),
location_list = list("rideable_type", "n"),
source_note = gt::md("**Source**: `db/bType.db`"),
noteColumns = TRUE
) |>
gt::cols_label(rideable_type = "Type") |>
gt::cols_align(columns = n, align = "right") |>
gt::cols_align(columns = rideable_type, align = "left") 

```
:::

### [Comparative Frequency Analysis]{.panel-tabset-label}

::: panel-tabset
#### [Frequency Plot]{.panel-tabset-label}

```{r}
#| label: fig-btypeGroups
#| fig-cap: Bicycle type membership frequencies
#| tidy: true
#| fig-dpi: 150

gplot <- 
transformData(
conn = dbconn, 
path = "db/bType.db", 
select_cols = c("rideable_type", "member_casual"),
group_cols = c("rideable_type", "member_casual"),
doWeights = TRUE
) |>
plotter(
title = "Bicycle Groups",
x_label = "Type",
y_label = "n",
x_col = rideable_type, 
y_col = n, 
group_col = member_casual,
geomType = "column", 
is_colGroup = TRUE,
color_col = "black",
colPosition = "dodge",
colGroup_palette = "Paired"
)

gplot

```

#### [Frequency Table]{.panel-tabset-label}

```{r}
#| label: tbl-btypeGroups
#| tbl-cap: Bicycle type membership frequencies
#| tidy: true

location_list <- list("member_casual", "n")

note_list <- list(
"membership status",
"observation count"
)

transformData(
conn = dbconn, 
path = "db/bType.db", 
select_cols = c("rideable_type", "member_casual"),
group_cols = c("rideable_type", "member_casual"),
doWeights = TRUE
) |>
tabler(
title = gt::md("Membership frequencies<br>by bicycle type?"), 
groupName = "rideable_type", 
location = n,
label_n = "n",
note_list = note_list,
location_list = location_list,
source_note = gt::md("**Source**: `db/bType.db`"),
noteColumns = TRUE,
isStub = TRUE,
stub_label = " ",
stub_note = "bicycle type"
) |>       
gt::cols_label(member_casual = " ", rideable_type = "Bicycle Type", n = " ") |>
gt::cols_align(columns = n, align = "right") |>
gt::cols_align(columns = member_casual, align = "left"
) 

```

#### [Chi-Squared]{.panel-tabset-label}

```{r}
#| label: chisquareScriptCycles
#| code-summary: The script I used to process the Chi-Squared test result. This code is reposted with all the chi-square tables in this report.  
#| file: "Scripts/chisqTest.R"
#| eval: false 
```

```{r}
#| label: bikesChiResult
#| code-summary: Save the chi-square statistic and degrees of freedom values in a tibble format to add to the gtsummary table.
#| tidy: true

data_tibble <- 
dplyr::tbl(dbconn, "db/bType.db") |>
dplyr::select(rideable_type, member_casual) |>
dplyr::collect()

chiResult <- chisqTest(data = data_tibble, variable = "rideable_type", by = "member_casual")

```

```{r}
#| label: tbl-btypeChiSquare
#| tbl-cap: Bicycle type membership distributions
#| tidy: true

chi_table <- tabler(
title = "Chi-Squared: Bicycle Type",
source_note = gt::md("**Source**: `db/bType.db`"),
label = list(
rideable_type = "Bicycle Type"),
by = member_casual,
isSummary = TRUE,
chiVar = "rideable_type",
chiBy = "member_casual",
tbl_name = data_tibble,
chi_result = chiResult
) 

chi_table

```

#### [Binary Logistic Regression]{.panel-tabset-label}

```{r}
#| label: btypeModel
#| code-summary: Predicting the log-odds of being a member versus being a casual user based on ...
#| tidy: true

model <- 
dplyr::tbl(dbconn, "db/bType_wb.db") |>
glm(formula = member_casual ~ rideable_type, weights = n, family = binomial)

```

```{r}
#| label: tbl-btypeModel
#| tidy: true

regression_tbl <- model |>
gtsummary::tbl_regression(
label = list(rideable_type = "Cycle Type"), conf.int = FALSE, exponentiate = TRUE)

regression_tbl |>
tabler(
title = gt::md("Binary Logistic Regression: <br> Bicycle Type"),
source_note = gt::md("**Source**: `db/bType_wb.db`"),
isBinary = TRUE)

```
:::
:::

## Duration {#sec-duration}

::: {.d-grid .gap-3}
::: {.callout-important .tableScroller .mb-3 icon="false" background-color="Black"}
<!-- -->

-   @tbl-triptimeTotals and @fig-triptimeTotals give an idea of the duration distribution.

-   @tbl-triptimeCompare and @fig-triptimeCompare summarize the duration distribution by membership.

-   @tbl-durationSummary gives the reader some idea of the variability, range, and quartile information about the duration data.

-   @fig-durationDensity shows a density plot comparing the duration of trips for two groups: "casual" users and "members". The x-axis represents time in minutes, ranging from 0 to 100, while the y-axis shows the density (a measure of relative frequency).

    -   Both groups exhibit right-skewed distributions, with a peak near the left side and a long tail extending to the right. This suggests that for both groups, shorter durations are more common, while longer durations occur less frequently but can extend quite far.

    -   The "member" group (darker blue) has a higher and narrower peak compared to the "casual" group (lighter blue). This indicates that members tend to have a more concentrated distribution of session durations around their most common length.

    -   The "casual" group appears to have a slightly fatter tail, extending further to the right than the "member" group. This suggests that casual users might occasionally have longer sessions than members, even if it's less common.

    -   Both groups seem to have their peak density around 5-10 minutes, with members peaking slightly earlier than casual users.

    -   The total area under each curve represents the entire population for that group. The curves appear to have similar total areas, suggesting the sample sizes might be comparable.

    -   There's significant overlap between the two distributions, indicating that while there are differences, there's also considerable similarity in duration patterns between casual users and members.

-   @tbl-modeldurationQ presents the results of a binary logistic regression analyzing the relationship between ride duration and membership status. The analysis divides ride durations into four quartiles, with Q1 (1.02 - 5.73 minutes) serving as the reference category.

    -   Compared to Q1, the odds of being a member versus a casual rider varied significantly across the other duration quartiles (p \< 0.001 for all comparisons).

    -   In Q2 (5.73 - 9.55 minutes), the odds of membership were 0.38 times as high as in Q1. This indicates a substantial decrease (62%) in the likelihood of membership for slightly longer rides.

    -   In Q3 (9.55 - 16.13 minutes), the odds of membership were 0.08 times as high as in Q1. This shows a dramatic decrease (92%) in membership likelihood for medium-length rides.

    -   In Q4 (16.13 - 475.22 minutes), the odds of membership were 0.06 times as high as in Q1. This represents an even more pronounced decrease (94%) in membership likelihood for the longest rides.

-   To visualize the full duration dataset as a histogram with illustrated, colored-coded quartiles as dotted lines, see @fig-durationHistogram (the solid yellow line represents the mean).
:::

::: {.btn-group role="group" aria-label="third"}
::: {.btn .btn-dark type="button" data-bs-toggle="offcanvas" data-bs-target="#offcanvas671" aria-controls="offcanvas"}
Database Operations
:::

::: {.btn .btn-secondary type="button" data-bs-toggle="offcanvas" data-bs-target="#offcanvas672" aria-controls="offcanvas"}
Table Preview
:::
:::
:::

::: {#offcanvas671 .offcanvas .offcanvas-start tabindex="-1" aria-labelledby="offcanvas" style="width: auto"}
::: offcanvas-header
::: {.h5 .offcanvas-title}
DB Operations
:::

::: {.btn-close type="button" data-bs-dismiss="offcanvas" ariaLabel="Close"}
:::
:::

::: offcanvas-body
::: flex-code
```{r}
#| label: write duration to duckdb
#| code-summary: Write ... to the database.
#| tidy: true

if (isFALSE(duckdb::dbExistsTable(dbconn, "db/duration.db"))) {
dplyr::tbl(dbconn, filtered_path) |>
dplyr::select(trip_time, member_casual) |>
dplyr::arrange(trip_time, member_casual) |>
dplyr::collect() |>
dplyr::mutate(
trip_time = round(trip_time, digits = 2),
mins = round(trip_time, digits = 0),
mins = forcats::as_factor(mins)
) |>
dplyr::arrange(trip_time, member_casual) |>
duckdb::dbWriteTable(conn = dbconn,
name = "db/duration.db",
overwrite = TRUE)
}

```
:::

::: flex-code
```{r}
#| label: durationWeightedQuantiles
#| code-summary: Query ... .db, transform and write weighted quartile data to ... _wq.db. 
#| tidy: true

if (isFALSE(duckdb::dbExistsTable(dbconn, "db/duration_wq.db"))) {
transformData(
conn = dbconn,
path = "db/duration.db",
select_cols = c("trip_time", "member_casual"),
group_cols = c("trip_time", "member_casual"),
binary_col = "member_casual",
pred_col = "trip_time",
ntile_col = "quartile",
zero_val = "casual",
one_val = "member",
qtile_levels = c(
"Q1 (1.02 - 5.73]",
"Q2 (5.73 - 9.55]",
"Q3 (9.55 - 16.13]",
"Q4 (16.13 - 475.22]"
),
doQuantile = TRUE,
doWeights = TRUE
) |>
duckdb::dbWriteTable(conn = dbconn,
name = "db/duration_wq.db",
overwrite = TRUE)
}

```
:::
:::
:::

::: {#offcanvas672 .offcanvas .offcanvas-start tabindex="1" aria-labelledby="offcanvas" style="width: auto"}
::: offcanvas-header
::: {.h5 .offcanvas-title}
Tables
:::

::: {.btn-close type="button" data-bs-dismiss="offcanvas" ariaLabel="Close"}
:::
:::

::: offcanvas-body
::: {layout="[[1,2]]"}
```{r}
#| label: tbl-kableDuration
#| tbl-cap: Kable output
#| tidy: true

dplyr::tbl(dbconn, "db/duration.db") |>
dplyr::collect() |>
head() |>
kableExtra::kable()

```

```{r}
#| label: tbl-kableDurationWQ
#| tbl-cap: Kable output
#| tidy: true

dplyr::tbl(dbconn, "db/duration_wq.db") |>
dplyr::collect() |>
head() |>
kableExtra::kable()

```
:::
:::
:::

::: panel-tabset
### [Overall Frequency Analysis]{.panel-tabset-label}

::: panel-tabset
#### [Frequency Plot]{.panel-tabset-label}

```{r}
#| label: fig-triptimeTotals
#| fig-cap: Trip Duration Totals
#| fig-dpi: 150
#| tidy: true

gplot <- 
transformData(
conn = dbconn, 
path = "db/duration.db", 
select_cols = "mins",
group_cols = "mins", 
doWeights = TRUE
) |>
plotter(
x_col = as.integer(mins), 
y_col = n,
geomType = "column", 
title = "Duration", 
x_label = "Minutes", 
y_label = "n",
color_col = "black") +
ggplot2::scale_x_continuous(
limits = c(0, 60),
breaks = seq(0, 60, by = 5),
guide = ggplot2::guide_axis(n.dodge = 1, angle = 45)
)

gplot +
ggplot2::theme(
axis.text = ggplot2::element_text(size = 8)
)


```

#### [Frequency Table]{.panel-tabset-label}

::: tableScroller
```{r}
#| label: tbl-triptimeTotals
#| tbl-cap: Trip-Time Totals
#| tidy: true

transformData(
conn = dbconn, 
path = "db/duration.db", 
select_cols = "mins",
group_cols = "mins", 
doWeights = TRUE
) |>
tabler( 
title = "Duration", 
note_list = list("trip duration", "observation count"),
location_list = list("mins", "n"),
source_note = gt::md("**Source**: `db/duration.db`"),
noteColumns = TRUE
) |>
gt::cols_label(mins = "Minutes") |>
gt::cols_align(columns = n, align = "right") |>
gt::cols_align(columns = mins, align = "center") 

```
:::
:::

### [Comparative Frequency Analysis]{.panel-tabset-label}

::: panel-tabset
#### [Frequency Plot]{.panel-tabset-label}

```{r}
#| label: fig-triptimeCompare
#| fig-cap: Trip-Time Group Frequency
#| fig-dpi: 150
#| tidy: true

gplot <- 
dplyr::tbl(dbconn, "db/duration.db") |>
dplyr::select(mins, member_casual) |>
dplyr::filter(as.integer(mins) <= 100) |>
dplyr::collect() |>
transformData(
conn = NULL, 
path = NULL,
select_cols = c("mins", "member_casual"),
group_cols = c("mins", "member_casual"), 
doWeights = TRUE,
isDF = TRUE
) |>
plotter(
title = "Duration Groups",
x_label = "Minutes",
y_label = "n",
x_col = mins, 
y_col = n, 
group_col = member_casual,
geomType = "column",
is_colGroup = TRUE,
colPosition = ggplot2::position_stack(reverse = TRUE),
color_col = "black"
) +
ggplot2::scale_x_discrete(
guide = ggplot2::guide_axis(n.dodge = 1, angle = 45), 
limits = forcats::as_factor(seq(0, 60)),
breaks = forcats::as_factor(seq(0, 60, by = 5)))

gplot +
ggplot2::theme(
axis.text = ggplot2::element_text(size = 8)
)

```

#### [Frequency Table]{.panel-tabset-label}

::: tableScroller
```{r}
#| label: tbl-triptimeCompare
#| tbl-cap: Trip Time Comparison
#| tidy: true

transformData(
conn = dbconn, 
path = "db/duration.db", 
select_cols = c("mins", "member_casual"),
group_cols = c("mins", "member_casual"), 
doWeights = TRUE
) |>
tabler(
title = "Duration - Membership", 
groupName = "mins", 
location = n,
label_n = "n",
note_list = list("membership status","observation count"),
location_list = list("member_casual", "n"),
source_note = gt::md("**Source**: `db/duration.db`"),
noteColumns = TRUE,
isStub = TRUE,
stub_label = "Trip Time",
stub_note = "trip duration (minutes)"
) |>
gt::cols_label(member_casual = "Membership") |>
gt::cols_align(columns = n, align = "right") |>
gt::cols_align(columns = member_casual, align = "left") |>
gt::cols_align(columns = "mins", align = "center")


```
:::

#### [Summary Stats]{.panel-tabset-label}

```{r}
#| label: tbl-durationSummary
#| tbl-cap: "Useful summary statistics."

gtTable <- dplyr::tbl(dbconn, "db/duration.db") |>
dplyr::select(mins, member_casual) |>
dplyr::mutate(
mins = as.numeric(mins)
) |>
dplyr::collect() |>
gtsummary::tbl_summary(
by = member_casual,
type = mins ~ "continuous2",
label = list(mins ~ "Duration (mins)"),
digits = list(
mins ~ c(1, 1)),
statistic = 
gtsummary::all_continuous() ~ c(
"{median} ({p25}, {p75})", 
"{mean} ({sd})", 
"{min}, {max}")
) |>
gtsummary::italicize_levels() |>
tabler(
title = gt::md("Summary Statistics:<br>Duration - Membership"),
source_note = gt::md("**Source**: `db/duration.db`"),
isBinary = TRUE
)

gtTable

```

#### [Density]{.panel-tabset-label}

```{r}
#| label: fig-durationDensity
#| code-summary: Density plot for duration by membership.
#| tidy: true
#| fig-dpi: 150

gplot <- 
transformData(
conn = dbconn, 
path = "db/duration.db",
select_cols = c("trip_time", "member_casual")) |>
plotter(
title = "Duration Group Density",
x_label = paste0("Minutes"),
x_col = trip_time, 
group_col = member_casual,
geomType = "column",
angle = 45,
color_col = "black",
density_alpha = 0.75,
isDensity = TRUE,
is_colGroup = TRUE,
breaks = seq(0, 100, by = 5),
limits = c(0, 100)
)

gplot
```

#### [Binary Logistic Regression]{.panel-tabset-label}

```{r}
#| label: modeldurationQ
#| code-summary: Query ..._wq.db, process and create model R object for hour based on quartile range.
#| tidy: true

model <- 
dplyr::tbl(dbconn, "db/duration_wq.db") |>
dplyr::collect() |> 
glm(
formula = member_casual ~ quartile, 
family = binomial,
weights = n)
```

```{r}
#| label: tbl-modeldurationQ
#| code-summary: Pipe model object to tbl_regression(), then further adjust output with tabler().  
#| tidy: true

model |>
gtsummary::tbl_regression(
label = list(quartile = "Duration Ranges"), 
conf.int = FALSE, 
exponentiate = TRUE) |>
tabler(
title = gt::md("Binary Logistic Regression: <br> Duration & Membership"),
source_note = gt::md("**Source**: `db/duration_wq.db`"),
isBinary = TRUE)
```

#### [Histogram Plot]{.panel-tabset-label}

```{r}
#| label: durationQuantile
#| code-summary: Create a data frame, then extract the desired quartile info to supplement histogram visualization for ... data.
#| tidy: true

qdf <- dplyr::tbl(dbconn, "db/duration.db") |>
dplyr::select(trip_time) |>
dplyr::collect()

quartiles <- quantile(qdf$trip_time, probs = c(0.25, 0.5, 0.75))
```

```{r}
#| label: fig-durationHistogram
#| tidy: true
#| fig-dpi: 150

gplot <- 
qdf |>
plotter(
title = "Duration",
x_label = "Duration",
y_label = "n",
x_col = trip_time, 
geomType = "column", 
isHistogram = TRUE,
angle = 45,
color_col = "transparent",
vline_color = "lightyellow",
vline_size = 0.5,
low = "blue",
high = "red",
limits = c(0,100),
breaks = seq(0, 100, by = 5),
binwidth = \(x) 2 * IQR(x) / (length(x)^(1/3)),
quartiles = quartiles
)

gplot
```
:::
:::

## Month {#sec-moy}

::: {.d-grid .gap-3}
::: {.callout-important .tableScroller .mb-3 icon="false"}
<!-- -->

-   @tbl-monthTotals and @fig-monthTotals give an aggregated distribution of the monthly ridership frequencies.

-   @tbl-monthCompare and @fig-monthCompare summarize the bicycle type frequencies by membership status.

-   In @tbl-chiMonths the null hypothesis is rejected. The type of bicycle and the rider's membership status can be considered dependent variables.

-   To visualize monthly users through the lens of their respective concentrations, see @fig-monthDensity. The plot looks a little different because I am directly plotting the x-axis using the original date-time data types. I think this maintains a more accurate view of the data.

-   @tbl-modelMonthsQ presents the results of a binary logistic regression analyzing the relationship between months of the year and membership status. The analysis divides the year into four quartiles, with Q1 (January 01 - May 20) serving as the reference category. Compared to Q1, the odds of being a member versus a casual rider varied significantly across the other time quartiles (p \< 0.001 for all comparisons).

    -   In Q2 (May 20 - Jul 21), the odds of membership were 0.57 times as high as in Q1. This indicates a substantial decrease (43%) in the likelihood of membership during late spring and early summer.

    -   In Q3 (Jul 21 - Sep 18), the odds of membership were 0.58 times as high as in Q1. This shows a similar decrease (42%) in membership likelihood during late summer and early fall, nearly identical to Q2.

    -   In Q4 (Sep 18 - Dec 31), the odds of membership were 0.87 times as high as in Q1. While still lower than Q1, this represents a less pronounced decrease (13%) in membership likelihood during fall and early winter.

-   To visualize the full monthly dataset as a histogram with illustrated, colored-coded quartiles as dotted lines, see @fig-monthHistogram (the solid yellow line represents the mean).
:::

::: {.btn-group role="group" aria-label="third"}
::: {.btn .btn-dark type="button" data-bs-toggle="offcanvas" data-bs-target="#offcanvas571" aria-controls="offcanvas"}
Database Operations
:::

::: {.btn .btn-secondary type="button" data-bs-toggle="offcanvas" data-bs-target="#offcanvas572" aria-controls="offcanvas"}
Table Preview
:::
:::
:::

::: {#offcanvas571 .offcanvas .offcanvas-start tabindex="-1" aria-labelledby="offcanvas" style="width: auto"}
::: offcanvas-header
::: {.h5 .offcanvas-title}
DB Operations
:::

::: {.btn-close type="button" data-bs-dismiss="offcanvas" ariaLabel="Close"}
:::
:::

::: offcanvas-body
::: flex-code
```{r}
#| label: writeMonths
#| code-summary: Write moy.db to the database.
#| tidy: true

if (isFALSE(duckdb::dbExistsTable(dbconn, "db/moy.db"))) {
dplyr::tbl(dbconn, filtered_path) |>
dplyr::select(started_at, member_casual) |>
dplyr::arrange(started_at) |>
dplyr::collect() |>
dplyr::mutate(
member_casual = factor(member_casual, levels = c("casual", "member")),
abbMonths = lubridate::month(started_at, label = TRUE, abbr = TRUE),
abbMonths = forcats::as_factor(abbMonths)
) |>
duckdb::dbWriteTable(conn = dbconn,
name = "db/moy.db",
overwrite = TRUE)
}

```
:::

::: flex-code
```{r}
#| label: monthsWeightedQuantiles
#| code-summary: Query ..., transform, and write weighted quartile data to ..._wq.db.
#| tidy: true

if (isFALSE(duckdb::dbExistsTable(dbconn, "db/moy_wq.db"))) {
transformData(
conn = dbconn,
path = "db/moy.db",
select_cols = c("started_at", "member_casual"),
group_cols = c("started_at", "member_casual"),
binary_col = "member_casual",
pred_col = "started_at",
ntile_col = "quartile",
zero_val = "casual",
one_val = "member",
qtile_levels = c(
"Q1 (Jan 01 - May 20]",
"Q2 (May 20 - Jul 21]",
"Q3 (Jul 21 - Sep 18]",
"Q4 (Sep 18 - Dec 31]"
),
doQuantile = TRUE,
doWeights = TRUE
) |>
duckdb::dbWriteTable(conn = dbconn,
name = "db/moy_wq.db",
overwrite = TRUE)
}

```
:::
:::
:::

::: {#offcanvas572 .offcanvas .offcanvas-start tabindex="1" aria-labelledby="offcanvas" style="width: auto"}
::: offcanvas-header
::: {.h5 .offcanvas-title}
Tables
:::

::: {.btn-close type="button" data-bs-dismiss="offcanvas" ariaLabel="Close"}
:::
:::

::: offcanvas-body
::: {layout="[[1,2]]"}
```{r}
#| label: tbl-kableMoy
#| tbl-cap: Kable output for months of the year
#| tidy: true

dplyr::tbl(dbconn, "db/moy.db") |>
dplyr::collect() |>
head() |>
kableExtra::kable()

```

```{r}
#| label: tbl-kableMoyW
#| tbl-cap: Kable output for weighted months of the year.
#| tidy: true

dplyr::tbl(dbconn, "db/moy_wq.db") |>
dplyr::collect() |>
head() |>
kableExtra::kable()

```
:::
:::
:::

::: panel-tabset
### [Overall Frequency Analysis]{.panel-tabset-label}

::: panel-tabset
#### [Frequency Plot]{.panel-tabset-label}

```{r}
#| label: fig-monthTotals
#| fig-cap: Month Total Frequency
#| fig-dpi: 150
#| tidy: true

gplot <- transformData(
conn = dbconn, 
path = "db/moy.db", 
select_cols = "abbMonths",
group_cols = "abbMonths", 
doWeights = TRUE
) |>
plotter(
x_col = abbMonths, 
y_col = n, 
geomType = "column", 
title = "Months", 
x_label = " ", 
y_label = " ")

gplot
```

#### [Frequency Table]{.panel-tabset-label}

```{r}
#| label: tbl-monthTotals
#| tbl-cap: Month Total Frequency
#| tidy: true

transformData(
conn = dbconn, 
path = "db/moy.db", 
select_cols = "abbMonths",
group_cols = "abbMonths", 
doWeights = TRUE
) |>
tabler(
title = "Months",
note_list = list("month of year", "observation count"),
location_list = c("abbMonths", "n"),
source_note = gt::md("**Source**: `db/moy.db`"),
noteColumns = TRUE
) |>
gt::cols_label(abbMonths = " ", n = " ") |>
gt::cols_align(columns = n, align = "right") |>
gt::cols_align(columns = abbMonths, align = "left") 

```
:::

### [Comparative Frequency Analysis]{.panel-tabset-label}

::: panel-tabset
#### [Frequency Plot]{.panel-tabset-label}

```{r}
#| label: fig-monthCompare
#| fig-cap: Month Group Frequency
#| fig-dpi: 150
#| tidy: true

gplot <- 
transformData(
conn = dbconn, 
path = "db/moy.db", 
select_cols = c("abbMonths", "member_casual"),
group_cols = c("abbMonths", "member_casual"), 
doWeights = TRUE
) |>
plotter(
title = "Month Groups",
x_label = "Months",
y_label = "n",
x_col = abbMonths, 
y_col = n, 
group_col = member_casual,
geomType = "column", 
isFaceted = TRUE,
is_colGroup = TRUE
)

gplot
```

#### [Frequency Table]{.panel-tabset-label}

::: tableScroller
```{r}
#| label: tbl-monthCompare
#| tbl-cap: Month Group Frequency
#| tidy: true

transformData(
conn = dbconn, 
path = "db/moy.db", 
select_cols = c("abbMonths", "member_casual"),
group_cols = c("abbMonths", "member_casual"), 
doWeights = TRUE
) |>
tabler(
title = gt::md("How many members<br>grouped by month?"), 
groupName = "abbMonths", 
location = n,
label_n = "n",
note_list = list("membership status", "observation count"),
location_list = list("member_casual", "n"),
source_note = gt::md("**Source**: `db/moy.db`"),
noteColumns = TRUE,
isStub = TRUE,
stub_label = " ",
stub_note = "months of the year (abbreviated)"
) |>
gt::cols_label(member_casual = "Membership") |>
gt::cols_align(columns = n, align = "right") |>
gt::cols_align(columns = member_casual, align = "left") 

```
:::

#### [Chi-Squared]{.panel-tabset-label}

```{r}
#| label: chisquareScriptMonths
#| code-summary: The script I used to process the Chi-Squared test result. This code is reposted with all the chi-square tables in this report.  
#| file: "Scripts/chisqTest.R"
#| eval: false 
```

```{r}
#| label: monthsChiResult
#| code-summary: Save the chi-square statistic and degrees of freedom values in a tibble format to add to the gtsummary table.
#| tidy: true

data_tibble <- 
dplyr::tbl(dbconn, "db/moy.db") |>
dplyr::select(abbMonths, member_casual) |>
dplyr::arrange(abbMonths, member_casual) |>
dplyr::collect()

chiResult <- chisqTest(data = data_tibble, variable = "abbMonths", by = "member_casual")

```

```{r}
#| label: tbl-chiMonths
#| tidy: true

chi_table <- tabler(
title = "Chi-Square: Month",
source_note = gt::md("**Source**: `db/moy.db`"),
label = list(abbMonths = "Month"),
by = member_casual,
isSummary = TRUE,
tbl_name = data_tibble,
chi_result = chiResult
)

chi_table
```

#### [Density]{.panel-tabset-label}

```{r}
#| label: fig-monthDensity
#| fig-dpi: 150
#| tidy: true

gplot <- 
dplyr::tbl(dbconn, "db/moy.db") |>
dplyr::collect() |>
plotter(
title = "Month Group Density",
x_label = paste0("Months"),
x_col = started_at, 
group_col = member_casual,
geomType = "other",
angle = 45,
color_col = "black",
density_alpha = 0.75,
isTime = TRUE,
date_breaks = "1 month",
date_labels = "%b",
)

gplot
```

#### [Binary Logistic Regression]{.panel-tabset-label}

```{r}
#| label: modelMonthsQ
#| code-summary: Query ..._wq.db, process and create model R object for hour based on quartile range. 
#| tidy: true

model <- 
dplyr::tbl(dbconn, "db/moy_wq.db") |>
dplyr::collect() |> 
glm(
formula = member_casual ~ quartile, 
family = binomial,
weights = n)
```

```{r}
#| label: tbl-modelMonthsQ
#| code-summary: Pipe model object to tbl_regression(), then further adjust output with tabler().  
#| tidy: true

model |>
gtsummary::tbl_regression(
label = list(quartile = "Months Ranges"), 
conf.int = FALSE, 
exponentiate = TRUE) |>
tabler(
title = gt::md("Binary Logistic Regression: <br> Months & Membership"),
source_note = gt::md("**Source**: `db/moy.db`"),
isBinary = TRUE)
```

#### [Histogram Plot]{.panel-tabset-label}

```{r}
#| label: fig-monthHistogram
#| fig-dpi: 150
#| tidy: true

qdf <- dplyr::tbl(dbconn, "db/moy.db") |>
dplyr::select(started_at) |>
dplyr::collect()

quartiles <- quantile(qdf$started_at, probs = c(0.25, 0.5, 0.75))

gplot <- dplyr::tbl(dbconn, "db/moy.db") |>
dplyr::select(started_at) |>
dplyr::collect() |>
plotter(
title = "Months",
x_label = "Months",
y_label = "n",
x_col = started_at, 
geomType = "column", 
isHistogram = TRUE,
isTimeHist = TRUE,
date_breaks = "1 month", 
date_labels = "%b", 
angle = 45,
color_col = "black",
vline_color = "lightyellow",
vline_size = 0.5,
low = "blue",
high = "red",
binwidth = \(x) 2 * IQR(x) / (length(x)^(1/3)),
quartiles = quartiles,
qformat = "%b-%d"
)

gplot
```
:::
:::

## Day {#sec-dow}

::: {.d-grid .gap-3}
::: {.callout-important .tableScroller .mb-3 icon="false"}
<!-- -->

-   @tbl-wkdayTotals and @fig-wkdayTotals give an aggregated distribution of ridership frequency by the day of the week.

-   @tbl-wkdayCompare and @fig-wkdayCompare summarize the duration distribution by membership.

-   In @tbl-chiDays, the null hypothesis is rejected. The type of bicycle and the rider's membership status can be considered dependent variables.

-   To visualize weekly users through the lens of their respective concentrations, see @fig-dayDensity. The plot looks a little different because I am directly plotting the x-axis using the original date-time data types. I think this maintains a more accurate view of the data.

-   @tbl-modelDaysQ presents the results of a binary logistic regression analyzing the relationship between days of the week and membership status. The analysis divides the week into four quartiles, with Q1 (Sunday 12:00 am - Monday 11:40 am) serving as the reference category.

    -   Compared to Q1, the odds of being a member versus a casual rider varied significantly across the other time quartiles (p \< 0.001 for all comparisons).

    -   Q2 (Monday 11:40 am - Wednesday 05:14 am): The odds of membership were 1.52 times higher than in Q1. This suggests a substantial increase in the likelihood of members riding during the early part of the work week.

    -   Q3 (Wednesday 05:14 am - Friday 12:19 pm), the odds of membership were 1.36 times higher than in Q1. This indicates a continued higher likelihood of membership during the latter part of the work week, though slightly lower than Q2.

    -   Q4 (Friday 12:19 pm - Saturday 11:59 pm), the odds of membership were 0.80 times as high as in Q1. This represents a significant decrease in the likelihood of membership during the weekend period.

-   To visualize the full weekly dataset as a histogram with illustrated, colored-coded quartiles as dotted lines, see @fig-Hdays (the solid yellow line represents the mean).
:::

::: {.btn-group role="group" aria-label="third"}
::: {.btn .btn-dark type="button" data-bs-toggle="offcanvas" data-bs-target="#offcanvas471" aria-controls="offcanvas"}
Database Operations
:::

::: {.btn .btn-secondary type="button" data-bs-toggle="offcanvas" data-bs-target="#offcanvas472" aria-controls="offcanvas"}
Table Preview
:::
:::
:::

::: {#offcanvas471 .offcanvas .offcanvas-start tabindex="-1" aria-labelledby="offcanvas" style="width: auto"}
::: offcanvas-header
::: {.h5 .offcanvas-title}
DB Operations
:::

::: {.btn-close type="button" data-bs-dismiss="offcanvas" ariaLabel="Close"}
:::
:::

::: offcanvas-body
::: flex-code
```{r}
#| label: writeDow
#| code-summary: Write dow.db to the database.
#| tidy: true

if (isFALSE(duckdb::dbExistsTable(dbconn, "db/dow.db"))) {
dplyr::tbl(dbconn, filtered_path) |>
dplyr::select(started_at, member_casual) |>
dplyr::arrange(started_at) |>
dplyr::collect() |>
dplyr::mutate(
wkdays = lubridate::wday(started_at, week_start = 7),
member_casual = factor(member_casual, levels = c("casual", "member")),
started_at = update(
started_at,
year = 2024,
month = 9,
day = wkdays
),
abbDays = lubridate::wday(started_at, label = TRUE, abbr = TRUE),
abbDays = forcats::as_factor(abbDays)
) |>
duckdb::dbWriteTable(conn = dbconn,
name = "db/dow.db",
overwrite = TRUE)
}

```
:::

::: flex-code
```{r}
#| label: writeDaysWQ
#| code-summary: Query ..., transform, and write weighted quartile data to ..._wq.db.
#| tidy: true

if (isFALSE(duckdb::dbExistsTable(dbconn, "db/dow_wq.db"))) {
transformData(
conn = dbconn,
path = "db/dow.db",
select_cols = c("started_at", "member_casual"),
group_cols = c("started_at", "member_casual"),
binary_col = "member_casual",
pred_col = "started_at",
ntile_col = "quartile",
zero_val = "casual",
one_val = "member",
qtile_levels = c(
"Q1 (Sun 12:00 am - Mon 11:40 am]",
"Q2 (Mon 11:40 am - Wed 05:14 am]",
"Q3 (Wed 05:14 am - Fri 12:19 pm]",
"Q4 (Fri 12:19 pm - Sat 11:59 pm]"
),
doQuantile = TRUE,
doWeights = TRUE
) |>
duckdb::dbWriteTable(conn = dbconn,
name = "db/dow_wq.db",
overwrite = TRUE)
}

```
:::
:::
:::

::: {#offcanvas472 .offcanvas .offcanvas-start tabindex="1" aria-labelledby="offcanvas" style="width: auto"}
::: offcanvas-header
::: {.h5 .offcanvas-title}
Tables
:::

::: {.btn-close type="button" data-bs-dismiss="offcanvas" ariaLabel="Close"}
:::
:::

::: offcanvas-body
::: {layout="[[1]]"}
```{r}
#| label: tbl-kableDay
#| tbl-cap: Days of the week
#| tidy: true

dplyr::tbl(dbconn, "db/dow.db") |>
dplyr::collect() |>
head() |>
kableExtra::kable()

```
:::
:::
:::

::: panel-tabset
### [Overall Frequency Analysis]{.panel-tabset-label}

::: panel-tabset
#### [Frequency Plot]{.panel-tabset-label}

```{r}
#| label: fig-wkdayTotals
#| fig-cap: Weekday Totals Frequency
#| fig-dpi: 150
#| tidy: true

# Values were too similar to visualize differences, see coord_cartesion()
gplot <- 
transformData(
conn = dbconn, 
path = "db/dow.db", 
select_cols = "abbDays",
group_cols = "abbDays", 
doWeights = TRUE
) |>
plotter(
x_col = abbDays, 
y_col = n, 
geomType = "column", 
title = "Days for all Riders", 
x_label = "Days of the Week",
y_label = "n"
) +
ggplot2::coord_cartesian(ylim = c(4.5 * 10^5, NA))

gplot
```

#### [Frequency Table]{.panel-tabset-label}

```{r}
#| label: tbl-wkdayTotals
#| tbl-cap: Weekday Total Frequency
#| tidy: true

transformData(
conn = dbconn, 
path = "db/dow.db", 
select_cols = "abbDays",
group_cols = "abbDays", 
doWeights = TRUE
) |>
tabler( 
title = "Days of the Week", 
note_list = list("day of the week", "observation count"),
location_list = list("abbDays", "n"),
source_note = gt::md("**Source**: `db/dow.db`"),
noteColumns = TRUE
) |>
gt::cols_label(abbDays = " ", n = " ") |>
gt::cols_align(columns = n, align = "right") |>
gt::cols_align(columns = abbDays, align = "left") 

```
:::

### [Comparative Frequency Analysis]{.panel-tabset-label}

::: panel-tabset
#### [Frequency Plot]{.panel-tabset-label}

```{r}
#| label: fig-wkdayCompare
#| fig-cap: Weekday Group Frequency
#| fig-dpi: 150
#| tidy: true

gplot <- 
transformData(
conn = dbconn, 
path = "db/dow.db", 
select_cols = c("abbDays", "member_casual"),
group_cols = c("abbDays", "member_casual"),
doWeights = TRUE
) |>
plotter(
title = "Day Groups",
x_label = "Days",
y_label = "n",
x_col = abbDays, 
y_col = n, 
group_col = member_casual,
geomType = "column", 
isFaceted = TRUE,
is_colGroup = TRUE
)

gplot
```

#### [Frequency Table]{.panel-tabset-label}

```{r}
#| label: tbl-wkdayCompare
#| tbl-cap: Weekday Group Frequency
#| tidy: true

transformData(
conn = dbconn, 
path = "db/dow.db", 
select_cols = c("abbDays", "member_casual"),
group_cols = c("abbDays", "member_casual"),
doWeights = TRUE
) |>
tabler(
title = gt::md("How many members<br>grouped by day?"), 
groupName = "abbDays", 
location = n,
label_n = "n",
note_list = list("membership status", "observation count"),
location_list = list("member_casual", "n"),
source_note = gt::md("**Source**: `db/dow.db`"),
noteColumns = TRUE,
isStub = TRUE,
stub_label = " ",
stub_note = "days of the week (abbreviated)"
) |>
gt::cols_label(member_casual = " ", n = " ") |>
gt::cols_align(columns = n, align = "right") |>
gt::cols_align(columns = member_casual, align = "left") 

```

#### [Chi-Squared]{.panel-tabset-label}

```{r}
#| label: chisquareScriptDays
#| code-summary: The script I used to process the Chi-Squared test result. This code is reposted with all the chi-square tables in this report.  
#| file: "Scripts/chisqTest.R"
#| eval: false 
```

```{r}
#| label: daysChiResult
#| code-summary: Save the chi-square statistic and degrees of freedom values in a tibble format to add to the gtsummary table.
#| tidy: true

data_tibble <- 
dplyr::tbl(dbconn, "db/dow.db") |>
dplyr::select(abbDays, member_casual) |>
dplyr::arrange(abbDays, member_casual) |>
dplyr::collect()

chiResult <- chisqTest(
data = data_tibble, variable = "abbDays", by = "member_casual")

```

```{r}
#| label: tbl-chiDays
#| tidy: true

tabler(
title = "Chi-Square: Days of the Week",
source_note = gt::md("**Source**: `db/moy.db`"),
label = list(abbDays = "Day"),
by = member_casual,
isSummary = TRUE,
tbl_name = data_tibble,
chi_result = chiResult
)

```

#### [Density]{.panel-tabset-label}

```{r}
#| label: fig-dayDensity
#| code-summary: Density by day of the week.
#| fig-dpi: 150
#| tidy: true

gplot <- 
dplyr::tbl(dbconn, "db/dow.db") |>
dplyr::select(started_at, member_casual) |>
dplyr::collect() |>
plotter(
title = "Weekday Group Density",
x_label = paste0("Day"),
x_col = started_at, 
group_col = member_casual,
geomType = "other",
angle = 45,
color_col = "black",
density_alpha = 0.75,
isTime = TRUE,
date_breaks = "1 day",
date_labels = "%a")

gplot
```

#### [Binary Logistic Regression]{.panel-tabset-label}

```{r}
#| label: modelDaysQ
#| code-summary: Query ..._wq.db, process and create model R object for hour based on quartile range. 
#| tidy: true

model <- 
dplyr::tbl(dbconn, "db/dow_wq.db") |>
dplyr::collect() |> 
glm(
formula = member_casual ~ quartile, 
family = binomial,
weights = n
)

```

```{r}
#| label: tbl-modelDaysQ
#| code-summary: Pipe model object to tbl_regression(), then further adjust output with tabler().  
#| tidy: true

model |>
gtsummary::tbl_regression(
label = list(quartile = "Weekday Ranges"), 
conf.int = FALSE, 
exponentiate = TRUE) |>
tabler(
title = gt::md("Binary Logistic Regression: <br> Week Days"),
source_note = gt::md("**Source**: `db/dow_wq.db`"),
isBinary = TRUE
)

```

#### [Histogram Plot]{.panel-tabset-label}

```{r}
#| label: fig-Hdays
#| fig-dpi: 150
#| tidy: true

qdf <- dplyr::tbl(dbconn, "db/dow.db") |>
dplyr::select(started_at) |>
dplyr::collect()

quartiles <- quantile(qdf$started_at, probs = c(0.25, 0.5, 0.75))

gplot <- qdf |>
plotter(
title = "Days",
x_label = "Days",
y_label = "n",
x_col = started_at, 
geomType = "column", 
isHistogram = TRUE,
isTimeHist = TRUE,
date_breaks = "1 day", 
date_labels = "%a", 
angle = 45,
color_col = "black",
vline_color = "lightyellow",
vline_size = 0.5,
low = "blue",
high = "red",
binwidth = \(x) 2 * IQR(x) / (length(x)^(1/3)),
quartiles = quartiles,
qformat = "%a %I %p"
) 

gplot

```
:::
:::

## Hour {#sec-hod}

::: {.d-grid .gap-3}
::: {.callout-important .tableScroller .mb-3 icon="false"}
<!-- -->

-   @tbl-hourTotals and @fig-hourTotals give an aggregated distribution of ridership frequency by the hour of the day.

-   @tbl-hourMembership and @fig-hourCompare summarize the hourly distribution by membership.

-   @tbl-chiHours, the null hypothesis is rejected. The type of bicycle and the rider's membership status can be considered dependent variables.

-   To visualize monthly users through the lens of their respective concentrations, see @fig-hourDensity. The plot looks a little different because I am directly plotting the x-axis using the original date-time data types. I think this maintains a more accurate view of the data.

-   @tbl-modelHourQ presents the results of a binary logistic regression analyzing the relationship between hour of the day and membership status. The analysis divides the day into four quartiles, with Q1 (12:00 am - 10:59 am) serving as the reference category.

    -   Compared to Q1, the odds of being a member versus a casual rider varied significantly across the other time quartiles (p \< 0.001 for all comparisons).

    -   Specifically, the odds of membership were 1.44 times as high in Q2 (10:59 am - 03:24 pm), 1.04 times as high in Q3 (03:24 pm - 06:05 pm), and 0.97 times as high in Q4 (06:05 pm - 11:59 pm).

    -   These results reveal a non-linear relationship between time of day and membership status. The highest likelihood of membership occurs during Q2, corresponding to midday hours.

    -   There's a slight increase in membership likelihood during Q3 (late afternoon) compared to the reference period, while evening hours (Q4) show a slight decrease in membership likelihood.

-   To visualize the full duration dataset as a histogram with illustrated, colored-coded quartiles as dotted lines, see @fig-Histogram (the solid yellow line represents the mean).
:::

::: {.btn-group role="group" aria-label="third"}
::: {.btn .btn-dark type="button" data-bs-toggle="offcanvas" data-bs-target="#offcanvas371" aria-controls="offcanvas"}
Database Operations
:::

::: {.btn .btn-secondary type="button" data-bs-toggle="offcanvas" data-bs-target="#offcanvas372" aria-controls="offcanvas"}
Table Preview
:::
:::
:::

::: {#offcanvas371 .offcanvas .offcanvas-start tabindex="-1" aria-labelledby="offcanvas" style="width: auto"}
::: offcanvas-header
::: {.h5 .offcanvas-title}
DB Operations
:::

::: {.btn-close type="button" data-bs-dismiss="offcanvas" ariaLabel="Close"}
:::
:::

::: offcanvas-body
::: flex-code
```{r}
#| label: writeHod
#| code-summary: Write ... to the database.
#| tidy: true

if (isFALSE(duckdb::dbExistsTable(dbconn, "db/hod.db"))) {
dplyr::tbl(dbconn, filtered_path) |>
dplyr::select(started_at, member_casual) |>
dplyr::arrange(started_at) |>
dplyr::collect() |>
dplyr::mutate(
started_at_time = update(
started_at,
year = 2023,
month = 1,
day = 1
),
hr = stringr::str_to_lower(format(
lubridate::round_date(started_at, unit = "hour"), "%I %p"
)),
hrMin = stringr::str_to_lower(format(
lubridate::round_date(started_at, unit = "minute"),
"%I:%M %p"
)),
hrminSec = stringr::str_to_lower(format(
lubridate::round_date(started_at, unit = "second"), "%r"
)),
hr = forcats::as_factor(hr),
hrMin = forcats::as_factor(hrMin)
) |>
dplyr::select(member_casual:hrminSec) |>
duckdb::dbWriteTable(conn = dbconn,
name = "db/hod.db",
overwrite = TRUE)
}

```
:::

::: flex-code
```{r}
#| label: hoursWeightedQuantiles
#| code-summary: Query ..., transform and write weighted quartile data to hod_wq.db.
#| tidy: true

if (isFALSE(duckdb::dbExistsTable(dbconn, "db/hod_wq.db"))) {
transformData(
conn = dbconn,
path = "db/hod.db",
select_cols = c("started_at_time", "member_casual"),
group_cols = c("started_at_time", "member_casual"),
binary_col = "member_casual",
pred_col = "started_at_time",
ntile_col = "quartile",
zero_val = "casual",
one_val = "member",
qtile_levels = c(
"Q1 (12:00 am - 10:59 am]",
"Q2 (10:59 am - 03:24 pm]",
"Q3 (03:24 pm - 06:05 pm]",
"Q4 (06:05 pm - 11:59 pm]"
),
doQuantile = TRUE,
doWeights = TRUE
) |>
duckdb::dbWriteTable(conn = dbconn,
name = "db/hod_wq.db",
overwrite = TRUE)
}

```
:::
:::
:::

::: {#offcanvas372 .offcanvas .offcanvas-start tabindex="1" aria-labelledby="offcanvas" style="width: auto"}
::: offcanvas-header
::: {.h5 .offcanvas-title}
Tables
:::

::: {.btn-close type="button" data-bs-dismiss="offcanvas" ariaLabel="Close"}
:::
:::

::: offcanvas-body
::: {layout="[[1,2],[3]]"}
```{r}
#| label: tbl-hodKable
#| tbl-cap: Kable output of hod.db
#| tidy: true

dplyr::tbl(dbconn, "db/hod.db") |>
dplyr::collect() |>
head() |>
kableExtra::kable()

```

```{r}
#| label: tbl-hoursKableWQ
#| tbl-cap: Kable output of hod_wq.db
#| tidy: true

dplyr::tbl(dbconn, "db/hod_wq.db") |>
dplyr::collect() |>
head() |>
kableExtra::kable()

```
:::
:::
:::

::: column-body-outset-right
::: panel-tabset
### [Overall Frequency Analysis]{.panel-tabset-label}

::: panel-tabset
#### [Frequency Plot]{.panel-tabset-label}

```{r}
#| label: fig-hourTotals
#| fig-cap: Total frequency by hour of day.
#| fig-dpi: 150
#| tidy: true

gplot <- transformData(
conn = dbconn, 
path = "db/hod.db", 
select_cols = "hr",
group_cols = "hr", 
doWeights = TRUE
) |>
plotter(
x_col = hr, 
y_col = n,
geomType = "column", 
title = "Hour of Day", 
x_label = "Hour", 
y_label = "n",
) +
ggplot2::scale_x_discrete(guide = ggplot2::guide_axis(n.dodge = 1, angle = 45)) +
ggplot2::theme(
axis.text = ggplot2::element_text(size = 8)
)

gplot

```

#### [Frequency Table]{.panel-tabset-label}

::: tableScroller
```{r}
#| label: tbl-hourTotals
#| tbl-cap: "Total freqeuncy by the hour of day"
#| tbl-cap-location: top
#| tidy: true

transformData(
conn = dbconn, 
path = "db/hod.db", 
select_cols = "hr",
group_cols = "hr", 
doWeights = TRUE
) |>
tabler( 
title = "Hours", 
note_list = list("hour of the day", "observation count"),
location_list = list("hr", "n"),
source_note = gt::md("**Source**: `db/hod.db`"),
noteColumns = TRUE
) |>
gt::cols_label("hr" = " ", n = " ") |>
gt::cols_align(columns = n, align = "right") |>
gt::cols_align(columns = hr, align = "left") 

```
:::
:::

### [Comparative Frequency Analysis]{.panel-tabset-label}

::: panel-tabset
#### [Frequency Plot]{.panel-tabset-label}

```{r}
#| label: fig-hourCompare
#| fig-cap: Grouped hour frequency
#| fig-dpi: 150
#| tidy: true

gplot <- 
transformData(
conn = dbconn, 
path = "db/hod.db", 
select_cols = c("hr", "member_casual"),
group_cols = c("hr", "member_casual"),
doWeights = TRUE
) |>
plotter(
title = "Hour Groups",
x_label = "Hour of Day",
y_label = "n",
x_col = hr, 
y_col = n, 
group_col = member_casual,
geomType = "column", 
isFaceted = TRUE,
is_colGroup = TRUE
) +
ggplot2::scale_x_discrete(guide = ggplot2::guide_axis(n.dodge = 1, angle = 45)) +
ggplot2::theme(
axis.text = ggplot2::element_text(size = 8)
)

gplot

```

#### [Frequency Table]{.panel-tabset-label}

::: tableScroller
```{r}
#| label: tbl-hourMembership
#| tbl-cap: Frequencies for membership by hour.
#| tidy: true

transformData(
conn = dbconn, 
path = "db/hod.db", 
select_cols = c("hr", "member_casual"),
group_cols = c("hr", "member_casual"),
doWeights = TRUE
) |>
tabler(
title = gt::md("How many members<br>grouped by hour?"), 
groupName = "hr", 
location = n,
label_n = "n",
note_list = list("membership status", "observation count"),
location_list = list("member_casual", "n"),
source_note = gt::md("**Source**: `db/hod.db`"),
noteColumns = TRUE,
isStub = TRUE,
stub_label = "Day",
stub_note = "hour of the day (12-hour clock)"
) |>
gt::cols_label(member_casual = " ") |>
gt::cols_align(columns = n, align = "right") |>
gt::cols_align(columns = member_casual, align = "left") 

```
:::

#### [Chi-Squared]{.panel-tabset-label}

```{r}
#| label: chisquareScriptHours
#| code-summary: The script I used to process the Chi-Squared test result. This code is reposted with all the chi-square tables in this report.  
#| file: "Scripts/chisqTest.R"
#| eval: false 
```

```{r}
#| label: hoursChiResult
#| code-summary: Save the chi-square statistic and degrees of freedom values in a tibble format to add to the gtsummary table.
#| tidy: true

data_tibble <- 
transformData(
conn = dbconn, 
path = "db/hod.db", 
select_cols = c("hr", "member_casual")
)

chiResult <- chisqTest(
data = data_tibble, variable = "hr", by = "member_casual")

```

```{r}
#| label: tbl-chiHours
#| tbl-cap: Chi-Squared for aggregated hours data to the level of rounded hours.
#| tidy: true

tabler(
title = "Chi-Square: Hour of the Day",
source_note = gt::md("**Source**: `db/hod.db`"),
label = list(hr = "Hour"),
by = member_casual,
isSummary = TRUE,
tbl_name = data_tibble,
chi_result = chiResult
)

```

#### [Density]{.panel-tabset-label}

```{r}
#| label: fig-hourDensity
#| fig-cap: Query, load, and plot the grouped densities 
#| fig-dpi: 150
#| tidy: true

gplot <- 
dplyr::tbl(dbconn, "db/hod.db") |>
dplyr::collect() |>
plotter(
title = "Hour Group Density",
x_label = paste0("Hours", "\n", "(12-hour clock)"),
x_col = started_at_time, 
group_col = member_casual,
geomType = "other",
angle = 45,
color_col = "black",
density_alpha = 0.75,
isTime = TRUE,
date_breaks = "1 hour",
date_labels = "%I %p",
)

gplot

```

#### [Binary Logistic Regression]{.panel-tabset-label}

```{r}
#| label: modelHourQ
#| code-summary: Query hod_wq.db, process and create model R object for hour based on quartile range. 
#| tidy: true

model <- 
dplyr::tbl(dbconn, "db/hod_wq.db") |>
dplyr::collect() |> 
glm(
formula = member_casual ~ quartile, 
family = binomial,
weights = n
)

```

```{r}
#| label: tbl-modelHourQ
#| code-summary: Pipe model object to tbl_regression(), then further adjust output with tabler().  
#| tidy: true

model |>
gtsummary::tbl_regression(
label = list(quartile = "Hour Ranges"), 
conf.int = FALSE, 
exponentiate = TRUE) |>
tabler(
title = gt::md("Binary Logistic Regression: <br> Hour"),
source_note = gt::md("**Source**: `db/hod_wq.db`"),
isBinary = TRUE)
```

#### [Histogram Plot]{.panel-tabset-label}

::: column-body-outset-right
```{r}
#| label: fig-Histogram
#| fig-dpi: 150
#| tidy: true
#| fig-cap-location: top
#| fig-cap: Hourly histogram with quartile ranges

qdf <- dplyr::tbl(dbconn, "db/hod.db") |>
dplyr::select(started_at_time) |>
dplyr::collect()

quartiles <- quantile(qdf$started_at_time, probs = c(0.25, 0.5, 0.75))

gplot <- qdf |>
plotter(
title = "Hours",
x_label = paste0("Hours\n", "(12-hour clock)"),
y_label = "n",
x_col = started_at_time, 
geomType = "column", 
isHistogram = TRUE,
isTimeHist = TRUE,
date_breaks = "2 hour", 
date_labels = "%I %p", 
angle = 45,
color_col = "black",
vline_color = "lightyellow",
vline_size = 0.5,
low = "blue",
high = "red",
binwidth = \(x) 2 * IQR(x) / (length(x)^(1/3)),
quartiles = quartiles,
qformat = "%I:%M %p"
) 

gplot +
ggplot2::theme(
axis.text.x = ggplot2::element_text(size = ggplot2::rel(.9)))

```
:::
:::
:::
:::

## Distance {#sec-distance}

::: {.d-grid .gap-3}
::: {.callout-important .tableScroller .mb-3 icon="false"}
<!-- -->

-   @tbl-milesTotals and @fig-milesTotals give an aggregated distribution of ridership frequency by the miles traveled in a trip.

-   @tbl-milesCompare and @fig-membershipMiles summarize the hourly distribution by membership.

-   To visualize users through the lens of their respective concentrations, see @fig-distanceDensity.

-   @tbl-modeldistanceQ presents the odds ratios for membership status across distance quartiles, with Q1 serving as the reference category.

    -   Compared to Q1, the odds of being a member versus a casual rider were significantly lower in all other quartiles (p \< 0.001 for all comparisons).

    -   Specifically, the odds of membership were 0.63 times as high in Q2, 0.59 times as high in Q3, and 0.65 times as high in Q4.

    -   These results indicate an inverse relationship between ride distance and membership status, with members generally associated with shorter ride distances.

    -   Interestingly, the lowest odds of membership were observed in Q3, rather than Q4, suggesting a non-linear relationship between distance and membership likelihood.

-   To visualize the full distance dataset as a histogram with illustrated, colored-coded quartiles as dotted lines, see @fig-distanceHistogram (the solid yellow line represents the mean).

-   @tbl-distanceSummary gives the reader some idea of the variability, range, and quartile information about the distance data.
:::

::: {.btn-group role="group" aria-label="third"}
::: {.btn .btn-dark type="button" data-bs-toggle="offcanvas" data-bs-target="#offcanvas271" aria-controls="offcanvas"}
Database Operations
:::

::: {.btn .btn-secondary type="button" data-bs-toggle="offcanvas" data-bs-target="#offcanvas272" aria-controls="offcanvas"}
Table Preview
:::
:::
:::

::: {#offcanvas271 .offcanvas .offcanvas-start tabindex="-1" aria-labelledby="offcanvas" style="width: auto"}
::: offcanvas-header
::: {.h5 .offcanvas-title}
DB Operations
:::

::: {.btn-close type="button" data-bs-dismiss="offcanvas" ariaLabel="Close"}
:::
:::

::: offcanvas-body
::: flex-code
```{r}
#| label: writeDistance
#| code-summary: Write distance.db to the database.
#| tidy: true

if (isFALSE(duckdb::dbExistsTable(dbconn, "db/distance.db"))) {
dplyr::tbl(dbconn, filtered_path) |>
dplyr::select(miles, member_casual) |>
dplyr::arrange(miles) |>
dplyr::collect() |>
dplyr::mutate(
milesR = miles,
milesR = dplyr::case_when(
milesR >= 1 ~ round(milesR, digits = 0),
miles < 1 ~ round(signif(milesR, 3), digits = 1)
),
milesR = forcats::as_factor(milesR)
) |>
duckdb::dbWriteTable(conn = dbconn,
name = "db/distance.db",
overwrite = TRUE)
}

```
:::

::: flex-code
```{r}
#| label: writeDistanceWeightedQuantiles
#| code-summary: Query ... .db, transform and write weighted quartile data to ... _wq.db. 
#| tidy: true

if (isFALSE(duckdb::dbExistsTable(dbconn, "db/distance_wq.db"))) {
transformData(
conn = dbconn,
path = "db/distance.db",
select_cols = c("miles", "member_casual"),
group_cols = c("miles", "member_casual"),
binary_col = "member_casual",
pred_col = "miles",
ntile_col = "quartile",
zero_val = "casual",
one_val = "member",
qtile_levels = c("Q1 (0.10 - 0.63]", "Q2 (0.63 - 1.02]", "Q3 (1.02 - 1.76]", "Q4 (1.76 - 20.5]"),
doQuantile = TRUE,
doWeights = TRUE
) |>
duckdb::dbWriteTable(conn = dbconn,
name = "db/distance_wq.db",
overwrite = TRUE)
}

```
:::
:::
:::

::: {#offcanvas272 .offcanvas .offcanvas-start tabindex="1" aria-labelledby="offcanvas" style="width: auto"}
::: offcanvas-header
::: {.h5 .offcanvas-title}
Tables
:::

::: {.btn-close type="button" data-bs-dismiss="offcanvas" ariaLabel="Close"}
:::
:::

::: offcanvas-body
::: {layout="[[1,2]]"}
```{r}
#| label: tbl-dbhod
#| tbl-cap: Distance
#| tidy: true

dplyr::tbl(dbconn, "db/distance.db") |>
dplyr::collect() |>
head() |>
kableExtra::kable()

```

```{r}
#| label: tbl-hoursWQ
#| tbl-cap: Distance, Weighted Quantiles
#| tidy: true

dplyr::tbl(dbconn, "db/distance_wq.db") |>
dplyr::collect() |>
head() |>
kableExtra::kable()

```
:::
:::
:::

::: panel-tabset
### [Overall Frequency Analysis]{.panel-tabset-label}

::: panel-tabset
#### [Frequency Plot]{.panel-tabset-label}

```{r}
#| label: fig-milesTotals
#| fig-cap: Miles Total Frequency
#| fig-dpi: 150
#| tidy: true

gplot <- 
transformData(
conn = dbconn, 
path = "db/distance.db", 
select_cols = "milesR",
group_cols = "milesR", 
doWeights = TRUE
) |>
plotter(
x_col = milesR, 
y_col = n,
geomType = "column", 
title = "Distance", 
x_label = "Miles", 
y_label = "n") 

gplot +
ggplot2::scale_x_discrete(guide = ggplot2::guide_axis(n.dodge = 1, angle = 45)) +
ggplot2::theme(
axis.text = ggplot2::element_text(size = 7)
)


```

#### Frequency [Table]{.panel-tabset-label}

::: tableScroller
```{r}
#| label: tbl-milesTotals
#| tbl-cap: Miles Total Frequency
#| tidy: true

transformData(
conn = dbconn, 
path = "db/distance.db", 
select_cols = "milesR",
group_cols = "milesR", 
doWeights = TRUE
) |>
tabler( 
title = "Distance", 
note_list = list("trip distance", "observation count"),
location_list = list("milesR", "n"),
source_note = gt::md("**Source**: `db/distance.db`"),
noteColumns = TRUE
) |>
gt::cols_label(milesR = "Miles") |>
gt::cols_align(columns = n, align = "right") |>
gt::cols_align(columns = milesR, align = "left") 

```
:::
:::

### [Comparative Frequency Analysis]{.panel-tabset-label}

::: panel-tabset
#### [Frequency Plot]{.panel-tabset-label}

```{r}
#| label: fig-membershipMiles
#| fig-cap: Distance by membership.
#| fig-dpi: 150
#| tidy: true

gplot <- 
transformData(
conn = dbconn, 
path = "db/distance.db", 
select_cols = c("milesR", "member_casual"),
group_cols = c("milesR", "member_casual"),
doWeights = TRUE
) |>
plotter(
title = "Distance Groups",
x_label = "Miles",
y_label = "n",
x_col = milesR, 
y_col = n, 
group_col = member_casual,
geomType = "column", 
isFaceted = TRUE,
is_colGroup = TRUE
)

gplot +
ggplot2::scale_x_discrete(guide = ggplot2::guide_axis(n.dodge = 1, angle = 45)) +
ggplot2::theme(
axis.text = ggplot2::element_text(size = 7)
)


```

#### [Frequency Table]{.panel-tabset-label}

::: tableScroller
```{r}
#| label: tbl-milesCompare
#| tbl-cap: Miles Group Frequency
#| tidy: true

transformData(
conn = dbconn, 
path = "db/distance.db", 
select_cols = c("milesR", "member_casual"),
group_cols = c("milesR", "member_casual"),
doWeights = TRUE
) |>
tabler(
title = gt::md("How many members<br>grouped by distance?"), 
groupName = "milesR", 
location = n,
label_n = "n",
note_list = list("membership status", "observation count"),
location_list = list("member_casual", "n"),
source_note = gt::md("**Source**: `db/distance.db`"),
noteColumns = TRUE,
isStub = TRUE,
stub_label = " ",
stub_note = "miles traveled per trip"
) |>
gt::cols_label(member_casual = " ") |>
gt::cols_align(columns = n, align = "right") |>
gt::cols_align(columns = member_casual, align = "left") 

```
:::

#### [Density]{.panel-tabset-label}

```{r}
#| label: fig-distanceDensity
#| fig-cap: Densities of miles to membership.
#| fig-dpi: 150
#| tidy: true

gplot <- 
dplyr::tbl(dbconn, "db/distance.db") |>
dplyr::select(miles, member_casual) |>
dplyr::collect() |>
plotter(
title = "Distance Group Density",
x_label = "Miles",
x_col = miles, 
group_col = member_casual,
geomType = "column",
angle = 45,
color_col = "black",
density_alpha = 0.75,
isDensity = TRUE,
is_colGroup = TRUE,
breaks = seq(0, 11, by = 1),
limits = c(0.1, 11)
)

gplot

```

#### [Binary Logistic Regression]{.panel-tabset-label}

```{r}
#| label: modeldistanceQ
#| code-summary: Query ..._wq.db, process and create model R object for hour based on quartile range. 
#| tidy: true

model <- 
dplyr::tbl(dbconn, "db/distance_wq.db") |>
dplyr::collect() |> 
glm(
formula = member_casual ~ quartile, 
family = binomial,
weights = n
)
```

```{r}
#| label: tbl-modeldistanceQ
#| code-summary: Pipe model object to tbl_regression(), then further adjust output with tabler().  
#| tidy: true

model |>
gtsummary::tbl_regression(
label = list(quartile = "Distance Ranges"), 
conf.int = FALSE, 
exponentiate = TRUE) |>
tabler(
title = gt::md("Binary Logistic Regression: <br> Distance"),
source_note = gt::md("**Source**: `db/distance_wq.db`"),
isBinary = TRUE
)

```

#### [Histogram Plot]{.panel-tabset-label}

```{r}
#| label: fig-distanceHistogram
#| fig-dpi: 150
#| tidy: true

qdf <-
dplyr::tbl(dbconn, "db/distance.db") |>
dplyr::select(miles) |>
dplyr::collect()

quartiles <- quantile(as.numeric(qdf$miles), probs = c(0.25, 0.5, 0.75))

gplot <- qdf |>
plotter(
title = "Distance",
x_label = "Miles",
y_label = "n",
x_col = miles, 
geomType = "column", 
isHistogram = TRUE,
angle = 45,
color_col = "transparent",
vline_color = "lightyellow",
vline_size = 0.5,
low = "blue",
high = "red",
binwidth = \(x) 2 * IQR(x) / (length(x)^(1/3)),
limits = c(0.1, 5),
breaks = seq(1, 5, by = 1),
quartiles = quartiles) 

gplot

```

#### [Summary Stats]{.panel-tabset-label}

```{r}
#| label: tbl-distanceSummary
#| tbl-cap: "Useful summary statistics."

gTable <- 
dplyr::tbl(dbconn, "db/distance.db") |>
dplyr::select(miles, member_casual) |>
dplyr::collect() |>
gtsummary::tbl_summary(
by = member_casual,
type = miles ~ "continuous2",
label = list(miles ~ "Distance (miles)"),
digits = list(
miles ~ c(2, 2)),
statistic = 
gtsummary::all_continuous() ~ c(
"{median} ({p25}, {p75})", 
"{mean} ({sd})", 
"{min}, {max}")
) |>
gtsummary::italicize_levels() |>
tabler(
title = gt::md("Summary Statistics:<br>Distance - Membership"),
source_note = gt::md("**Source**: `db/distance.db`"),
isBinary = TRUE
)

gTable

```
:::
:::

## Speed {#sec-speed}

::: {.d-grid .gap-3}
::: {.callout-important .tableScroller .mb-3 icon="false"}
<!-- -->

-   @tbl-mphTotals and @fig-mphTotals, give an aggregated distribution of ridership frequency by the hour of the day.

-   @tbl-mphCompare and @fig-mphCompare, summarize the trip speed distribution by membership.

-   To visualize monthly users through the lens of their respective concentrations, see @fig-speedDensity.

    -   The plot looks a little different because I am directly plotting the x-axis using the original date-time data types. I think this maintains a more accurate view of the data.

-   @tbl-modelspeedQ presents the odds ratios for membership status across speed quartiles, with Q1 serving as the reference category.

    -   Compared to Q1, the odds of being a member versus a casual rider were significantly higher in all other quartiles ($p < 0.001$ for all comparisons).

    -   Specifically, the odds of membership were 2.09 times higher in Q2, 2.50 times higher in Q3, and 2.69 times higher in Q4.

    -   These results suggest a strong positive association between riding speed and membership status, with the likelihood of membership increasing monotonically across speed quartiles.

-   To visualize the full speed dataset as a histogram with illustrated, colored-coded quartiles as dotted lines, see @fig-speedHistogram (the solid yellow line represents the mean).

-   @tbl-speedSummary gives the reader some idea of the variability, range, and quartile information about the distance data.
:::

::: {.btn-group role="group" aria-label="third"}
::: {.btn .btn-dark type="button" data-bs-toggle="offcanvas" data-bs-target="#offcanvas171" aria-controls="offcanvas"}
Database Operations
:::

::: {.btn .btn-secondary type="button" data-bs-toggle="offcanvas" data-bs-target="#offcanvas172" aria-controls="offcanvas"}
Table Preview
:::
:::
:::

::: {#offcanvas171 .offcanvas .offcanvas-start tabindex="-1" aria-labelledby="offcanvas" style="width: auto"}
::: offcanvas-header
::: {.h5 .offcanvas-title}
DB Operations
:::

::: {.btn-close type="button" data-bs-dismiss="offcanvas" ariaLabel="Close"}
:::
:::

::: offcanvas-body
::: flex-code
```{r}
#| label: writeSpeed
#| code-summary: Write ... to the database.
#| tidy: true

if (isFALSE(duckdb::dbExistsTable(dbconn, "db/speed.db"))) {
dplyr::tbl(dbconn, filtered_path) |>
dplyr::select(mph, member_casual) |>
dplyr::collect() |>
dplyr::mutate(
mphR = round(mph, digits = 0)) |>
dplyr::arrange(mph, member_casual) |>
duckdb::dbWriteTable(
conn = dbconn,
name = "db/speed.db",
overwrite = TRUE)
}

```
:::

::: flex-code
```{r}
#| label: speedWeightedQuantiles
#| code-summary: Query ... .db, transform and write weighted quartile data to ... _wq.db. 
#| tidy: true

if (isFALSE(duckdb::dbExistsTable(dbconn, "db/speed_wq.db"))) {
transformData(
conn = dbconn,
path = "db/speed.db",
select_cols = c("mph", "member_casual"),
group_cols = c("mph", "member_casual"),
binary_col = "member_casual",
pred_col = "mph",
ntile_col = "quartile",
zero_val = "casual",
one_val = "member",
qtile_levels = c("Q1 (1.0 - 5.4]", "Q2 (5.4 - 7.0]", "Q3 (7.0 - 8.6]", "Q4 (8.6 - 20]"),
doQuantile = TRUE,
doWeights = TRUE
) |>
duckdb::dbWriteTable(conn = dbconn, name = "db/speed_wq.db", overwrite = TRUE)
}

```
:::
:::
:::

::: {#offcanvas172 .offcanvas .offcanvas-start tabindex="1" aria-labelledby="offcanvas" style="width: auto"}
::: offcanvas-header
::: {.h5 .offcanvas-title}
Tables
:::

::: {.btn-close type="button" data-bs-dismiss="offcanvas" ariaLabel="Close"}
:::
:::

::: offcanvas-body
::: {layout="[[1,2]]"}
```{r}
#| label: tbl-kableSpeed
#| tbl-cap: Speed
#| tidy: true

dplyr::tbl(dbconn, "db/speed.db") |>
dplyr::collect() |>
head() |>
kableExtra::kable()

```

```{r}
#| label: tbl-kableSpeedWQ
#| tbl-cap: Speed with weighted quartile groups.
#| tidy: true

dplyr::tbl(dbconn, "db/speed_wq.db") |>
dplyr::collect() |>
head() |>
kableExtra::kable()

```
:::
:::
:::

::: panel-tabset
### [Overall Frequency Analysis]{.panel-tabset-label}

::: panel-tabset
#### [Frequency Plot]{.panel-tabset-label}

```{r}
#| label: fig-mphTotals
#| fig-cap: Mph Total Frequency
#| tidy: true
#| fig-dpi: 150

gplot <- 
transformData(
conn = dbconn, 
path = "db/speed.db", 
select_cols = "mphR",
group_cols = "mphR", 
doWeights = TRUE
) |>
plotter(
x_col = mphR, 
y_col = n,
geomType = "column", 
title = "Speed", 
x_label = "Miles per Hour", 
y_label = "n")

gplot

```

#### [Frequency Table]{.panel-tabset-label}

::: tableScroller
```{r}
#| label: tbl-mphTotals
#| tbl-cap: Mph Total Frequency
#| tidy: true

transformData(
conn = dbconn, 
path = "db/speed.db", 
select_cols = "mphR",
group_cols = "mphR", 
doWeights = TRUE
) |>
tabler(
title = "Speed",
note_list = list("trip speed (miles per hour)", "observation count"),
location_list = list("mphR", "n"),
source_note = gt::md("**Source**: `db/speed.db`"),
noteColumns = TRUE,
#hide_column_labels = TRUE
) |>
gt::cols_label(mphR = " ") |>
gt::cols_align(columns = n, align = "right") |>
gt::cols_align(columns = mphR, align = "center") 

```
:::
:::

### [Comparative Frequency Analysis]{.panel-tabset-label}

::: panel-tabset
#### [Frequency Plot]{.panel-tabset-label}

```{r}
#| label: fig-mphCompare
#| fig-cap: Mph Group Frequency
#| tidy: true
#| fig-dpi: 150

gplot <- 
transformData(
conn = dbconn, 
path = "db/speed.db", 
select_cols = c("mphR", "member_casual"),
group_cols = c("mphR", "member_casual"),
doWeights = TRUE
) |>
plotter(
title = "Speed Groups (Aggregated)",
x_label = "Miles per Hour",
y_label = "n",
x_col = mphR, 
y_col = n, 
color_col = member_casual,
geomType = "column",
is_colGroup = TRUE,
isFaceted = TRUE
)

gplot
```

#### [Frequency Table]{.panel-tabset-label}

::: tableScroller
```{r}
#| label: tbl-mphCompare
#| tbl-cap: Mph Group Frequency
#| tidy: true


transformData(
conn = dbconn, 
path = "db/speed.db", 
select_cols = c("mphR", "member_casual"),
group_cols = c("mphR", "member_casual"),
doWeights = TRUE
) |>
tabler(
title = gt::md("How many members<br>grouped by trip speed?"), 
groupName = "mphR", 
location = n,
label_n = "n",
note_list = list("membership status", "observation count"),
location_list = list("member_casual", "n"),
source_note = gt::md("**Source**: `db/speed.db`"),
noteColumns = TRUE,
isStub = TRUE,
stub_label = " ",
stub_note = "estimated average speed traveled per trip (mph)"
) |>
gt::cols_label(member_casual = " ") |>
gt::cols_align(columns = n, align = "right") |>
gt::cols_align(columns = member_casual, align = "left") 

```
:::

#### [Density]{.panel-tabset-label}

```{r}
#| label: fig-speedDensity
#| fig-cap: Density plot for speed by membership.
#| tidy: true
#| fig-dpi: 150

gplot <- 
dplyr::tbl(dbconn, "db/speed.db") |>
dplyr::select(mph, member_casual) |>
dplyr::collect() |>
plotter(
title = "Speed Group Density",
x_label = "MPH (miles per hour)",
x_col = mph, 
group_col = member_casual,
geomType = "column",
angle = 45,
color_col = "black",
density_alpha = 0.75,
isDensity = TRUE,
is_colGroup = TRUE,
breaks = seq(1, 20, by = 1),
limits = c(1, 20)
)

gplot

```

#### [Binary Logistic Regression]{.panel-tabset-label}

```{r}
#| label: modelspeedQ
#| code-summary: Query ..._wq.db, process and create model R object for hour based on quartile range. 
#| tidy: true

model <- 
dplyr::tbl(dbconn, "db/speed_wq.db") |>
dplyr::collect() |> 
glm(
formula = member_casual ~ quartile, 
family = binomial,
weights = n)

```

```{r}
#| label: tbl-modelspeedQ
#| code-summary: Pipe model object to tbl_regression(), then further adjust output with tabler().  
#| tidy: true

model |>
gtsummary::tbl_regression(
label = list(quartile = "Speed Ranges"), 
conf.int = FALSE, 
exponentiate = TRUE) |>
tabler(
title = gt::md("Binary Logistic Regression: <br> Speed & Membership"),
source_note = gt::md("**Source**: `db/speed_wq.db`"),
isBinary = TRUE)

```

#### [Histogram Plot]{.panel-tabset-label}

```{r}
#| label: fig-speedHistogram
#| tidy: true
#| fig-dpi: 150

qdf <-
dplyr::tbl(dbconn, "db/speed.db") |>
dplyr::select(mph) |>
dplyr::collect()

quartiles <- quantile(qdf$mph, probs = c(0.25, 0.5, 0.75))

gplot <- qdf |>
plotter(
title = "Estimated Average Trip Speed",
x_label = "Miles per Hour",
y_label = "n",
x_col = mph, 
geomType = "column", 
isHistogram = TRUE,
angle = 45,
color_col = "transparent",
vline_color = "lightyellow",
vline_size = 0.5,
low = "blue",
high = "red",
binwidth = \(x) 2 * IQR(x) / (length(x)^(1/3)),
breaks = seq(1, 20, by = 2),
limits = c(1, 20),
quartiles = quartiles) 

gplot

```

#### [Summary Stats]{.panel-tabset-label}

```{r}
#| label: tbl-speedSummary
#| tbl-cap: "Useful summary statistics."

dplyr::tbl(dbconn, "db/speed.db") |>
dplyr::select(mph, member_casual) |>
dplyr::collect() |>
gtsummary::tbl_summary(
by = member_casual,
type = mph ~ "continuous2",
label = list(mph ~ "Speed"),
digits = list(
mph ~ c(1, 1)),
statistic = 
gtsummary::all_continuous() ~ c(
"{median} ({p25}, {p75})", 
"{mean} ({sd})", 
"{min}, {max}")
) |>
gtsummary::italicize_levels() |>
tabler(
title = gt::md("Summary Statistics:<br>Speed - Membership"),
source_note = gt::md("**Source**: `db/speed.db`"),
isBinary = TRUE
)

```
:::
:::

## Interpretation of EDA

The EDA (exploratory data analysis) sections employ various statistical methods to uncover patterns in user behavior and preferences. A chi-square analysis reveals a significant association between bicycle type and membership status (p \< 0.001). Binary logistic regression further quantifies this relationship, showing that electric bike users have lower odds of being members compared to classic bike users. @sec-btypes

Trip duration analysis, also utilizing binary logistic regression, uncovers a notable trend: the odds of membership decrease substantially as trip duration increases. This model, using quartiles of trip duration, indicates that members generally take shorter, more concentrated trips, while casual users are more likely to engage in longer rides. @sec-duration

Seasonal trends emerge when examining monthly ridership patterns through another logistic regression model. The odds of membership fluctuate throughout the year, with the highest proportion of members riding during the colder months and early spring. As the weather warms, there's a noticeable shift towards more casual ridership, as evidenced by lower odds ratios in the summer months. @sec-moy

Weekly and daily patterns, analyzed using similar regression techniques, provide further insights into user behavior. Weekdays, @sec-dow, particularly during typical commuting hours, @sec-hod, see higher odds of member rides. In contrast, weekends and evenings show decreased odds of membership, suggesting an increased likelihood of casual ridership during these times.

The analysis of trip distances, again using logistic regression with distance quartiles, reveals an inverse relationship with membership status. Members are more likely to take shorter trips, while casual users tend to embark on longer journeys. This aligns with the duration findings and reinforces the idea that members use the service for quick, routine travel. @sec-duration

Interestingly, trip speed shows a strong positive association with membership status in the logistic regression model. The odds of membership increase monotonically across speed quartiles, indicating that members generally ride at higher speeds compared to casual users. @sec-speed

These findings, derived from a combination of chi-square tests for independence and multiple binary logistic regression models, paint a picture of two distinct user groups: members who typically use the bike share for short, fast, routine trips during weekdays, and casual users who tend to take longer, slower rides, often during leisure hours or weekends.

Contingency tables and visualizations, including density plots and histograms, supplement these statistical analyses, providing a comprehensive view of the data distribution across various parameters such as bike type, trip duration, time of day, and day of the week.

The robust statistical approach, combining hypothesis testing (chi-square) with predictive modeling (logistic regression), provides strong evidence for the observed patterns in user behavior. These insights could prove valuable for tailoring marketing strategies, optimizing bike distribution, and enhancing service offerings to better serve both user segments.

## Geographic Data

### Traffic Flow {#sec-epiflow}

::: p-1
@fig-epiflowNetwork presents an intriguing bird's-eye view of trip behaviors through an interactive *epiflows* graph. \]@moraga\] This R package used for creating this graph was re-purposed from its original intent for visualizing the spread of disease. This visualization employs a network of nodes (circles) connected by lines, where the thickness of the lines roughly corresponds to the volume of trips between the nodes, with thicker lines indicating a higher number of trips. The top 34 most frequently traveled stations are depicted in this visual network diagram.

The interactive nature of the epiflows allows users to click on individual nodes and lines to access more detailed information. Additionally, a drop-down window provides further exploration capabilities, enabling users to delve deeper into the data.

These stations represent the most active locations within the system. Fortunately, @sec-mapview explores a potential approach to gain insights into the typical high-traffic station locations and the underlying reasons behind their elevated activity levels.
:::

::: {#offcanvas13 .offcanvas .offcanvas-start tabindex="-1" aria-labelledby="offcanvas" style="width: auto"}
::: offcanvas-header
::: {.h5 .offcanvas-title}
Creating an EpiFlow
:::

::: {.btn-close type="button" data-bs-dismiss="offcanvas" ariaLabel="Close"}
:::
:::

::: offcanvas-body
::: flex-code
```{r}
#| label: createFlows dataframe
#| code-summary: First, creates the frequency of trips taken to and from pairs of stations. We are only going to look deeper into the top 50 most traveled pairs.
#| tidy: true

flowData <- dplyr::tbl(dbconn, filtered_path) |>
dplyr::select(start_station_name, end_station_name) |>
dplyr::group_by(start_station_name, end_station_name) |>
dplyr::summarize(n = n()) |>
dplyr::ungroup() |>
dplyr::arrange(desc(n)) |>
dplyr::rename("from_station" = start_station_name, "to_station" = end_station_name) |>
dplyr::collect() |>
dplyr::slice_head(n = 50)
```

```{r}
#| label: location stats
#| code-summary: Second, we need statistics but also to combine the statistics for every unique station name. 
#| tidy: true

locationData <- dplyr::tbl(dbconn, filtered_path) |>
dplyr::select(start_station_name,
end_station_name,
started_at,
ended_at,
trip_time) |>
dplyr::group_by(start_station_name, end_station_name) |>
dplyr::mutate("trip_time" = round(trip_time, digits = 0)) |>
dplyr::summarize(
"trip_count" = dplyr::n(),
"first_date" = min(started_at),
"last_date" = max(ended_at),
) |>
dplyr::ungroup() |>
dplyr::rename("from_station" = start_station_name, "to_station" = end_station_name) |>
dplyr::arrange(desc(trip_count)) |>
dplyr::collect()

# Need to combine all names to single column and recalculate
# or retain other stats.
locationData_pivoted <- locationData |>
tidyr::pivot_longer(cols = 1:2, values_to = "allNames") |>
dplyr::group_by(allNames) |>
dplyr::summarize(
"trips_toAndfrom" = sum(trip_count),
first_date = min(first_date),
last_date = max(last_date),
) |>
dplyr::arrange(trips_toAndfrom)
```

```{r}
#| label: MakeEpiflows
#| code-summary: Third, creates epiflow objects, which take in a pair of dataframes and creates the flows between them.
#| tidy: true

# for all the pairs
ef_test <- epiflows::make_epiflows(flows = flowData,
locations = locationData_pivoted,
num_cases = "trips_toAndfrom")

```
:::
:::
:::

::: {#offcanvas14 .offcanvas .offcanvas-end tabindex="1" aria-labelledby="offcanvas" style="width: auto"}
::: offcanvas-header
::: {.h5 .offcanvas-title}
Tables
:::

::: {.btn-close type="button" data-bs-dismiss="offcanvas" ariaLabel="Close"}
:::
:::

::: offcanvas-body
::: flex-code
```{r}
#| label: summaryFlowData
#| code-summary: First, just a quick view of the flow data table we made earlier.
#| title: Flow Data View
#| tidy: true

flowData
```

```{r}
#| label: pivotedLocations
#| code-summary: Second, another quick view, but for thethe location data we pivoted earlier.
#| title: Pivoted Location Data
#| tidy: true

locationData_pivoted |>
dplyr::arrange(desc(trips_toAndfrom))

```
:::
:::
:::

::: {.article style="color: Black"}
```{r}
#| label: fig-epiflowNetwork
#| fig-cap: EpiFlow Network
#| echo: false
#| tidy: true

epiflows::vis_epiflows(ef_test)
```
:::

::: {.d-grid .gap-0}
::: {.btn-group role="group" aria-label="third"}
::: {.btn .btn-primary type="button" data-bs-toggle="offcanvas" data-bs-target="#offcanvas13" aria-controls="offcanvas"}
Code Steps
:::

::: {.btn .btn-secondary type="button" data-bs-toggle="offcanvas" data-bs-target="#offcanvas14" aria-controls="offcanvas"}
Table Preview
:::
:::
:::

### Checking the Map {#sec-mapview}

::: p-1
This section was made possible thanks to the latitude and longitude coordinates data provided alongside the stations names. Coming from the epiflow diagram, this should help make the data less abstract. The accordion below expands and collapses four *OpenStreet* maps found in the callout section below. These maps were split for viewing logistics. They contain from the epiflow in the section above. These maps are interactive, so the default views are zoom-able and movable. The transparent burst buttons enable snappy zooming-in of the station groups.
:::

::: {#offcanvas20 .offcanvas .offcanvas-start tabindex="-1" aria-labelledby="offcanvas" style="width: auto"}
::: offcanvas-header
::: {.h5 .offcanvas-title}
Code for Mapping
:::

::: {.btn-close type="button" data-bs-dismiss="offcanvas" ariaLabel="Close"}
:::
:::

::: offcanvas-body
::: flex-code
```{r}
#| label: mapData
#| code-summary: "Processing 'flowData' created earlier to include geolocation data for mapview plots."
#| tidy: true

# All distinct stations in one column
names <- flowData |>
dplyr::select(from_station, to_station) |>
tidyr::pivot_longer(cols = 1:2,
names_to = NULL,
values_to = "station_names") |>
dplyr::distinct()

# The important geo-coordinates corresponding to station names
mapData <- dplyr::tbl(dbconn, filtered_path, check_from = FALSE) |>
dplyr::select(start_station_name,
start_lat,
start_lng,
end_station_name,
end_lat,
end_lng)

# Filter to include all observations that match the station names listed in 'names'. We need the geo-coordinates alongside the names.
mapData1 <- mapData |>
dplyr::collect() |>
# Filter, but through a vector of conditions.
dplyr::filter(start_station_name %in% names[[1]], end_station_name %in% names[[1]]) |>
dplyr::select(start_station_name:start_lng)

# Had to split 'mapData' into two and pivot into a single table.
mapData2 <- mapData |>
dplyr::collect() |>
dplyr::filter(start_station_name %in% names[[1]], end_station_name %in% names[[1]]) |>
dplyr::select(end_station_name:end_lng)

# Nice grouping
stations_groupMap <- dplyr::bind_rows(mapData1, mapData2) |>
dplyr::select(start_station_name, start_lat, start_lng) |>
dplyr::rename("station_names" = start_station_name,
"lat" = start_lat,
"lng" = start_lng) |>
dplyr::distinct() |>
dplyr::group_by(station_names)

# Setting seed for sampling
set.seed(113)

# Taking 10 random samples from each station_name group
sampled_stations <- stations_groupMap |>
dplyr::slice_sample(n = 10) |>
tidyr::drop_na()

```

```{r}
#| label: mapColors
#| code-summary: "Creates a map coloring palette excluding grays."
#| tidy: true

# All of the r-colors
allPalette <- colors()

# The grays are vast so we don't want those watering down the samples.
colorfulPal <- purrr::discard(allPalette, stringr::str_detect(allPalette, "gr(a|e)y"))

# When we sample the colors, 10 should be slightly more than needed.
n_colors <- 10
```

```{r}
#| label: mapViewer
#| code-summary: First, sourcing the script needed to generate the maps and creating the list of vectors used as input. These vectors are the slices of the top most traveled stations.
#| tidy: true

slicerVector <- list(c(1:9), c(10:18), c(19:27), c(28:34))
source("Scripts/mapViewer.R")
```

```{r}
#| file: "Scripts/mapViewer.R"
#| eval: false
#| code-summary: "The script used to generate the maps."
#| label: mapViewerScript
```
:::
:::
:::

::: {#accordionParent .accordion .mt-3 .mb-3}
::: accordion-item
::: {#headingOne .accordion-header}
::: {.accordion-button .collapsed type="button" data-bs-toggle="collapse" data-bs-target="#collapseOne" aria-expanded="true" aria-controls="collapseOne" style="background-color: #222"}
Benson Ave & Church St ... Ellis Ave & 60th St
:::
:::

::: {#collapseOne .accordion-collapse .collapse aria-labelledby="headingOne" data-bs-parent="#accordionParent"}
::: accordion-body
```{r}
#| label: fig-map1
#| fig-cap: "Benson Ave & Church St - Ellis Ave & 60th St"
#| tidy: true

set.seed(240)
randomColors <- sample(colorfulPal, n_colors)
mapViewer(slicerVector[[1]])
```
:::
:::
:::

::: accordion-item
::: {#headingTwo .accordion-header}
::: {.accordion-button .collapsed type="button" data-bs-toggle="collapse" data-bs-target="#collapseTwo" aria-expanded="false" aria-controls="collapseTwo" style="background-color: #222"}
Greenview Ave & Fullteron Ave ... Loomis Ave & Lexington St
:::
:::

::: {#collapseTwo .accordion-collapse .collapse aria-labelledby="headingTwo" data-bs-parent="#accordionParent"}
::: accordion-body
```{r}
#| label: fig-map2
#| echo: false
#| fig-cap: "Greenview Ave & Fullteron Ave - Loomis Ave & Lexington St"
#| tidy: true

set.seed(241)
randomColors <- sample(colorfulPal, n_colors)
mapViewer(slicerVector[[2]])
```
:::
:::
:::

::: accordion-item
::: {#headingThree .accordion-header}
::: {.accordion-button .collapsed type="button" data-bs-toggle="collapse" data-bs-target="#collapseThree" aria-expanded="false" aria-controls="collapseThree" style="background-color: #222"}
Michigan Ave & Oak St ... State St & 33rd St
:::
:::

::: {#collapseThree .accordion-collapse .collapse aria-labelledby="headingThree" data-bs-parent="#accordionParent"}
::: accordion-body
```{r}
#| label: fig-map3
#| echo: false
#| fig-cap: "Michigan Ave & Oak St - State St & 33rd St"
#| tidy: true

set.seed(242)
randomColors <- sample(colorfulPal, n_colors)
mapViewer(slicerVector[[3]])
```
:::
:::
:::

::: accordion-item
::: {#headingFour .accordion-header}
::: {.accordion-button .collapsed type="button" data-bs-toggle="collapse" data-bs-target="#collapseFour" aria-expanded="false" aria-controls="collapseFour" style="background-color: #222"}
Street Dr & Grand Ave ... Woodlawn Ave & 55th St
:::
:::

::: {#collapseFour .accordion-collapse .collapse aria-labelledby="headingFour" data-bs-parent="#accordionParent"}
::: accordion-body
```{r}
#| label: fig-map4
#| fig-cap: "Street Dr & Grand Ave - Woodlawn Ave & 55th St"
#| tidy: true

set.seed(243)
randomColors <- sample(colorfulPal, n_colors)
mapViewer(slicerVector[[4]])
```
:::
:::
:::
:::

### Interpretation of the Geographic Data

::: p-1
For example, suppose the user selects *University Ave & 57th St* in the epiflow visualization. This intersection happens to be at the heart of the University of Chicago campus. The natural next question is: where does the traffic to and from this location typically flow? By selecting one of the other nodes highlighted with flows directing away from the previous node, the user can identify *Kimbark Ave and 53rd St*. As seen in the map view, this location is situated adjacent to the *Vue 53 Apartments* complex. By analyzing such connections between nodes, the user can gain insights into common routes and destinations originating from a particular point of interest, potentially revealing patterns related to student housing, campus facilities, or other points of interest in the vicinity.

The data suggests individual members utilize the service multiple times weekly. However, further analysis is needed to determine if a significantly larger volume of unique individuals are annual members. Verifying associations between specific locations and higher or lower traffic could be a next step. Preliminary observations indicate universities, shopping centers, major companies, and nearby apartment complexes tend to have the highest ridership volumes.

To improve membership, addressing factors deterring individuals from becoming annual members could be key. These may include a lack of stations within walking distance of residences or destinations, or concerns over electric bicycle battery life and charging station availability, potentially explaining their lower utilization compared to classic bikes. Offering trial periods could allow casual users to experience the service's reliability and convenience, encouraging conversion to annual memberships.
:::

## Updated Database Tables List

```{r}
#| label: tbl-dbList2
#| code-summary: "Revisiting the list of db tables, with many more tables added. All of these tables are stored within the data/data.db file."
#| tbl-cap: "Database Table List: Post-Exploratory Analysis"
#| tidy: true

dbList2 <- 
duckdb::dbListTables(dbconn) |>
as.data.frame() |>
tabler(
title = "Post-Exploratory Database Tables",
note_list = list(gt::md("Tables in `db/data.db` at this stage")),
location_list = list("duckdb::dbListTables(dbconn)"),
noteColumns = TRUE,
source_note = gt::md("**Source**: `db/data.db`")) |>
gt::cols_label("duckdb::dbListTables(dbconn)" = "Table Paths") |>
gt::cols_align(align = "left")

dbList2

```

![Notice the size difference from the previous image. The database is still represented in db folder as one file..](images/dbFiles_end.png){.column-margin}

# Conclusion

::: p-1
These findings and recommendations are based on robust statistical analyses, including chi-square tests, binary logistic regression models, and visualization techniques. They provide a data-driven foundation for enhancing the Divvy bike-sharing service and better serving the residents of Chicago.
:::

## Key Findings

::: p-1
-   Membership status significantly influences bike usage patterns:

    -   Members prefer classic bikes (65%) over electric bikes (35%).
    -   Casual users have a higher electric bike usage (41%) compared to members.
    -   Members typically take shorter, faster trips.
    -   Casual users tend to engage in longer, slower rides.

-   Temporal patterns reveal distinct user behaviors:

    -   Weekdays and typical commuting hours see higher member activity.
    -   Weekends and evenings show increased casual ridership.
    -   Membership likelihood is highest during colder months and early spring.
    -   Summer months see a shift towards more casual ridership.

-   Trip characteristics vary by user type:

    -   Members are associated with shorter ride distances.
    -   Trip speed shows a strong positive association with membership status.
    -   The likelihood of membership decreases as trip duration increases.

-   High-traffic stations are often near universities, shopping centers, major companies, and apartment complexes.

-   The large sample size (nearly 4 million users) allows for high statistical significance in observed differences.
:::

## Recommendations

::: p-1
1.  Tailor marketing strategies to target potential members for short, frequent trips, especially for commuting purposes.

2.  Optimize bike distribution to meet the demand for classic bikes among members and electric bikes among casual users.

3.  Implement promotional campaigns during summer months to convert casual users to members.

4.  Consider offering trial periods to allow casual users to experience the benefits of membership.

5.  Investigate factors deterring individuals from becoming annual members, such as station proximity to residences or destinations.

6.  Address potential concerns over electric bicycle battery life and charging station availability.

7.  Focus on improving service near high-traffic areas like universities, shopping centers, and residential complexes.

8.  Develop targeted strategies to encourage casual users of longer, leisure rides to consider membership benefits.

9.  Utilize the epiflows visualization tool to identify and optimize popular routes and destinations.

10. Continue to collect and analyze data to refine understanding of user behavior and preferences over time.
:::

```{r}
#| eval: false
#| include: false

# If you need to drop any tables without deleting the entire database.
source("Scripts/duckDrops.R")
```
